[{"title":"诗词收集","url":"/2023/01/09/Books_%E8%AF%97%E8%AF%8D%E6%94%B6%E9%9B%86/","content":"收集一些自己平时见到的诗句：\n\n\nUpdate 2018-12-24\n\n邻人焉有许多鸡，乞丐何曾有二妻。当时尚有周天子，何事纷纷说魏齐。\n算不尽芸芸众生微贱命，回头看五味杂陈奈何天。\n何时杖策相随去，任性逍遥不学禅。\n百无一用是情深，万般奈何为情困。\n桃李春风一杯酒，江湖夜雨十年灯。\n杀人放火金腰带，修桥补路无尸骸。\n我是人间惆怅客，知君何事泪纵横。断肠声里忆平生。\n曾经沧海难为水，除却巫山不是云。取次花丛懒回顾，半缘修道半缘君。\n人生到处应何似，应似飞鸿踏雪泥。\n山有木兮木有枝，心悦君兮君不知。\n嫦娥应悔偷灵药，碧海青天夜夜心。\n满堂花醉三千客，一剑霜寒十四州。\n早岁读书无甚解，晚年省事有奇功。\n愿我如星君如月，夜夜流光相皎洁。\n一身诗意千寻瀑，万古流芳四月天。\n未曾相逢先一笑，初会便已许平生。\n三千年读史不过功名利禄，九万里悟道终归诗酒田园。\n君埋泉下泥销骨，我寄人间雪满头。\n伤心桥下春波绿，曾是惊鸿照影来。\n醉后不知天在水，满船清梦压星河。\n秋风吹尽旧庭柯，黄叶丹枫客里过。一点禅灯半轮月，今宵寒较昨宵多。\n细雨生寒未有霜，庭前木叶半青黄。小春此去无多日，何处梅花一绽香。\n岁久人无千日好，春深花有几时红。是非入耳君须忍，半作痴呆半作聋。\n清风不识字，何故乱翻书。\n三过平山堂下，半生弹指声中。十年不见老仙翁，壁上龙蛇飞动。欲吊文章太守，仍歌杨柳春风。休言万事转头空，未转头时皆梦。\n山月不知心里事，水风空落眼前花。\n未若锦囊收艳骨，一抔冷土掩风流。\n莫言下岭便无难，赚得行人空喜欢。正入万山圈子里，一山放过一山拦。\n有美一人兮婉如清扬，识曲别音兮令姿煌煌。绣袂捧琴兮登君子堂，如彼萱草兮使我忧忘。欲赠之以紫玉尺，白银珰，久不见之兮湘水茫茫。\n生如逆旅，一苇以航。\n\n\nUpdate 2023-01-09\n\n回头万里，故人长绝。易水萧萧西风冷，满座衣冠似雪。《贺新郎·别茂嘉十二弟》辛弃疾\n洛阳城里春光好，洛阳才子他乡老。《菩萨蛮·洛阳城里春光好》韦庄\n伤心桥下春波绿，曾是惊鸿照影来。《沈园二首·其一》陆游\n谁见幽人独往来，缥缈孤鸿影。《卜算子·黄州定慧院寓居作》苏轼\n琵琶弦上说相思。当时明月在，曾照彩云归。《临江仙·梦后楼台高锁》晏几道\n此后锦书休寄，画楼云雨无凭。《清平乐·留人不住》晏几道\n美人自刎乌江岸，战火曾烧赤壁山，将军空老玉门关。\n须知少时凌云志，曾许人间第一流。哪晓岁月蹉跎过，依旧名利两无收。\n时人不识凌云木，直待凌云始道高。\n总有千古，横有八荒，前途似海，来日方长。\n鹏北海，风朝阳，又携书剑路茫茫。明年此日青云去，却笑人间举子忙。\n\n\nUpdate 2023-03-01 派克直播间飞花令， 学点儿\n\n不是逢人苦誉君，亦狂亦侠亦温文。照人胆似秦时月，送我情如岭上云。 \n一生一代一双人，争教两处销魂。相思相望不相亲，天为谁春？\n也信美人终作土，不堪幽梦太匆匆。\n银字笙调。心字香烧。料芳悰、乍整还凋。待将春恨，都付春潮。过窈娘堤，秋娘渡，泰娘桥。 \n中心藏之，何日忘之!\n\n\nUpdate 2023-03-05 派克直播间飞花令，再学点儿\n\n钓鱼台，十年不上野鸥猜。白云来往青山在，对酒开怀。欠伊周济世才，犯刘阮贪杯戒，还李杜吟诗债。酸斋笑我，我笑酸斋。晚归来，西湖山上野猿哀。二十年多少风流怪，花落花开。望云霄拜将台。袖星斗安邦策，破烟月迷魂寨。酸斋笑我，我笑酸斋。  \n知君用心如日月，事夫誓拟同生死。还君明珠双泪垂，恨不相逢未嫁时。  \n千古风流八咏楼，江山留与后人愁。水通南国三千里，气压江城十四州。  \n问余何意栖碧山，笑而不答心自闲。桃花流水窅然去，别有天地非人间。\n\n","categories":["Books"],"tags":["诗"]},{"title":"制作一个可用的malloc image","url":"/2023/01/03/malloc-docker-image-md/","content":"写一个melloc的C程序#include &lt;stdio.h>\n#include &lt;stdlib.h>\n#include &lt;string.h>\n \n#include &lt;signal.h>\n#include &lt;unistd.h>\n \n#define SIGTERM_MSG \"SIGTERM received.\\n\"\n \nvoid sig_term_handler(int signum, siginfo_t *info, void *ptr)\n&#123;\n    write(STDERR_FILENO, SIGTERM_MSG, sizeof(SIGTERM_MSG));\n&#125;\n \nvoid catch_sigterm()\n&#123;\n    static struct sigaction _sigact;\n \n    memset(&amp;_sigact, 0, sizeof(_sigact));\n    _sigact.sa_sigaction = sig_term_handler;\n    _sigact.sa_flags = SA_SIGINFO;\n \n    sigaction(SIGTERM, &amp;_sigact, NULL);\n&#125;\n\nint main(int argc, char *argv[])\n&#123;\n    if ( argc != 2 )\n    &#123;\n        printf(\"ERROR ELEMENT COUNTS.\");\n        return 1;\n    &#125;\n\n    // printf (\"%ld\\n\", atol(argv[1]));\n\n    int *p;\n    long n = atol(argv[1]) * 1024 * 1024;\n\n    // printf (\"%ld\\n\", n);\n    \n    printf(\"Allocate Memory Size: %ld MB.\\n\", atol(argv[1]));\n    \n    // Allocate Memory.\n    p = (int *)malloc(n);\n    \n    if (p != NULL)\n        printf(\"SUCCESS.\");\n    else\n        printf(\"FAILED.\");\n    \n    memset(p, 0, n);\n\n    catch_sigterm();\n\n    free(p);\n    \n    sleep(3000);\n\n    return 0;\n&#125;\n\n写一个DockerfileFROM alpine\nRUN apk add build-base\nCOPY mem.c .\nRUN gcc -o mem mem.c\n# or RUN gcc -static -o mem mem.c\n\nFROM alpine\nCOPY --from=0 ./mem .\nENTRYPOINT [ \"/mem\" ]\n\n进行一个很新的测试Build Image.\ndive build -t liarlee-malloc:latest .\nOR\ndocker build -t liarlee-malloc:latest .\nDocker RUN.docker run --name malloc --rm -dt liarlee-malloc:latest 200\n# docker run --name malloc --rm -dt liarlee-malloc:latest [MemorySize(MB)]\nKubernetes Deployment RUN.---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: liarlee-malloc\nspec:\n  selector:\n    matchLabels:\n      app: malloc\n  replicas: 1 \n  template: \n    metadata:\n      labels:\n        app: malloc\n    spec:\n      containers:\n      - name: malloc\n        image: liarlee-malloc:latest\n        args: [ \"200\" ]\n","categories":["Linux"],"tags":["docker"]},{"title":"bpftrace 使用以及理解","url":"/2022/12/29/bpftrace-record-md/","content":"BPFtrace oneline program[root@localhost-live ~]# bpftrace -e 'tracepoint:syscalls:sys_enter_execve &#123; printf(\"%s %s\\n\", comm, str(args->filename));&#125;'\n\nTracepoint如何获取可用参数的解释[root@localhost-live sys_enter_execve]# pwd\n/sys/kernel/tracing/events/syscalls/sys_enter_execve\n[root@localhost-live sys_enter_execve]# grep -ri .\nformat:name: sys_enter_execve\nformat:ID: 742\nformat:format:\nformat:\tfield:unsigned short common_type;\toffset:0;\tsize:2;\tsigned:0;\nformat:\tfield:unsigned char common_flags;\toffset:2;\tsize:1;\tsigned:0;\nformat:\tfield:unsigned char common_preempt_count;\toffset:3;\tsize:1;\tsigned:0;\nformat:\tfield:int common_pid;\toffset:4;\tsize:4;\tsigned:1;\nformat:\tfield:int __syscall_nr;\toffset:8;\tsize:4;\tsigned:1;\nformat:\tfield:const char * filename;\toffset:16;\tsize:8;\tsigned:0;\nformat:\tfield:const char *const * argv;\toffset:24;\tsize:8;\tsigned:0;\nformat:\tfield:const char *const * envp;\toffset:32;\tsize:8;\tsigned:0;\nformat:print fmt: \"filename: 0x%08lx, argv: 0x%08lx, envp: 0x%08lx\", ((unsigned long)(REC->filename)), ((unsigned long)(REC->argv)), ((unsigned long)(REC->envp))\ntrigger:# Available triggers:\ntrigger:# traceon traceoff snapshot stacktrace enable_event disable_event enable_hist disable_hist hist\nfilter:none\nid:742\nenable:0\n\n","categories":["Linux"],"tags":[]},{"title":"Nginx性能调整（不一定对","url":"/2022/12/27/Nginx-Performance-Tuning-Pre-md/","content":"Nginx 性能优化看了不少的文档和说明， 尝试调整一下Nginx，看看与默认的设置性能表现能有多大的差距,顺便记录一下步骤，不记录的话自己会忘记的。 \nsysctl 参数~]$ cat /etc/sysctl.d/99-hayden.cof\nnet.ipv4.tcp_wmem = 8192 4194304 8388608\nnet.ipv4.tcp_rmem = 8192 4194304 8388608\n\nnet.core.somaxconn = 262144\n\nnet.core.default_qdisc=fq\nnet.ipv4.tcp_congestion_control=bbr\nNginx Config~]$ cat /etc/nginx/nginx.conf\n\nuser nginx;\nworker_processes 1;\nworker_cpu_affinity 10;\n# error_log /var/log/nginx/error.log;\npid /run/nginx.pid;\n\n# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.\ninclude /usr/share/nginx/modules/*.conf;\n\nevents &#123;\n    worker_connections 10240;\n&#125;\n...# 后面的都是默认值。\n\nSystemd Nginx Service~]$ systemctl cat nginx\n# /usr/lib/systemd/system/nginx.service\n[Unit]\nDescription=The nginx HTTP and reverse proxy server\nAfter=network-online.target remote-fs.target nss-lookup.target\nWants=network-online.target\n\n[Service]\nType=forking\nPIDFile=/run/nginx.pid\n# Nginx will fail to start if /run/nginx.pid already exists but has the wrong\n# SELinux context. This might happen when running `nginx -t` from the cmdline.\n# https://bugzilla.redhat.com/show_bug.cgi?id=1268621\nExecStartPre=/usr/bin/rm -f /run/nginx.pid\nExecStartPre=/usr/sbin/nginx -t\nExecStart=/usr/sbin/nginx\nExecReload=/usr/sbin/nginx -s reload\nKillSignal=SIGQUIT\nTimeoutStopSec=5\nKillMode=process\nPrivateTmp=true\nLimitAS=infinity\nLimitRSS=infinity\nLimitCORE=infinity\nLimitNOFILE=65536\nLimitNPROC=65535\nNice=-20\n\n[Install]\nWantedBy=multi-user.target\nCPU Isolate~]$ cat /etc/default/grub\nGRUB_CMDLINE_LINUX_DEFAULT=\"console=tty0 console=ttyS0,115200n8 net.ifnames=0 biosdevname=0 nvme_core.io_timeout=4294967295 rd.emergency=poweroff rd.shell=0 isolcpus=1 nohz_full=1 rcu_nocbs=1\"\nGRUB_TIMEOUT=0\nGRUB_DISABLE_RECOVERY=\"true\"\nGRUB_TERMINAL=\"ec2-console\"\nGRUB_X86_USE_32BIT=\"true\"\n\n调整中断~]$ cat /proc/interrupts\n           CPU0       CPU1\n 27:          0        845   PCI-MSI 81920-edge      ena-mgmnt@pci:0000:00:05.0\n 28:    1455005          0   PCI-MSI 81921-edge      eth0-Tx-Rx-0\n 29:          0    1870959   PCI-MSI 81922-edge      eth0-Tx-Rx-1\nLOC:     119785      15755   Local timer interrupts\n\n~]$ echo 0 > /proc/irq/29/smp_affinity_list\n~]$ echo 0 > /proc/irq/28/smp_affinity_list\n~]$ echo 0 > /proc/irq/27/smp_affinity_list\n\n关闭 irqbalance~]$ sudo systemctl stop irqbalance.service\n~]$ sudo systemctl disable irqbalance.service\nRemoved symlink /etc/systemd/system/multi-user.target.wants/irqbalance.service.\n\n测试这些都做完之后，启动一个新的实例，安装nginx之后不做任何的动作， 使用ab命令测试默认页 index.html。 对比\n调整后的实例结果如下使用命令： ab -c 3000 -n 50000 http://172.31.37.166:80/index.html\n第一次Document Path:          &#x2F;index.html\nDocument Length:        732 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   2.225 seconds\nComplete requests:      50000\nFailed requests:        0\nTotal transferred:      48250000 bytes\nHTML transferred:       36600000 bytes\nRequests per second:    22474.64 [#&#x2F;sec] (mean)\nTime per request:       133.484 [ms] (mean)\nTime per request:       0.044 [ms] (mean, across all concurrent requests)\nTransfer rate:          21179.71 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   67 139.7     47    1124\nProcessing:    25   63  16.1     61     274\nWaiting:        0   47  13.7     46     254\nTotal:         57  130 143.2    106    1191\n\nPercentage of the requests served within a certain time (ms)\n  50%    106\n  66%    125\n  75%    131\n  80%    135\n  90%    142\n  95%    149\n  98%    165\n  99%   1173\n 100%   1191 (longest request)\n第二次Document Path:          &#x2F;index.html\nDocument Length:        732 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   2.232 seconds\nComplete requests:      50000\nFailed requests:        0\nTotal transferred:      48250000 bytes\nHTML transferred:       36600000 bytes\nRequests per second:    22397.25 [#&#x2F;sec] (mean)\nTime per request:       133.945 [ms] (mean)\nTime per request:       0.045 [ms] (mean, across all concurrent requests)\nTransfer rate:          21106.78 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   67 141.3     47    1092\nProcessing:    22   63  15.8     60     106\nWaiting:        0   47  13.5     46      84\nTotal:         60  130 143.5    108    1175\n\nPercentage of the requests served within a certain time (ms)\n  50%    108\n  66%    122\n  75%    131\n  80%    134\n  90%    141\n  95%    149\n  98%    165\n  99%   1150\n 100%   1175 (longest request)\n第三次Document Path:          &#x2F;index.html\nDocument Length:        732 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   2.220 seconds\nComplete requests:      50000\nFailed requests:        0\nTotal transferred:      48250000 bytes\nHTML transferred:       36600000 bytes\nRequests per second:    22526.08 [#&#x2F;sec] (mean)\nTime per request:       133.179 [ms] (mean)\nTime per request:       0.044 [ms] (mean, across all concurrent requests)\nTransfer rate:          21228.19 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   64 123.5     49    1087\nProcessing:    35   65  15.1     63     108\nWaiting:        0   48  13.2     48      88\nTotal:         61  129 125.5    115    1166\n\nPercentage of the requests served within a certain time (ms)\n  50%    115\n  66%    123\n  75%    132\n  80%    136\n  90%    140\n  95%    145\n  98%    159\n  99%   1139\n 100%   1166 (longest request)\n\n并未调整的实例结果如下使用命令：ab -c 3000 -n 50000 http://172.31.45.66:80/index.html\n第一次Document Path:          &#x2F;index.html\nDocument Length:        615 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   2.197 seconds\nComplete requests:      50000\nFailed requests:        9856\n   (Connect: 0, Receive: 0, Length: 4928, Exceptions: 4928)\nTotal transferred:      38221056 bytes\nHTML transferred:       27719280 bytes\nRequests per second:    22754.61 [#&#x2F;sec] (mean)\nTime per request:       131.841 [ms] (mean)\nTime per request:       0.044 [ms] (mean, across all concurrent requests)\nTransfer rate:          16986.43 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   61 115.4     47    1110\nProcessing:    23   65  15.2     65     112\nWaiting:        0   45  18.6     48      88\nTotal:         61  126 115.4    111    1209\n\nPercentage of the requests served within a certain time (ms)\n  50%    111\n  66%    120\n  75%    125\n  80%    128\n  90%    139\n  95%    144\n  98%    157\n  99%   1143\n 100%   1209 (longest request)\n\n第二次Document Path:          &#x2F;index.html\nDocument Length:        615 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   2.202 seconds\nComplete requests:      50000\nFailed requests:        9408\n   (Connect: 0, Receive: 0, Length: 4704, Exceptions: 4704)\nTotal transferred:      38411008 bytes\nHTML transferred:       27857040 bytes\nRequests per second:    22710.18 [#&#x2F;sec] (mean)\nTime per request:       132.099 [ms] (mean)\nTime per request:       0.044 [ms] (mean, across all concurrent requests)\nTransfer rate:          17037.52 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   64 123.9     48    1089\nProcessing:    22   65  15.9     65     116\nWaiting:        0   46  18.9     48      89\nTotal:         65  128 125.4    111    1171\n\nPercentage of the requests served within a certain time (ms)\n  50%    111\n  66%    124\n  75%    128\n  80%    132\n  90%    141\n  95%    146\n  98%    166\n  99%   1124\n 100%   1171 (longest request)\n\n第三次Document Path:          &#x2F;index.html\nDocument Length:        615 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   2.236 seconds\nComplete requests:      50000\nFailed requests:        9216\n   (Connect: 0, Receive: 0, Length: 4608, Exceptions: 4608)\nTotal transferred:      38492416 bytes\nHTML transferred:       27916080 bytes\nRequests per second:    22361.25 [#&#x2F;sec] (mean)\nTime per request:       134.161 [ms] (mean)\nTime per request:       0.045 [ms] (mean, across all concurrent requests)\nTransfer rate:          16811.30 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   64 111.8     52    1110\nProcessing:    26   67  15.8     67     113\nWaiting:        0   47  19.0     49      86\nTotal:         65  130 114.2    118    1205\n\nPercentage of the requests served within a certain time (ms)\n  50%    118\n  66%    126\n  75%    131\n  80%    134\n  90%    141\n  95%    147\n  98%    158\n  99%   1117\n 100%   1205 (longest request)\n\n总结如果对比的话， 可以明显的发现，Connection Time 的输出中， 明显看到Processing 阶段（在 15.8 - 16.1 之间） 和 Waiting阶段 （在 13.2 - 13.7 之间） 变得稳定。\n观察CPU使用率， 默认的设置 CPU 的使用率在多个核心之间均衡， IRQ以及 us sys 在多个核心上平均分布。调整之后， Nginx在CPU1上处理数据， 中断以及其他进程完全跑满CPU0， Nginx单独在CPU1上面工作， 最大程度的处理数据。 这也是ab命令中返回的 processing time 更短更稳定的原因（大概。\n在调整之后， CPU1 上面会触发大量的RES中断， 这会导致当前的CPU无法完全的跑满， 这个部分还是一个问题， 我不知道RES的中断能不能也移除， 不过我感觉这个可能并不能， 应该是调度的需要， 但是过多的中断数量本身也会导致性能问题， 可能还有其他的解法， 暂时不去考虑了。遗留。\n附加静态页面之后的测试Hexo Render Project\n调整之后的测试命令： ab -c 3000 -n 50000 http://172.31.37.166:80/index.html\n第一次Document Path:          /index.html\nDocument Length:        35312 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   3.373 seconds\nComplete requests:      50000\nFailed requests:        0\nTotal transferred:      1777400000 bytes\nHTML transferred:       1765600000 bytes\nRequests per second:    14822.50 [#/sec] (mean)\nTime per request:       202.395 [ms] (mean)\nTime per request:       0.067 [ms] (mean, across all concurrent requests)\nTransfer rate:          514560.94 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0   54 104.2     44    1061\nProcessing:    37  144  23.0    145     196\nWaiting:        0   41  14.4     38     103\nTotal:         80  198 108.1    193    1229\n\nPercentage of the requests served within a certain time (ms)\n  50%    193\n  66%    200\n  75%    207\n  80%    209\n  90%    215\n  95%    218\n  98%    231\n  99%   1194\n 100%   1229 (longest request)\n\n第二次Document Path:          /index.html\nDocument Length:        35312 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   3.094 seconds\nComplete requests:      50000\nFailed requests:        0\nTotal transferred:      1777400000 bytes\nHTML transferred:       1765600000 bytes\nRequests per second:    16157.77 [#/sec] (mean)\nTime per request:       185.669 [ms] (mean)\nTime per request:       0.062 [ms] (mean, across all concurrent requests)\nTransfer rate:          560914.54 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0   54 120.1     39    1070\nProcessing:    64  127  19.7    128     180\nWaiting:        0   37  14.1     35      98\nTotal:        106  181 123.6    171    1240\n\nPercentage of the requests served within a certain time (ms)\n  50%    171\n  66%    180\n  75%    183\n  80%    184\n  90%    190\n  95%    195\n  98%    204\n  99%   1184\n 100%   1240 (longest request)\n\n第三次Document Path:          /index.html\nDocument Length:        35312 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   3.212 seconds\nComplete requests:      50000\nFailed requests:        0\nTotal transferred:      1777400000 bytes\nHTML transferred:       1765600000 bytes\nRequests per second:    15565.24 [#/sec] (mean)\nTime per request:       192.737 [ms] (mean)\nTime per request:       0.064 [ms] (mean, across all concurrent requests)\nTransfer rate:          540344.85 [Kbytes/sec] received\n\nConnection Times (ms)\n              min  mean[+/-sd] median   max\nConnect:        0   55 115.1     43    1107\nProcessing:    57  133  23.3    136     190\nWaiting:        0   39  14.6     36     101\nTotal:        110  188 121.2    182    1290\n\nPercentage of the requests served within a certain time (ms)\n  50%    182\n  66%    188\n  75%    194\n  80%    198\n  90%    205\n  95%    212\n  98%    223\n  99%   1238\n 100%   1290 (longest request)\n\n\n未调整的测试命令： ab -c 3000 -n 50000 http://172.31.35.221:80/index.html\n第一次Document Path:          &#x2F;index.html\nDocument Length:        0 bytes\n\nConcurrency Level:      3000\nTime taken for tests:   3.217 seconds\nComplete requests:      50000\nFailed requests:        50000\n   (Connect: 0, Receive: 0, Length: 48496, Exceptions: 1504)\nTotal transferred:      1723935808 bytes\nHTML transferred:       1712490752 bytes\nRequests per second:    15543.00 [#&#x2F;sec] (mean)\nTime per request:       193.013 [ms] (mean)\nTime per request:       0.064 [ms] (mean, across all concurrent requests)\nTransfer rate:          523342.50 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   50 115.9     37    1091\nProcessing:    28  139  29.4    148     338\nWaiting:        0   45  13.6     48     249\nTotal:         60  189 122.8    185    1287\n\nPercentage of the requests served within a certain time (ms)\n  50%    185\n  66%    195\n  75%    197\n  80%    198\n  90%    211\n  95%    216\n  98%    225\n  99%   1233\n 100%   1287 (longest request)\n未调整的情况下直接就失败了。。 所有的请求最终都Failed了， 尝试调小。命令： ab -c 1897 -n 50000 http://172.31.35.221/index.html可以接受的最大值是：\nDocument Path:          &#x2F;index.html\nDocument Length:        35312 bytes\n\nConcurrency Level:      1897\nTime taken for tests:   3.206 seconds\nComplete requests:      50000\nFailed requests:        0\nTotal transferred:      1777400000 bytes\nHTML transferred:       1765600000 bytes\nRequests per second:    15597.31 [#&#x2F;sec] (mean)\nTime per request:       121.624 [ms] (mean)\nTime per request:       0.064 [ms] (mean, across all concurrent requests)\nTransfer rate:          541458.01 [Kbytes&#x2F;sec] received\n\nConnection Times (ms)\n              min  mean[+&#x2F;-sd] median   max\nConnect:        0   30  86.9     23    1062\nProcessing:    30   90  14.9     93     274\nWaiting:        0   30   8.6     30     225\nTotal:         53  120  89.5    117    1180\n\nPercentage of the requests served within a certain time (ms)\n  50%    117\n  66%    121\n  75%    124\n  80%    125\n  90%    132\n  95%    140\n  98%    145\n  99%    147\n 100%   1180 (longest request)\n","categories":[],"tags":["Linux"]},{"title":"Docker/Containerd 配置代理","url":"/2022/12/20/Docker-daemon-proxy-setting-md/","content":"记录一下 Docker Daemon &#x2F; Containerd 配置代理的步骤，尽管能用的时候不太多。\nDocker创建文件]$ mkdir -pv /etc/systemd/system/docker.service.d\n]$ touch /etc/systemd/system/docker.service.d/proxy.conf\n\n写入内容[Service]\nEnvironment=\"HTTP_PROXY=socks5://&lt;-->:&lt;-->/\"\nEnvironment=\"HTTPS_PROXY=socks5://&lt;-->:&lt;-->/\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1\"\n\n重启DockerDaemon]$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker\n\n\nContainerd创建环境变量文件]$ mkdir -pv /etc/systemd/system/containerd.service.d\n]$ touch /etc/systemd/system/containerd.service.d/proxy.conf\n\n写入内容[Service]\nEnvironment=\"HTTP_PROXY=socks5://&lt;-->:&lt;-->/\"\nEnvironment=\"HTTPS_PROXY=socks5://&lt;-->:&lt;-->/\"\nEnvironment=\"NO_PROXY=localhost,127.0.0.1\"\n\n重启 containerd]$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker\n","categories":[],"tags":[]},{"title":"git使用以及命令记录","url":"/2022/12/19/git-1/","content":"Method 1Step 1Github上面创建一个新的仓库， 页面创建即可， 然后记录下URL。\nhttps://github.com/LiarLee/vps-init.git\n\nstep 2本地创建目录， 并初始化本地的仓库路径。\nmkdir vps-init\ncd ./vps-init\ngit clone https://github.com/LiarLee/vps-init.git\n\nMethod 2step 1创建一个本地仓库。\nmkdir vps-init\ncd ./vps-init\ngit init .\ntouch README\necho \"For init server use DroneCI.\"\ngit add -A\ngit commit -m \"init\"\ngit remote add origin https://github.com/LiarLee/vps-init.git\ngit -u origin master\n","categories":[],"tags":["Linux"]},{"title":"完全隔离CPU的方法","url":"/2022/10/11/isolate-cpu-and-taskset-to-it/","content":"完全隔离CPU， 并排空CPU所有的进程，分配指定的任务到CPU上。\n\n当前的情况下， 我有CPU 0 - 3， 我希望隔离出CPU3 来进行指定的任务运行（方法是传递内核参数并重启）\nvim /etc/default/grub\n\nGRUB_CMDLINE_LINUX='................isolcpu=3 nohz_full=3'\n\ngrub2-mkconfig -o /boot/grub2/grub.cfg\nsync &amp;&amp; systemctl reboot\n\n重启之后，CPU3 就已经从CFS的调度列表上面拿掉了， 可以通过如下的参数证明。\n# 查看设置是否已经正确的生效， 分离的CPU会表示为序号， 一般是： 0-2 ， 3 。 这两种表示方式。\ncat /sys/devices/system/cpu/isolated\n3\ncat /sys/devices/system/cpu/nohz_full\n3\n\n关于nohz_full这个参数， 需要编译内核的时候就启用这个功能。\n# 需要启用的参数如下：\nCONFIG_NO_HZ_COMMON=y\nCONFIG_NO_HZ_FULL=y\nCONFIG_NO_HZ=y\n这部分的内容还包括的Kernel的Tickless等等知识， 这个部分的内容我在Redhat的文档中有看到。但是文档比较旧了， 这个设置是基于Redhat 7 版本的说明， 可能现在有更好的方法，我不太确定。\n\n重启之后， CPU3上面已经完全不会有用户空间的进程被调度上去了，同时， 由于已经配置了NOHZ的参数，CPU3 上面也不会有Kernel Timer Interrept触发，因此也少了一部分中断。\n\nRedhat的文档中定义了如下的方式进行验证， 我直接抄下面的内容了。\n\n关于如何验证Cpu隔离的文档： https://access.redhat.com/solutions/3875421配置隔离CPU的方法已经确认功能是否激活的方法：https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/index\n\nperf stat -C 1 -e irq_vectors:local_timer_entry taskset -c 3 sleep 3\n先记录这么多吧， 这个功能用到的地方实在是有限，基本上不怎么需要。并且需要知道是现在已经不怎么需要使用这个工具进行调优了， Redhat有 Tuned 守护进程， 可以通过默认的profile对OS进行调优。\n\n\n","categories":[],"tags":["Linux"]},{"title":"Cilium 踩坑总结","url":"/2022/08/19/CiliumInstallation&Summary/","content":"测试环境\nKVM  - hayden@HaydenArchDesktop ~&gt; virsh version\n\n Compiled against library: libvirt 8.6.0 Using library: libvirt 8.6.0 Using API: QEMU 8.6.0 Running hypervisor: QEMU 7.0.0\n\n\nVM OS： root@fedora ~# cat &#x2F;etc&#x2F;os-release\n\n NAME&#x3D;”Fedora Linux” VERSION&#x3D;”36 (Thirty Six)” ID&#x3D;fedora VERSION_ID&#x3D;36\n\n\nKVM 虚拟机两台\n\n\nMaster： hostname: fedora\nNode： hostname: knode1\n\n\n\nkubernetes 版本： v1.24.3\n\nkubernetes 安装方式： kubeadm \n\ndocker版本:  docker:&#x2F;&#x2F;20.10.17 \n\nNOTE: (尝试使用Containerd， 但是翻车了， 控制平面的Pod启动不了， 所以放弃了，遂使用CRI-Dockerd，配置了Docker runtime， Containerd使用默认的参数无法正常的启动， 看起来即使真的升级到了1.24 迁移还是一个问题)\n\n\nKernel Version: 5.17.5-300.fc36.x86_64\n\n\nHelm参数如果是在KVM启动的虚拟机，可以通过这个安装参数来开启更多功能，但是受限于我的KVM虚拟网卡驱动不能attach xdp 程序， 所以。。。。xdp 加速无法启用，但是其他的高级特性均可开启， 集群状态正常。\nhelm upgrade -i cilium cilium/cilium \\\n  --namespace kube-system \\\n  --set tunnel=disabled \\\n  --set autoDirectNodeRoutes=true \\\n  --set loadBalancer.mode=dsr \\\n  --set kubeProxyReplacement=strict \\\n  --set enableIPv4Masquerade=false \\\n  --set loadBalancer.algorithm=maglev \\\n  --set devices=enp1s0 \\\n  --set k8sServiceHost=192.168.31.100 \\\n  --set k8sServicePort=6443 \\\n  --set hubble.relay.enabled=true \\\n  --set hubble.ui.enabled=true\n\n\n特性以及状态检查默认的安装完成之后开启特性如下：\n\nKubeproxy Bypass\nIptables Bypass\nLoadBalancer 算法： Meglav\nLoadBalancer 特性： DSR\n报文Masquerade 封装： Disabled\nHubble ： Enable\n隧道封包： Disabled\n\n\n下面是 CIlium Status的命令返回结果：\nroot@fedora ~# cilium status\n    /¯¯\\\n /¯¯\\__/¯¯\\    Cilium:         OK\n \\__/¯¯\\__/    Operator:       OK\n /¯¯\\__/¯¯\\    Hubble:         OK\n \\__/¯¯\\__/    ClusterMesh:    disabled\n    \\__/\n\nDeployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2\nDeployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1\nDaemonSet         cilium             Desired: 2, Ready: 2/2, Available: 2/2\nDeployment        hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1\nContainers:       hubble-relay       Running: 1\n                  hubble-ui          Running: 1\n                  cilium             Running: 2\n                  cilium-operator    Running: 2\nCluster Pods:     14/14 managed by Cilium\nImage versions    hubble-ui          quay.io/cilium/hubble-ui:v0.9.0@sha256:0ef04e9a29212925da6bdfd0ba5b581765e41a01f1cc30563cef9b30b457fea0: 1\n                  hubble-ui          quay.io/cilium/hubble-ui-backend:v0.9.0@sha256:000df6b76719f607a9edefb9af94dfd1811a6f1b6a8a9c537cba90bf12df474b: 1\n                  cilium             quay.io/cilium/cilium:v1.12.0@sha256:079baa4fa1b9fe638f96084f4e0297c84dd4fb215d29d2321dcbe54273f63ade: 2\n                  cilium-operator    quay.io/cilium/operator-generic:v1.12.0@sha256:bb2a42eda766e5d4a87ee8a5433f089db81b72dd04acf6b59fcbb445a95f9410: 2\n                  hubble-relay       quay.io/cilium/hubble-relay:v1.12.0@sha256:ca8033ea8a3112d838f958862fa76c8d895e3c8d0f5590de849b91745af5ac4d: 1\n\nds&#x2F;cilium 中的命令返回结果：\nroot@fedora:/home/cilium# cilium status\nKVStore:                 Ok   Disabled\nKubernetes:              Ok   1.24 (v1.24.3) [linux/amd64]\nKubernetes APIs:         [\"cilium/v2::CiliumClusterwideNetworkPolicy\", \"cilium/v2::CiliumEndpoint\", \"cilium/v2::CiliumNetworkPolicy\", \"cilium/v2::CiliumNode\", \"core/v1::Namespace\", \"core/v1::Node\", \"core/v1::Pods\", \"core/v1::Service\", \"discovery/v1::EndpointSlice\", \"networking.k8s.io/v1::NetworkPolicy\"]\nKubeProxyReplacement:    Strict   [enp1s0 192.168.31.100 (Direct Routing)]\nHost firewall:           Disabled\nCNI Chaining:            none\nCilium:                  Ok   1.12.0 (v1.12.0-9447cd1)\nNodeMonitor:             Listening for events on 4 CPUs with 64x4096 of shared memory\nCilium health daemon:    Ok\nIPAM:                    IPv4: 3/254 allocated from 10.0.0.0/24,\nBandwidthManager:        Disabled\nHost Routing:            BPF\nMasquerading:            Disabled\nController Status:       23/23 healthy\nProxy Status:            OK, ip 10.0.0.78, 0 redirects active on ports 10000-20000\nGlobal Identity Range:   min 256, max 65535\nHubble:                  Ok   Current/Max Flows: 283/4095 (6.91%), Flows/s: 1.62   Metrics: Disabled\nEncryption:              Disabled\nCluster health:          2/2 reachable   (2022-08-23T01:57:33Z)\n\nCilium verbose的详细结果：\nroot@fedora:/home/cilium# cilium status --verbose\nKVStore:                Ok   Disabled\nKubernetes:             Ok   1.24 (v1.24.3) [linux/amd64]\nKubernetes APIs:        [\"cilium/v2::CiliumClusterwideNetworkPolicy\", \"cilium/v2::CiliumEndpoint\", \"cilium/v2::CiliumNetworkPolicy\", \"cilium/v2::CiliumNode\", \"core/v1::Namespace\", \"core/v1::Node\", \"core/v1::Pods\", \"core/v1::Service\", \"discovery/v1::EndpointSlice\", \"networking.k8s.io/v1::NetworkPolicy\"]\nKubeProxyReplacement:   Strict   [enp1s0 192.168.31.100 (Direct Routing)]\nHost firewall:          Disabled\nCNI Chaining:           none\nCilium:                 Ok   1.12.0 (v1.12.0-9447cd1)\nNodeMonitor:            Listening for events on 4 CPUs with 64x4096 of shared memory\nCilium health daemon:   Ok\nIPAM:                   IPv4: 3/254 allocated from 10.0.0.0/24,\nAllocated addresses:\n  10.0.0.11 (kube-system/coredns-6d4b75cb6d-tbgrr)\n  10.0.0.201 (health)\n  10.0.0.78 (router)\nBandwidthManager:       Disabled\nHost Routing:           BPF\nMasquerading:           Disabled\nClock Source for BPF:   ktime\nController Status:      23/23 healthy\n  Name                                  Last success   Last error   Count   Message\n  cilium-health-ep                      29s ago        never        0       no error\n  dns-garbage-collector-job             34s ago        never        0       no error\n  endpoint-1313-regeneration-recovery   never          never        0       no error\n  endpoint-2868-regeneration-recovery   never          never        0       no error\n  endpoint-2945-regeneration-recovery   never          never        0       no error\n  endpoint-gc                           3m34s ago      never        0       no error\n  ipcache-inject-labels                 3m30s ago      3m33s ago    0       no error\n  k8s-heartbeat                         4s ago         never        0       no error\n  link-cache                            15s ago        never        0       no error\n  metricsmap-bpf-prom-sync              4s ago         never        0       no error\n  resolve-identity-1313                 3m29s ago      never        0       no error\n  resolve-identity-2868                 3m29s ago      never        0       no error\n  resolve-identity-2945                 3m30s ago      never        0       no error\n  sync-endpoints-and-host-ips           30s ago        never        0       no error\n  sync-lb-maps-with-k8s-services        3m30s ago      never        0       no error\n  sync-node-with-ciliumnode (fedora)    3m32s ago      3m33s ago    0       no error\n  sync-policymap-1313                   26s ago        never        0       no error\n  sync-policymap-2868                   26s ago        never        0       no error\n  sync-policymap-2945                   26s ago        never        0       no error\n  sync-to-k8s-ciliumendpoint (1313)     9s ago         never        0       no error\n  sync-to-k8s-ciliumendpoint (2868)     9s ago         never        0       no error\n  sync-to-k8s-ciliumendpoint (2945)     0s ago         never        0       no error\n  template-dir-watcher                  never          never        0       no error\nProxy Status:            OK, ip 10.0.0.78, 0 redirects active on ports 10000-20000\nGlobal Identity Range:   min 256, max 65535\nHubble:                  Ok   Current/Max Flows: 377/4095 (9.21%), Flows/s: 1.67   Metrics: Disabled\nKubeProxyReplacement Details:\n  Status:                 Strict\n  Socket LB:              Enabled\n  Socket LB Protocols:    TCP, UDP\n  Devices:                enp1s0 192.168.31.100 (Direct Routing)\n  Mode:                   DSR\n  Backend Selection:      Maglev (Table Size: 16381)\n  Session Affinity:       Enabled\n  Graceful Termination:   Enabled\n  NAT46/64 Support:       Disabled\n  XDP Acceleration:       Disabled\n  Services:\n  - ClusterIP:      Enabled\n  - NodePort:       Enabled (Range: 30000-32767)\n  - LoadBalancer:   Enabled\n  - externalIPs:    Enabled\n  - HostPort:       Enabled\nBPF Maps:   dynamic sizing: on (ratio: 0.002500)\n  Name                          Size\n  Non-TCP connection tracking   65536\n  TCP connection tracking       131072\n  Endpoint policy               65535\n  Events                        4\n  IP cache                      512000\n  IP masquerading agent         16384\n  IPv4 fragmentation            8192\n  IPv4 service                  65536\n  IPv6 service                  65536\n  IPv4 service backend          65536\n  IPv6 service backend          65536\n  IPv4 service reverse NAT      65536\n  IPv6 service reverse NAT      65536\n  Metrics                       1024\n  NAT                           131072\n  Neighbor table                131072\n  Global policy                 16384\n  Per endpoint policy           65536\n  Session affinity              65536\n  Signal                        4\n  Sockmap                       65535\n  Sock reverse NAT              65536\n  Tunnel                        65536\nEncryption:            Disabled\nCluster health:        2/2 reachable    (2022-08-23T01:59:33Z)\n  Name                 IP               Node        Endpoints\n  fedora (localhost)   192.168.31.100   reachable   reachable\n  knode1               192.168.31.101   reachable   reachable\n\ncilium-health status –probe 之后的结果， 多个节点之间的联通性正常：\nroot@fedora:/home/cilium# cilium-health status --probe\nProbe time:   2022-08-23T02:01:04Z\nNodes:\n  fedora (localhost):\n    Host connectivity to 192.168.31.100:\n      ICMP to stack:   OK, RTT=140.825µs\n      HTTP to agent:   OK, RTT=199.54µs\n    Endpoint connectivity to 10.0.0.201:\n      ICMP to stack:   OK, RTT=128.2µs\n      HTTP to agent:   OK, RTT=263.034µs\n  knode1:\n    Host connectivity to 192.168.31.101:\n      ICMP to stack:   OK, RTT=235.981µs\n      HTTP to agent:   OK, RTT=330.706µs\n    Endpoint connectivity to 10.0.1.251:\n      ICMP to stack:   OK, RTT=177.807µs\n      HTTP to agent:   OK, RTT=275.869µs\n\n没有执行Cilium connectivity Test， 这个测试和 1.1.1.1:443 的测试在基本上每次都是失败的。目前原因还不清楚， 没调查，但是集群内的Pod访问外部的网络是完全正常的。 \n基于AWS EC2 测试的结果（无法关闭 Masquerade 以及 绕过 Iptables）， 与flannel相比，Cilium当前的性能还是少少差一点点，但是跨越公网访问的稳定性更强，HTTP完全没有Failed请求。\n没有开启XDP， 原因是我使用的是KVM的虚拟机 ，使用的驱动程序是VirtIO， 看起来这个驱动在我当前版本的虚拟化里面可能需要做一些特殊的设置才能开启， 我没有继续进行测试。\n目前也没法对性能做任何的对比测试，  没有条件启动两个集群做对比，可以想见因为底层的响应时间极快，所以没有办法看出差距； 要么就是资源的抢占导致本身的测试结果不够稳定。\n我目前还在 EC2 以及 EKS 中调试：\n\nEKS：AWS CNI替换掉之后有自己的问题， 不能开启完全的Iptables Bypass， 由于启用了ENI MODE 就会提供 EndpointRoute， 感觉这已经是BypassIptables， 但是没能验证，不确定。\n\nEC2 部署的 Kubernetes： 集群不能开启HostRouting， 开启之后cilium-health status 无法完成对端节点上面Endpoint的检查， Connection Timeout。也不能完全work。\n\n\n其他资料\n其他公司业务的配置以及测试。\nhttps://www.ebpf.top/post/cilium-standalone-L4LB-XDP-zh/\n按照Linux基金会的PPT中描述， KVM 虚拟化条件下也是可以开启XDP的，但是我只是测试， 就不折腾了， 我自己的PC 效果不一定明显。\nhttps://events19.linuxfoundation.cn/wp-content/uploads/2017/11/Accelerating-VM-Networking-through-XDP_Jason-Wang.pdf \nhttps://lore.kernel.org/bpf/b41ab0f0-4537-74b5-d7c3-b20ce082bdd6@redhat.com/T/\nhttps://lwn.net/Articles/841755/\n\n","categories":[],"tags":[]},{"title":"如何配置kubelet的节点自动回收资源","url":"/2022/04/19/Linux/EKS_ConfigureNodeGC/","content":"配置Node节点按照磁盘阈值回收空间\nhttps://aws.amazon.com/cn/premiumsupport/knowledge-center/eks-worker-nodes-image-cache/\n\n\n\n修改Kubelet参数\nKubelet默认提供了GC的参数  --image-gc-high-threshold 参数用于定义触发映像垃圾收集的磁盘使用百分比。默认值为 85%。\n--image-gc-low-threshold 参数用于定义映像垃圾收集尝试释放的磁盘使用百分比。默认值为 80%。  \n如果是自己管理的Node，最好的方式是直接配置kubelet命令行的参数，将上面的参数指定需要的阈值，然后重启kubelet即可。配置文件一般在 ： &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.config\"imageGCHighThresholdPercent\": 70, \n\"imageGCLowThresholdPercent\": 50,  \n\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"由 CPU Steal Time 想到的问题以及验证","url":"/2022/04/19/Linux/Linux_CalcStealtime/","content":"我们在观察虚拟化的时候，可以看到Stealtime增加，大部分时候都是因为虚拟机的超卖， 总结来说st指示了vCPU的繁忙程度。\n进程相关参数的说明https://www.kernel.org/doc/html/latest/scheduler/sched-stats.html\n\nschedstats also adds a new &#x2F;proc&#x2F;&#x2F;schedstat file to include some of the same information on a per-process level. There are three fields in this file correlating for that process to:   1 time spent on the cpu   2 time spent waiting on a runqueue   3 # of timeslices run on this cpu  \n\nhayden@VM-16-6-ubuntu /p/3720475> cat schedstat\n2236062 223986 22\n- 2236062 进程在CPU的时间\n- 223986 进程在CPU调度上面等待的时间\n- 22 在这个CPU运行的时间片数量\n  NOTE: 有一个博客写这个是 上下文交换的次数 ， 和sched 文件中的 nr_switches 数量相同， 不能确定是否正确。\n\n\n\n\n\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"Epoll vs select vs poll vs io_uring","url":"/2022/04/19/Linux/Linux_Epoll/","content":"select方式：使用fd_set结构体告诉内核同时监控那些文件句柄，使用逐个排查方式去检查是否有文件句柄就绪或者超时。该方式有以下缺点：文件句柄数量是有上限的，逐个检查吞吐量低，每次调用都要重复初始化fd_set。poll方式：该方式主要解决了select方式的2个缺点，文件句柄上限问题(链表方式存储)以及重复初始化问题(不同字段标注关注事件和发生事件)，但是逐个去检查文件句柄是否就绪的问题仍然没有解决。epoll方式：该方式可以说是C10K问题的killer，他不去轮询监听所有文件句柄是否已经就绪。epoll只对发生变化的文件句柄感兴趣。其工作机制是，使用”事件”的就绪通知方式，通过epoll_ctl注册文件描述符fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd, epoll_wait便可以收到通知, 并通知应用程序。而且epoll使用一个文件描述符管理多个描述符,将用户进程的文件描述符的事件存放到内核的一个事件表中, 这样数据只需要从内核缓存空间拷贝一次到用户进程地址空间。而且epoll是通过内核与用户空间共享内存方式来实现事件就绪消息传递的，其效率非常高。但是epoll是依赖系统的(Linux)。异步I&#x2F;O以及Windows，该方式在windows上支持很好，这里就不具体介绍啦。\nio_uring 其实是内核5.10之后引进的一种方式，目前还没有应用是使用这个模式的， 但是这个方式大大的减少了应用程序的系统调用次数。 性能有增长。\n","categories":["Linux"],"tags":["Linux"]},{"title":"关于内核Config中的参数 CONFIG_NO_HZ","url":"/2022/04/19/Linux/Linux_KernelParam_NOHZ/","content":"关于Tick， Tickless的研究。\n\n\n\nhttps://www.kernel.org/doc/html/latest/timers/no_hz.html\n\n这几个参数的最终意义都和 Jitter 相关， 设置的参数含义是 ： CPU时钟中断的周期， 如果是 100HZ ， 那么1s的时间内CPU会中断100次。目前最新的内核支持 NOHZ ， 也就是在没有任务的时候处于节能的考虑不进行中断。当有需要运行的业务时还是会正常的触发CPU中断。NOHZ主要的功能时省电， 调整这个参数的意义就是让CPU处在合理的中断次数。过多的中断会导致相关的任务被打断。\n\ngit:&#x2F;&#x2F;git.kernel.org&#x2F;pub&#x2F;scm&#x2F;linux&#x2F;kernel&#x2F;git&#x2F;frederic&#x2F;dynticks-testing.git\n\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"OOM行为","url":"/2022/04/19/Linux/Linux_OOMKiller/","content":"关于OOM行为的思考 以及Kswapd的动作和行为。http://evertrain.blogspot.com/2018/04/oom.html更详细的打分算法见源码  https://github.com/torvalds/linux/blob/master/mm/oom_kill.c\n\n发生之后OOM killer会将kill的信息记录到系统日志&#x2F;var&#x2F;log&#x2F;messages，检索相关信息就能匹配到是否触发。\ngrep 'Out of memory' /var/log/messages\n也可以通过dmesg\ndmesg -Tx | egrep -i 'killed process'\n\n查看分数最高的进程#!/bin/bash\nfor proc in $(find /proc -maxdepth 1 -regex '/proc/[0-9]+'); do\n    printf \"%2d %5d %s\\n\" \\\n        \"$(cat $proc/oom_score)\" \\\n        \"$(basename $proc)\" \\\n        \"$(cat $proc/cmdline | tr '\\0' ' ' | head -c 50)\"\ndone 2>/dev/null | sort -nr | head -n 10\n\n保护措施设置OverCommit只有在OverCommit的时候才会触发OOM， 默认是许可一定程度的OverCommit的。 \nhttps://docs.kernel.org/vm/overcommit-accounting.html\nvm.overcommit_memory 作用是控制OverCommit是否被许可。\n0\n    Heuristic overcommit handling. Obvious overcommits of address space are refused. Used for a typical system. It ensures a seriously wild allocation fails while allowing overcommit to reduce swap usage. root is allowed to allocate slightly more memory in this mode. This is the default.\n1\n    Always overcommit. Appropriate for some scientific applications. Classic example is code using sparse arrays and just relying on the virtual memory consisting almost entirely of zero pages.\n2\n    Don’t overcommit. The total address space commit for the system is not permitted to exceed swap + a configurable amount (default is 50%) of physical RAM. Depending on the amount you use, in most situations this means a process will not be killed while accessing pages but will receive errors on memory allocation as appropriate.\n\n    Useful for applications that want to guarantee their memory allocations will be available in the future without having to initialize every page.\n\n[root@ip-172-31-9-192 log]# cat /proc/meminfo | grep Comm\nCommitLimit:     4794236 kB\nCommitted_AS:    2344744 kB\n---\nCommitLimit： 可提交内存的上限， 超过这个上限系统认为目前内存已经是OverCommit。\nCommitted_AS： 已经提交内存的上限，当前所有进程已经提交的内存使用，这个不是已经分配出去的， 是进程申请的。\n\n设置Killer的行为root@ip-172-31-11-235:/home/ec2-user|⇒  cat /proc/sys/vm/oom_kill_allocating_task\n# 值为0：会 kill 掉得分最高的进程\n# 值为非0：会kill 掉当前申请内存而触发OOM的进程\n\n设置进程\n对于需要保护的进程可以使用OOM_ADJ&#x3D;-17 将这个进程从OOM Killer的列表中移除（已经在内核的2.6之后废弃，处于兼容性保留了这个文件接口\n调整OOM_SCORE_ADJ, 范围是 -1000 &lt; oom_score_adj &lt; 1000\n\n直接调整到-1000，会出现在计算分数列表的最后  echo -1000 > /proc/31595/oom_score_adj\n  手动触发一次OOM规则， Kill符合要求的进程  echo f > /proc/sysrq-trigger\n调整服务的OOM Score对于服务本身的保护方式， 可以采用使用Systemd Unit file里面进行 OOMADJSCORE&#x3D;*** 的方式来指定，例如保护MySQL的进程不会在OOM Killer的列表中。\ncat /usr/lib/systemd/system/mariadb.service\n\n    [Service]\n    Type=simple\n    User=mysql\n    Group=mysql\n\n    ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n\n    ExecStart=/usr/bin/mysqld_safe --basedir=/usr\n    ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID\n    # Setting Here. and setting in the /proc/$PID/oom_score_adj.\n    OOMScoreAdjust=-1000\n\nsudo systemctl daemon-reload &amp;&amp; sudo systemctl restart mariadb\n\n避免OOM的方式\n关闭OverCommit： 关闭OverCommit会导致进程如果无法拿到内存就fast fail ， 不会出现OOMKiller干掉无辜进程的情况。\n开启OverCommit， 开启一定程度的Swap： 开启Swap会导致内存在接近99%的时候，会出现系统响应变慢的问题，但是会由于申请的内存没有超过TotalMEM + TotalSWAP，因此不会触发OOM ，但是会导致明显的系统响应问题。 如果超过了 TotalMEM + TotalSWAP 会立刻触发一次OOMkiller结束进程。\n开启OverCommit， 调整进程的优先级： 对于特定的进程进行保护。OOM会按照设置的积分计算需要Kill的进程。\n开启OverCommit， 调整OOMkiller的行为方式： 计算积分后Kill 或者 直接Kill当前新申请内存的进程。这个方式感觉和默认的关闭OverCommit的行为类型，都是拒绝新的进程以保证旧的进程可以存活。\n\n更可靠的方式Faceboook的oomdhttps://github.com/facebookincubator/oomdoomd使用的是 PSI接口来评估内存的压力，可以通过自定义规则的方式来对来进行内存压力的分析，而不是简单的内存用量。\nFedora的Early OOMhttps://github.com/rfjakob/earlyoomEarlyOOM的作用是提前OOM，这样可以保障用户空间的图形桌面不会到交换空间去， 主要解决的问题是内存压力过大的交换动作会将桌面环境换出导致响应变慢。\n","categories":["Linux"],"tags":["Linux"]},{"title":"Perf 命令的Performance分析","url":"/2022/04/19/Linux/Linux_Perf/","content":"https://blog.gmem.cc/perf  一个非常详细的博客， 太牛逼了。\n使用perf进行性能的简单输出root@ip-172-31-11-235:~|⇒  perf stat htop -d 1\n\n Performance counter stats for 'htop -d 1':\n\n        181.764747      task-clock (msec)         #    0.055 CPUs utilized\n                52      context-switches          #    0.286 K/sec\n                 0      cpu-migrations            #    0.000 K/sec\n               320      page-faults               #    0.002 M/sec\n   &lt;not supported>      cycles\n   &lt;not supported>      instructions\n   &lt;not supported>      branches\n   &lt;not supported>      branch-misses\n\n       3.283236218 seconds time elapsed\n\n\n\n使用perf记录性能指标到文件[root@ip-172-31-41-141 tmp]# perf record -F 99 -a -g -p 44551\n[root@ip-172-31-41-141 tmp]# sudo perf record -F 99 -a -g -- sleep 60\nWarning:\nPID/TID switch overriding SYSTEM\n^C[ perf record: Woken up 1 times to write data ]\n[ perf record: Captured and wrote 0.021 MB perf.data (2 samples) ]\nroot@ip-172-31-11-235:~|⇒  sudo perf script > out.perf\n\n\n生成火焰图通常的做法是将 out.perf 拷贝到本地机器，在本地生成火焰图：\n$ git clone --depth 1 https://github.com/brendangregg/FlameGraph.git\n# 折叠调用栈\n$ perf script > out.perf\n$ FlameGraph/stackcollapse-perf.pl out.perf > out.folded\n# 生成火焰图\n$ FlameGraph/flamegraph.pl out.folded > out.svg\n生成火焰图可以指定参数，–width 可以指定图片宽度，–height 指定每一个调用栈的高度，生成的火焰图，宽度越大就表示CPU耗时越多。FlameGraph&#x2F;flamegraph.pl &lt; out.profile &gt; out.svg\n[root@ip-172-31-18-198 timechart]# perf timechart record -g – curl http://localhost:19999[root@ip-172-31-18-198 timechart]# perf timechartWritten 0.0 seconds of trace to output.svg.\n制造一个D进程Most proper way is to use freezer cgroup. It puts process to uninterruptible sleep in case of FROZEN cgroup state.\nmkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezermount -t cgroup -ofreezer freezer &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezermkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozenecho FROZEN &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;freezer.stateecho pidof you_process &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;tasksecho pgrep cp &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;tasksecho THAWED &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;freezer.state\nTo put again to interruptible sleep, just change cgroup state to THAWED.\n动态追踪\n添加一个动态追踪的Tracepoint Eventperf probe --add&#x3D;&quot;probe:io_schedule_timeout&quot;\nperf probe --add&#x3D;&quot;probe:io_schedule_timeout%return&quot;\n# 使用\nperf record -e probe:tcp_sendmsg -a -g -- sleep 5\n# 分析\nperf report --stdio\n移除一个动态追踪的Tracepoint Eventperf probe --del&#x3D;&quot;probe:io_schedule_timeout&quot;\nperf probe -d &quot;probe:io_schedule_timeout&quot;\n列出所有存在的probe perf probe -l\n查看追踪的结果perf script\nperf probe -V tcp_sendmsg # 列出可用的变量列表\nperf probe --add &#39;tcp_sendmsg size&#39; # 追踪这个变量\n# Add a tracepoint for tcp_sendmsg() return, and capture the return value:\nperf probe &#39;tcp_sendmsg%return $retval&#39;\n\n关于Off-cpu进程的分析\n按步骤生成]$ &#x2F;usr&#x2F;share&#x2F;bcc&#x2F;tools&#x2F;offcputime -df -p &#96;pgrep -nx mysqld&#96; 30 &gt; out.stacks\n[...copy out.stacks to your local system if desired...]\n]$ git clone https:&#x2F;&#x2F;github.com&#x2F;brendangregg&#x2F;FlameGraph\n]$ cd FlameGraph\n]$ .&#x2F;flamegraph.pl --color&#x3D;io --title&#x3D;&quot;Off-CPU Time Flame Graph&quot; --countname&#x3D;us &lt; out.stacks &gt; out.svg\n一条命令出图 ]$ grep do_command &lt; out.stacks | .&#x2F;flamegraph.pl --color&#x3D;io --title&#x3D;&quot;Off-CPU Time Flame Graph&quot; --countname&#x3D;us &gt; out.svg\n\nperf命令的常见参数\n内核设置要启用内核动态追踪，需要使用内核编译参数CONFIG_KPROBES&#x3D;y、CONFIG_KPROBE_EVENTS&#x3D;y。要追踪基于帧指针的内核栈，需要内核编译参数CONFIG_FRAME_POINTER&#x3D;y。要启用用户动态追踪，需要使用内核编译参数CONFIG_UPROBES&#x3D;y、CONFIG_UPROBE_EVENTS&#x3D;y\n子命令列表\n\nperf支持一系列的子命令：子命令 \t说明annotate \t读取perf.data并显示被注解的代码bench \t基准测试的框架config \t在配置文件中读写配置项diff \t读取perf.data并显示剖析差异evlist \t列出perf.data中的事件名称inject \t用于增强事件流的过滤器kmem \t跟踪&#x2F;度量内核内存属性kvm \t跟踪&#x2F;度量KVM客户机系统list \t显示符号化的事件列表lock \t分析锁事件mem \t分析内存访问record \t执行剖析report \t显示剖析结果sched \t分析调度器stat \t获取性能计数top \t显示成本最高的操作并动态刷新trace \t类似于strace的工具probe \t定义新的动态追踪点\n\nperrf record 命令参数 --exclude-perf \t不记录perf自己发起的事件\n-p \t收集指定进程的事件，逗号分割的PID列表\n-a \t使用Per-CPU模式，如果不指定-C，则相当于全局模式。如果指定-C，则可以选定若干CPU\n-g \t记录调用栈\n-F \t以指定的频率剖析\n-T \t记录样本时间戳\n-s \t记录每个线程的事件计数器，配合 perf report -T使用\n\n微基准测试From youtube video： \n&#96;&#96;&#96;bashroot@HaydenArchDesktop &#x2F;tmp# perf bench sched pipe\nRunning ‘sched&#x2F;pipe’ benchmark:Executed 1000000 pipe operations between two processes Total time: 2.407 [sec]\n\n   2.407455 usecs/op\n     415376 ops/sec\n\nroot@HaydenArchDesktop &#x2F;tmp# taskset -c 0 perf bench sched pipe\nRunning ‘sched&#x2F;pipe’ benchmark:Executed 1000000 pipe operations between two processes Total time: 2.381 [sec]\n\n   2.381081 usecs/op\n     419977 ops/sec\n\n这里的时间提升不明显的原因是， 我的Archlinux是ZenKernel， 感觉可能在调度上已经做了不少的事情 ，如果随便启动一个redhat , 这个指标的差距会比较大。","categories":["Linux"],"tags":["Linux"]},{"title":"Sysctl 云平台参数的收集以及一部分解释","url":"/2022/04/19/Linux/Linux_Sysctl/","content":"一些关于sysctl参数设置的收集和解释。\n\n\n&#x2F;etc&#x2F;sysctl.d&#x2F;00-defaults.confkernel.printk输出内核日志信息的级别。\n# 映射到的proc文件系统位置 - /proc/sys/kernel/printk\n# Maximize console logging level for kernel printk messages\nkernel.printk = 8 4 1 7\n\n# (1) 控制台日志级别：优先级高于该值的消息将被打印至控制台。\n# (2) 缺省的消息日志级别：将用该值来打印没有优先级的消息。\n# (3) 最低的控制台日志级别：控制台日志级别可能被设置的最小值。\n# (4) 缺省的控制台：控制台日志级别的缺省值。\n内核中共提供了八种不同的日志级别，在 linux&#x2F;kernel.h 中有相应的宏对应。\n#define KERN_EMERG  &quot;&lt;0&gt;&quot;   &#x2F;* systemis unusable *&#x2F;\n#define KERN_ALERT  &quot;&lt;1&gt;&quot;   &#x2F;* actionmust be taken immediately *&#x2F;\n#define KERN_CRIT    &quot;&lt;2&gt;&quot;   &#x2F;*critical conditions *&#x2F;\n#define KERN_ERR     &quot;&lt;3&gt;&quot;   &#x2F;* errorconditions *&#x2F;\n#define KERN_WARNING &quot;&lt;4&gt;&quot;   &#x2F;* warning conditions *&#x2F;\n#define KERN_NOTICE  &quot;&lt;5&gt;&quot;   &#x2F;* normalbut significant *&#x2F;\n#define KERN_INFO    &quot;&lt;6&gt;&quot;   &#x2F;*informational *&#x2F;\n#define KERN_DEBUG   &quot;&lt;7&gt;&quot;   &#x2F;*debug-level messages *&#x2F;\nkernel.panic设置内核的Panic之后自动重启\n# Wait 30 seconds and then reboot\nkernel.panic = 30\nneigh.default.gc设置arp缓存相关的参数https://zhuanlan.zhihu.com/p/94413312\n# Allow neighbor cache entries to expire even when the cache is not full\nnet.ipv4.neigh.default.gc_thresh1 = 0\nnet.ipv6.neigh.default.gc_thresh1 = 0\n\n# Avoid neighbor table contention in large subnets\nnet.ipv4.neigh.default.gc_thresh2 = 15360\nnet.ipv6.neigh.default.gc_thresh2 = 15360\nnet.ipv4.neigh.default.gc_thresh3 = 16384\nnet.ipv6.neigh.default.gc_thresh3 = 16384\n\n# gc_thresh1\n存在于ARP高速缓存中的最少层数，如果少于这个数，\n垃圾收集器将不会运行。\n缺省值是128。\n# gc_thresh2\n保存在 ARP 高速缓存中的最多的记录软限制。\n垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。\n缺省值是 512。\n# gc_thresh3\n保存在 ARP 高速缓存中的最多记录的硬限制，\n一旦高速缓存中的数目高于此，\n垃圾收集器将马上运行。\n缺省值是1024。\n\n&#x2F;etc&#x2F;sysctl.d&#x2F;99-amazon.confsched_autogroup_enabled通过CFS分组提高了桌面环境的性能表现。这个小小的补丁仅为 Linux Kernel 增加了 233 行代码，却将高负荷下桌面响应最大延迟降低到原先的十分之一，平均延迟降低到六十分之一！该补丁的作用是为每个 TTY 动态地创建任务分组。(https://linuxtoy.org/archives/small-patch-but-huge-improvement.html)\n# https://cateee.net/lkddb/web-lkddb/SCHED_AUTOGROUP.html\n# https://www.postgresql.org/message-id/50E4AAB1.9040902@optionshouse.com\n# This setting enables better interactivity for desktop workloads and not\n# suitable for many server workloads.\n# 启用后，内核会创建任务组来优化桌面程序的调度。它将把占用大量资源的应用程序放在它们自己的任务组，根据PostgreSQL的测试， 关闭这个选项会将数据库的性能提高30%（上面的Link）。 在后台的服务进程中是提高性能的选项。\n# 0：禁止\n# 1：开启\n\nkernel.sched_autogroup_enabled=0\n\n&#x2F;usr&#x2F;lib&#x2F;sysctl.d&#x2F;00-system.confbridge-nf-call-iptables网桥设备关闭netfilter模块，开关需要按需求来指定。关闭这个模块会在网桥2层可以转发的时候直接转发， 不会走三层进行数据传输，也就是说不会过Iptables。Kubernetes需要开启这个参数的原因是： https://imroc.cc/post/202105/why-enable-bridge-nf-call-iptables/， 修复了Coredns不定期解析失败的问题。\n# Disable netfilter on bridges.\nnet.bridge.bridge-nf-call-ip6tables = 0\nnet.bridge.bridge-nf-call-iptables = 0\nnet.bridge.bridge-nf-call-arptables = 0\n\n&#x2F;usr&#x2F;lib&#x2F;sysctl.d&#x2F;10-default-yama-scope.confyamaYama is a Linux Security Module that collects system-wide DAC security protections that are not handled by the core kernel itself. This is selectable at build-time with CONFIG_SECURITY_YAMA, and can be controlled at run-time through sysctls in &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;yama  \nhttps://www.kernel.org/doc/html/latest/admin-guide/LSM/Yama.html \n[root@ip-172-31-11-235 sysctl.d]$ cat 10-default-yama-scope.conf\n# When yama is enabled in the kernel it might be used to filter any user\n# space access which requires PTRACE_MODE_ATTACH like ptrace attach, access\n# to /proc/PID/&#123;mem,personality,stack,syscall&#125;, and the syscalls\n# process_vm_readv and process_vm_writev which are used for interprocess\n# services, communication and introspection (like synchronisation, signaling,\n# debugging, tracing and profiling) of processes.\n#\n# Usage of ptrace attach is restricted by normal user permissions. Normal\n# unprivileged processes cannot interact through ptrace with processes\n# that they cannot send signals to or processes that are running set-uid\n# or set-gid.\n#\n# yama ptrace scope can be used to reduce these permissions even more.\n# This should normally not be done because it will break various programs\n# relying on the default ptrace security restrictions. But can be used\n# if you don't have any other way to separate processes in their own\n# domains. A different way to restrict ptrace is to set the selinux\n# deny_ptrace boolean. Both mechanisms will break some programs relying\n# on the ptrace system call and might force users to elevate their\n# priviliges to root to do their work.\n#\n# For more information see Documentation/security/Yama.txt in the kernel\n# sources. Which also describes the defaults when CONFIG_SECURITY_YAMA\n# is enabled in a kernel build (currently 1 for ptrace_scope).\n#\n# This runtime kernel parameter can be set to the following options:\n# (Note that setting this to anything except zero will break programs!)\n#\n# 0 - Default attach security permissions.\n# 1 - Restricted attach. Only child processes plus normal permissions.\n# 2 - Admin-only attach. Only executables with CAP_SYS_PTRACE.\n# 3 - No attach. No process may call ptrace at all. Irrevocable.\n#\nkernel.yama.ptrace_scope = 0\n\n&#x2F;usr&#x2F;lib&#x2F;sysctl.d&#x2F;50-default.confsysrq是一个Magic Key的设置，无论内核当前在做什么都会立刻响应这个MagicKey。\n\n#  This file is part of systemd.\n#\n#  systemd is free software; you can redistribute it and/or modify it\n#  under the terms of the GNU Lesser General Public License as published by\n#  the Free Software Foundation; either version 2.1 of the License, or\n#  (at your option) any later version.\n\n# See sysctl.d(5) and core(5) for for documentation.\n\n# To override settings in this file, create a local file in /etc\n# (e.g. /etc/sysctl.d/90-override.conf), and put any assignments\n# there.\n\n# System Request functionality of the kernel (SYNC)\n#\n# Use kernel.sysrq = 1 to allow all keys.\n# See http://fedoraproject.org/wiki/QA/Sysrq for a list of values and keys.\nkernel.sysrq = 16\n\nWhen running a kernel with SysRq compiled in, /proc/sys/kernel/sysrq controls the functions allowed to be invoked via the SysRq key. Here is the list of possible values in /proc/sys/kernel/sysrq:\n\n    0 - disable sysrq completely\n    1 - enable all functions of sysrq\n    >1 - bitmask of allowed sysrq functions (see below for detailed function description):\n        2 - enable control of console logging level\n        4 - enable control of keyboard (SAK, unraw)\n        8 - enable debugging dumps of processes etc.\n        16 - enable sync command\n        32 - enable remount read-only\n        64 - enable signalling of processes (term, kill, oom-kill)\n        128 - allow reboot/poweroff\n        256 - allow nicing of all RT tasks\n# 触发的方式\n- echo t > /proc/sysrq-trigger\n- You press the key combo Alt+SysRq+&lt;command key>.\n\ncore_uses_pid# Append the PID to the core filename\n# DEBUG相关的功能，如果这个文件的内容被配置成1，即使core_pattern中没有设置%p，最后生成的core dump文件名仍会加上进程ID。 \nkernel.core_uses_pid = 1\n\nkptr_restrict# 查看内核地址符号的命令： cat /proc/kallsyms\n# https://bugzilla.redhat.com/show_bug.cgi?id=1689344\nkernel.kptr_restrict = 1\n# 0 所有用户都可以查看内核的地址空间输出信息\n# 1 只有root用户可以，其他用户看到的都是0\n# 2 所有的用户都不可以，输出的结果都是0\n\n# Other Link: https://blog.csdn.net/qq1602382784/article/details/80066847\n\nrp_filterrp_filter参数用于控制系统是否开启对数据包源地址的校验。\n# Source route verification\nnet.ipv4.conf.default.rp_filter = 1\nnet.ipv4.conf.all.rp_filter = 1\n两个参数之中有一个是1， 那么这个功能就是启用的\n- 0：不开启源地址校验。\n- 1：开启严格的反向路径校验。对每个进来的数据包，校验其反向路径是否是最佳路径。如果反向路径不是最佳路径，则直接丢弃该数据包。\n- 2：开启松散的反向路径校验。对每个进来的数据包，校验其源地址是否可达，即反向路径是否能通（通过任意网口），如果反向路径不同，则直接丢弃该数据包。\n\naccept_source_route指允许数据包的发送者指定数据包的发送路径，以及返回给发送者的数据包所走的路径。\n# Do not accept source routing\nnet.ipv4.conf.default.accept_source_route = 0\nnet.ipv4.conf.all.accept_source_route = 0\n\npromote_secondariesdown掉所属某个子网的primary ip的时候， 所有相关的secondary ip也会down掉,启用的时候，当primary ip宕掉时可以将secondary ip提升为primary ip。 \n# Promote secondary addresses when the primary address is removed\nnet.ipv4.conf.default.promote_secondaries = 1\nnet.ipv4.conf.all.promote_secondaries = 1\n\nprotected_linksfs.protected_hardlinks \t用于限制普通用户建立硬链接  \n\n0：不限制用户建立硬链接  \n1：限制，如果文件不属于用户，或者用户对此用户没有读写权限，则不能建立硬链接\n\nfs.protected_symlinks \t用于限制普通用户建立软链接  \n\n0：不限制用户建立软链接  \n1：限制，允许用户建立软连接的情况是 软连接所在目录是全局可读写目录或者软连接的uid与跟从者的uid匹配，又或者目录所有者与软连接所有者匹配  # Enable hard and soft link protection\nfs.protected_hardlinks = 1\nfs.protected_symlinks = 1\n\n&#x2F;usr&#x2F;lib&#x2F;sysctl.d&#x2F;9-ipv6.conf关于IPv6地址重复的问题。In case DAD does find a duplicate address, the address we tried to set is deleted (and in the syslog you see a message like this: “eth0: duplicate address detected!”). In such a case, a sysadmin needs to configure an address manually.\nnet.ipv6.conf.all.accept_dad=0\nnet.ipv6.conf.default.accept_dad=0\nnet.ipv6.conf.eth0.accept_dad=0\n\n\n\n\n\nCase Update\ntcp_tw_recycle（Before 4.12 available）net.ipv4.tcp_timestamps 这个参数开启的时候， 才会生效，可以快速回收处于TIME_WAIT状态的socket。当tcp_tw_recycle开启时（tcp_timestamps同时开启，快速回收socket的效果达到），对于位于NAT设备后面的Client来说，是一场灾难。https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=4396e46187ca5070219b81773c4e65088dac50cc这个参数在2017年之后的内核已经不存在了， 开发者的Blog https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux内核彻底移除这个参数的版本是 4.12 ， 如果内核的版本是4.12 之后，这个问题就不存在了。对于NAT后端的多个实例之间，如果有时间的差距会导致另一个实例收到的报文的Timestamp是之前一个时间点的，这个时候会认为这个请求是来自之前的某个请求的返回。 然而其实是因为时间的原因， 并且这个请求是一个新的请求，由于时间的原因被识别成了旧的请求。这个配置对 Incomming 和 Outgoing的tcp连接都会生效。由于网络的设计是默认不相信可靠的， 所以在TCP的最后进行等待，这样会导致大量的TIME_WAIT， 而这些TIME_WAIT其实是已经不用的， 但是由于无法获得ACK因此无法释放。\n\ntcp_tw_reuse http://lxr.linux.no/#linux+v3.2.8/Documentation/networking/ip-sysctl.txt#L464  这个配置只是对客户端生效。也就是说， 客户端发起了FIN之后， 只要客户端收到服务端的FIN， 后续的网络报文已经不再继续关注了。 这样客户端的TIME_WAIT就会下降， 提高了客户端的Timewait的利用率，减少了客户端TW连接的数量。可以提高客户端的并发请求数量。 这个配置参数只对 Outgoing 的参数生效。 NOTE： 现在这个选项支持3个参数，0 关闭， 1 开启 ，2 只对回环开启。\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"SSH转发X11","url":"/2022/04/19/Linux/Linux_X11Forward/","content":"记录转发的配置和步骤\n\n\nssh转发X11sudo yum install xorg-x11-xauth xorg-x11-fonts-* xorg-x11-font-utils xorg-x11-fonts-Type1 xclock\n\nvim /etc/ssh/sshd_config\n  X11Forwarding yes\n  X11UseLocalhost no\n运行图形化的命令 查看mobexterm是否已经正常的启动了一个临时的窗口来进行数据的转发即可 。\nssh 转发连接到远端的数据ssh -L local-port:target-host:target-port tunnel-host","categories":["Linux"],"tags":["Linux"]},{"title":"关于BTRFS的一些测试","url":"/2022/04/19/Linux/Linux_btrfs/","content":"创建BTRFS卷mkfs.btrfs -d single -m raid1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 \n\n\n\n更改btrfs的元数据冗余  这里最好的办法当然是创建的时候就规划和指定好。    btrfs balance start -dconvert&#x3D;raid1 -mconvert&#x3D;raid1 &#x2F;mnt\n创建轻量副本文件cp --reflink source dest \n\n记录测试结果：\n\n如果是 -d raid0 -m raid1 可以直接将三个EBS IO1 3000IOPS的卷吃满， 直接到9000\n如果是 -d raid1 -m raid1 只能达到3000IOPS， 但是容量会有冗余。\n\nrandread[global]\ndirectory=/mnt\nioengine=libaio\ndirect=1\nrw=randread\nbs=16M\nsize=64M\ntime_based\nruntime=20\ngroup_reporting\nnorandommap\nnumjobs=1\nthread\n\n[job1]\niodepth=2\n\nresult[root@ip-172-31-10-64 fio]# fio ./job1\njob1: (g=0): rw=randread, bs=16M-16M/16M-16M/16M-16M, ioengine=libaio, iodepth=2\nfio-2.14\nStarting 1 thread\njob1: Laying out IO file(s) (1 file(s) / 64MB)\nJobs: 1 (f=1): [r(1)] [100.0% done] [256.0MB/0KB/0KB /s] [16/0/0 iops] [eta 00m:00s]\njob1: (groupid=0, jobs=1): err= 0: pid=26359: Thu Nov 18 07:59:17 2021\n  read : io=5232.0MB, bw=266745KB/s, iops=16, runt= 20085msec\n    slat (msec): min=1, max=70, avg=21.39, stdev=23.43\n    clat (msec): min=2, max=130, avg=101.29, stdev=31.01\n     lat (msec): min=8, max=139, avg=122.68, stdev=24.96\n    clat percentiles (msec):\n     |  1.00th=[    7],  5.00th=[   60], 10.00th=[   63], 20.00th=[   66],\n     | 30.00th=[   93], 40.00th=[  110], 50.00th=[  118], 60.00th=[  120],\n     | 70.00th=[  122], 80.00th=[  126], 90.00th=[  127], 95.00th=[  128],\n     | 99.00th=[  130], 99.50th=[  131], 99.90th=[  131], 99.95th=[  131],\n     | 99.99th=[  131]\n    lat (msec) : 4=0.31%, 10=3.36%, 20=0.92%, 50=0.31%, 100=28.75%\n    lat (msec) : 250=66.36%\n  cpu          : usr=0.04%, sys=3.80%, ctx=994, majf=0, minf=8193\n  IO depths    : 1=0.3%, 2=99.7%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     issued    : total=r=327/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=2\n\nRun status group 0 (all jobs):\n   READ: io=5232.0MB, aggrb=266744KB/s, minb=266744KB/s, maxb=266744KB/s, mint=20085msec, maxt=20085msec\n\nrandwrite[global]\ndirectory=/mnt\nioengine=libaio\ndirect=1\nrw=randread\nbs=16M\nsize=64M\ntime_based\nruntime=20\ngroup_reporting\nnorandommap\nnumjobs=1\nthread\n\n[job1]\niodepth=2\nresult[root@ip-172-31-10-64 fio]# fio ./job1\njob1: (g=0): rw=randwrite, bs=16M-16M/16M-16M/16M-16M, ioengine=libaio, iodepth=2\nfio-2.14\nStarting 1 thread\nJobs: 1 (f=1): [w(1)] [100.0% done] [0KB/256.0MB/0KB /s] [0/16/0 iops] [eta 00m:00s]\njob1: (groupid=0, jobs=1): err= 0: pid=26385: Thu Nov 18 08:00:56 2021\n  write: io=5248.0MB, bw=267987KB/s, iops=16, runt= 20053msec\n    slat (msec): min=1, max=67, avg=12.64, stdev=21.62\n    clat (msec): min=12, max=141, avg=109.45, stdev=31.98\n     lat (msec): min=18, max=142, avg=122.10, stdev=25.25\n    clat percentiles (msec):\n     |  1.00th=[   16],  5.00th=[   23], 10.00th=[   66], 20.00th=[   72],\n     | 30.00th=[  120], 40.00th=[  124], 50.00th=[  126], 60.00th=[  127],\n     | 70.00th=[  128], 80.00th=[  130], 90.00th=[  133], 95.00th=[  135],\n     | 99.00th=[  139], 99.50th=[  139], 99.90th=[  141], 99.95th=[  141],\n     | 99.99th=[  141]\n    lat (msec) : 20=4.57%, 50=0.91%, 100=19.21%, 250=75.30%\n  cpu          : usr=1.51%, sys=0.82%, ctx=720, majf=0, minf=1\n  IO depths    : 1=0.3%, 2=99.7%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     issued    : total=r=0/w=328/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=2\n\nRun status group 0 (all jobs):\n  WRITE: io=5248.0MB, aggrb=267987KB/s, minb=267987KB/s, maxb=267987KB/s, mint=20053msec, maxt=20053msec\n\nxfs 单独磁盘Randwrite[root@ip-172-31-10-64 fio]# fio ./job1\njob1: (g=0): rw=randwrite, bs=16M-16M/16M-16M/16M-16M, ioengine=libaio, iodepth=2\nfio-2.14\nStarting 1 thread\njob1: Laying out IO file(s) (1 file(s) / 64MB)\nJobs: 1 (f=1): [w(1)] [100.0% done] [0KB/128.0MB/0KB /s] [0/8/0 iops] [eta 00m:00s]\njob1: (groupid=0, jobs=1): err= 0: pid=26541: Thu Nov 18 08:04:01 2021\n  write: io=2704.0MB, bw=137688KB/s, iops=8, runt= 20110msec\n    slat (msec): min=12, max=130, avg=118.63, stdev=24.61\n    clat (msec): min=12, max=130, avg=118.91, stdev=23.73\n     lat (msec): min=34, max=255, avg=237.54, stdev=47.53\n    clat percentiles (msec):\n     |  1.00th=[   14],  5.00th=[   32], 10.00th=[  125], 20.00th=[  125],\n     | 30.00th=[  125], 40.00th=[  125], 50.00th=[  125], 60.00th=[  126],\n     | 70.00th=[  126], 80.00th=[  126], 90.00th=[  126], 95.00th=[  126],\n     | 99.00th=[  129], 99.50th=[  131], 99.90th=[  131], 99.95th=[  131],\n     | 99.99th=[  131]\n    lat (msec) : 20=1.78%, 50=3.55%, 100=0.59%, 250=94.08%\n  cpu          : usr=0.74%, sys=0.49%, ctx=2132, majf=0, minf=1\n  IO depths    : 1=0.6%, 2=99.4%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     issued    : total=r=0/w=169/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=2\n\nRun status group 0 (all jobs):\n  WRITE: io=2704.0MB, aggrb=137687KB/s, minb=137687KB/s, maxb=137687KB/s, mint=20110msec, maxt=20110msec\n\nDisk stats (read/write):\n  nvme4n1: ios=0/10699, merge=0/0, ticks=0/720588, in_queue=702508, util=99.40%\nRandread[root@ip-172-31-10-64 fio]# fio ./job1\njob1: (g=0): rw=randread, bs=16M-16M/16M-16M/16M-16M, ioengine=libaio, iodepth=2\nfio-2.14\nStarting 1 thread\nJobs: 1 (f=1): [r(1)] [100.0% done] [128.0MB/0KB/0KB /s] [8/0/0 iops] [eta 00m:00s]\njob1: (groupid=0, jobs=1): err= 0: pid=26570: Thu Nov 18 08:05:27 2021\n  read : io=2704.0MB, bw=137694KB/s, iops=8, runt= 20109msec\n    slat (msec): min=9, max=165, avg=118.64, stdev=25.59\n    clat (msec): min=14, max=165, avg=118.93, stdev=24.65\n     lat (msec): min=25, max=288, avg=237.57, stdev=46.62\n    clat percentiles (msec):\n     |  1.00th=[   16],  5.00th=[   57], 10.00th=[  123], 20.00th=[  123],\n     | 30.00th=[  124], 40.00th=[  124], 50.00th=[  124], 60.00th=[  124],\n     | 70.00th=[  129], 80.00th=[  129], 90.00th=[  129], 95.00th=[  129],\n     | 99.00th=[  131], 99.50th=[  165], 99.90th=[  165], 99.95th=[  165],\n     | 99.99th=[  165]\n    lat (msec) : 20=4.73%, 100=1.78%, 250=93.49%\n  cpu          : usr=0.00%, sys=0.59%, ctx=3558, majf=0, minf=8193\n  IO depths    : 1=0.6%, 2=99.4%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%\n     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%\n     issued    : total=r=169/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0\n     latency   : target=0, window=0, percentile=100.00%, depth=2\n\nRun status group 0 (all jobs):\n   READ: io=2704.0MB, aggrb=137694KB/s, minb=137694KB/s, maxb=137694KB/s, mint=20109msec, maxt=20109msec\n\nDisk stats (read/write):\n  nvme4n1: ios=10711/1, merge=0/0, ticks=598336/56, in_queue=579492, util=99.49%\n\nslat 表示submission latency，也就是发送这个IO给内核处理的提交时间花费。  （请求抵达内核的时间）clat 是提交IO请求给内核之后到IO完成之间的时间，不包括submission latency。 （内核到块设备 ——&gt; 请求处理完成的时间）bw Bandwidth  带宽READ: 读取的速率\n","categories":["Linux"],"tags":["Linux"]},{"title":"Fio 命令使用的说明","url":"/2022/04/19/Linux/Linux_fio_blktrace/","content":"Fio 一些测试和思考， fio ，blktrace等等。\n\n\ngithub地址  https://github.com/axboe/fio  https://tobert.github.io/post/2014-04-17-fio-output-explained.html  http://xiaqunfeng.cc/2017/07/12/fio-test-ceph/  https://fio.readthedocs.io/en/latest/fio_doc.html  http://linuxperf.com/?p=156\n命令和说明# fio test file defination\n# ini file format \n ; -- start job file --\n[global]\nsize=1G # 设置测试文件的大小\nruntime=30 # 设定运行时间， 如果运行时间之内已经完成了文件大小的写入则会保持文件大小和负载继续写。\nbs=4k # 块大小\nnumjobs=1 # 定义测试的进程数量 默认是1\ndirect=1 # 是否绕过操作系统的Buffer缓冲区， 1 Enable\nioengine=libaio # io引擎使用libaio模式, 查看可用IO引擎的命令 \ngroup_reporting # 汇总信息显示\nthread # 使用单进程多线程的模型， 默认是使用多进程模型。\ntime_based # 用运行时间为基准， 如果时间没有到达指定的值就继续执行相同的操作， 直接到时间满足要求。\n\n[job1]\nrw=write\n\n[job2]\nrw=read\n\n[job3]\nrw=randwrite\n\n[job4]\nrw=randread\n\n[job5]\nrw=randrw\n; -- end job file --\n\n关于硬盘性能 iostat首先， 在man iostat的时候已经明确的提示了，iostat 的 svctm （也就是硬盘的servicetime）是一个不准确的值， 会在后续的版本中移除， 因为他所依赖的数据来源（&#x2F;proc&#x2F;diskstats）中，是从Block Level 出发来进行计算的， 所以svctm其实并不是IO控制器所需要的准确时间， 那么出现了两个问题， 我们能观察的数据是那个？ 以及背后的含义是什么?  \n调度算法 和 读写请求的合并内核会对IO的请求进行合并， 但是这个合并不是一直存在的， 反映到IOstat的指标是 rrqm&#x2F;s &amp; wrqm&#x2F;s,  这两个表示对于硬盘的读写请求中， 每秒合并的请求数量； 如果你去查看IO的调度算法， none 算法是完全不合并的， 所以这两列一直都是0.  \n队列的等待时间iostat中的await一列， 表示请求的等待时间，正常的情况下应该比较低，那么究竟什么情况下才是性能不佳的表现？await 每个I&#x2F;O的平均耗时是用await表示(blktrace command results: Q2C – 整个IO请求所消耗的时间(Q2I + I2D + D2C &#x3D; Q2C)，相当于iostat的await。)，包括了 IO请求在Kernel中的等待时间 + IO请求从内核出发处理完成回到内核的时间，也就是IO time + Service Time。await 一般情况下应该小于10ms ， 如果没有或者比较大的情况下 ， 应该考虑负载的类型， 来衡量硬盘的负载水平。  \nblktrace一个I&#x2F;O请求进入block layer之后，可能会经历下面的过程：\nQ Remap: 可能被DM(Device Mapper)或MD(Multiple Device, Software RAID) remap到其它设备\nG Split: 可能会因为I/O请求与扇区边界未对齐、或者size太大而被分拆(split)成多个物理I/O\nI Merge: 可能会因为与其它I/O请求的物理位置相邻而合并(merge)成一个I/O\nD 被IO Scheduler依照调度策略发送给driver\nC 被driver提交给硬件，经过HBA、电缆（光纤、网线等）、交换机（SAN或网络）、最后到达存储设备，设备完成IO请求之后再把结果发回。\n\n常见的状态切换 Q–G–I–D–C \n259,2    0        2     0.000001080  7134  Q   W 756856 + 800 [dd]\n259,2    0        3     0.000004247  7134  X   W 756856 / 757368 [dd]\n259,2    0        4     0.000005806  7134  G   W 756856 + 512 [dd]\n259,2    0        5     0.000008792  7134  I   W 756856 + 512 [dd]\n259,2    0        6     0.000012844  7134  G   W 757368 + 288 [dd]\n259,2    0        7     0.000013202  7134  I   W 757368 + 288 [dd]\n259,2    0        8     0.000031708  1830  D   W 756856 + 512 [kworker/0:1H]\n259,2    0        9     0.000034250  1830  D   W 757368 + 288 [kworker/0:1H]\n259,2    0       10     0.001598439  7135  C   W 756856 + 512 [0]\n259,2    0       11     0.001689880  7135  C   W 757368 + 288 [0]\n# 主，从设备号 ， 起始Sector 为0 ， 写几个， 时间 ， pid ， 状态 ， R/W ， 未知\n\n\n保留blktrace的结果为bin文件\nblktrace -d /dev/sdb\n\n使用blkparse 分析已经有的记录\n# 记录性能数据 到 文件。\nroot@ip-172-31-11-235:/home/ec2-user|⇒  blktrace -d /dev/nvme0n1p1\n^C=== nvme0n1p1 ===\n  CPU  0:                    1 events,        1 KiB data\n  CPU  1:                    0 events,        0 KiB data\n  Total:                     1 events (dropped 0),        1 KiB data\n# 每个cpu设备每个设备存储一个单独的文件。\nroot@ip-172-31-11-235:/home/ec2-user|⇒  ll\ntotal 4.0K\n-rw-r--r-- 1 root root 56 Nov  4 17:55 nvme0n1p1.blktrace.0\n-rw-r--r-- 1 root root  0 Nov  4 17:55 nvme0n1p1.blktrace.1\n\nroot@ip-172-31-11-235:/home/ec2-user|⇒  blkparse -i nvme0n1p1.blktrace.0\n# 格式化分析的数据为bin。\nroot@ip-172-31-11-235:/home/ec2-user|⇒  blkparse -i nvme0n1p1 -d nvme0n1p1.blktrace.bin\n# 使用一个简易的图形方式分析结果。\nroot@ip-172-31-11-235:/home/ec2-user|⇒  btt -i nvme0n1p1.blktrace.bin\n\n\n历史背景\nmdadm 已经不怎么更新和开发了默认推荐使用LVMLVM 和 mdadm 在操作系统都是使用的Raid驱动（内核模块）。\n其实也是可以使用btrfs ， 这个测试的结果是 btrfs 的性能确实是比LVM更好。\n\n相关的问题如果说有一个性能的问题， IOPS达不到指定的数值， 思路？首先查看队列深度是不是足够，看svctm时间长不长，看队列的长度 \niostat命令的理解iostat -xkt 1rrqm&#x2F;s wrqm&#x2F;s  - 读写请求的合并数量r&#x2F;s w&#x2F;s - 读写请求数量avgrq - 队列长度await - 时延util - 时间度量 ， 时间周期之内进行IO操作所占的比例。例如 1 秒的时间之内， 取样的点中有多少是在执行IO操作。\n一个例子 ， 如果采样的周期为1s， 那么采样的范围之内 ， 前面的0.5秒有执行IO的操作， 后面的0.5秒没有执行任何的操作， 那么 最后 Util 现在的结果就是50% 。avgrq也是一直平均值， 在采样周期之内如果前后的状况不一致 也会进行平均。\n一般情况下这个参数是准确的，但是大部分的性能问题都是取决数值的取样周期的。\n操作系统默认输出的块大小是 ： 256 ， 参数可见 ：╰─# cat &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;max_sectors_kb256\nmax_segments表示设备能够允许的最大段的数目。    – 这应该是一个内存或者buffer的分段指标。 （待定）max_sectors_kb表示设备允许的最大请求大小。      – 可改 。max_hw_sectors_kb表示单个请求所能处理的最大KB（硬约束） – 这个是上一个参数的Limit。  \n","categories":["Linux"],"tags":["Linux"]},{"title":"Shell脚本处理目录或者文件名中的空格","url":"/2022/04/15/Linux/Linux_Shell-Script-Process-Space/","content":"问题：问题是： 罗列指定的目录下面的文件， 符合要求的文件保留， 未匹配的删除。\n\n\n解决：简单的Bash脚本， 使用ls拿文件名， 使用For + IF判断即可。 \n但是文件的目录中有的文件名是带有空格的， 而Shell 使用空格做分隔符，因此无法正确的处理完整的文件名。 \n解决方案有特别多， 只记录一个我最后使用的方案：\n使用IFS变量来定义Shell 的默认分隔符， 将空格替换成\\n\\b. \n#!/bin/bash\n#\nSAVEIFS=$IFS\nIFS=$(echo -en \"\\n\\b\")\n\nfor number in &#123;1..10&#125;\ndo\n  mkdir -pv \"dir $number\"\ndone\n\ntree\n\nFILE=`ls`\n\nfor i in $FILE;\ndo\n  echo \"The DirName: $i\"\ndone\n\nIFS=$SAVEIFS\n\n\n这样就可以拿到名称为dir 1的完整目录名字了，否则会默认吧空格分割的文件名称作为两个目录提取。\n⚡ ./delete.sh\n.\n├── delete.sh\n├── dir 1\n├── dir 10\n├── dir 2\n├── dir 3\n├── dir 4\n├── dir 5\n├── dir 6\n├── dir 7\n├── dir 8\n└── dir 9\n\n10 directories, 1 file\nThe DirName: delete.sh\nThe DirName: dir 1\nThe DirName: dir 10\nThe DirName: dir 2\nThe DirName: dir 3\nThe DirName: dir 4\nThe DirName: dir 5\nThe DirName: dir 6\nThe DirName: dir 7\nThe DirName: dir 8\nThe DirName: dir 9\n\n","categories":["Linux"],"tags":["Shell"]},{"title":"Linux中一些常见的性能分析命令","url":"/2021/12/26/Linux/Linux-PerfCommand/","content":"记录性能分析的思路。\n\n\n最近的这个半年越来越好奇的事情是， 为什么命令会卡住，为什么命令会执行不下去，为什么命令会等待，等等等等。\n那么这些问题， 有的是可以有答案的， 目前也不知道的。 \n已经大概掌握的几个不同的方法以及观测的工具， 大概做一个记录。\nStrace命令strace 命令的常见用法strace命令是用来追踪系统调用的，常见的可以追踪的系统调用需要阅读内核部分的代码。 但是常见的系统调用就是集中， read() , write() , ioctl(), futex() , mmap()大部分的时候 我们都是可以观测到卡住的部分的 ， 这种追踪我认为常见的使用场景就是命令卡住了， 或者执行中的程序卡住了。 \n命令卡住的分析对于命令卡住的情况， 可以使用类似于如下的命令： \nstrace -f -ttt -s 512 echo \"123\"\n这样的话， 在执行的过程中就可以查看相关的内容，比如常见的卡在了系统调用的某个函数上， 这个可以用来定位，命令打开了那些文件，申请的那些内存地址，打开了什么文件，关闭Socket等等等等。目前我的办法的通过对比这个卡住的命令执行到什么函数出现的问题， 对应的在正常的机器上进行对比，就可以猜到大概的问题出现在了哪里。\n已经运行中的程序分析strace -f -ttt -s 512 -p 123\n执行进程的PID ， 然后strace会attach到进程上， 输出的内容， 也可以查看到当前程序的运行状态。 \n总结如上面的两种方式，都可以对运行中明显的问题进行观察， 但是如果没有卡在系统调用的部分， 通过这个命令的观察其实是无法查看的， 因为他记录的是应用程序指令陷入到内核态的部分， 但是常见的应用程序基本上都是用户态的，所以这个部分如果是应用卡在用户态上， 观测的信息就比较有限了。\nPerf命令perf简单的分析perf命令的简单分析， 首先是\n\nperf topperf top 可以用来实时的查看应用程序的相关问题， 收集指标的范围是整个操作系统，所以是比较消耗资源的， 输出的结果也是直接可以查看的， 看完了结果打断即可。 \n\nperf statperf stat 查看相关的统计信息，如下是一个样例，提供了一些静态的指标。 \nsudo perf stat\nPerformance counter stats for 'system wide':\n        61,167.25 msec cpu-clock                 #   16.000 CPUs utilized\n            4,955      context-switches          #   81.007 /sec\n               63      cpu-migrations            #    1.030 /sec\n              930      page-faults               #   15.204 /sec\n    6,266,524,215      cycles                    #    0.102 GHz                      (83.32%)\n      244,046,608      stalled-cycles-frontend   #    3.89% frontend cycles idle     (83.34%)\n       66,809,179      stalled-cycles-backend    #    1.07% backend cycles idle      (83.34%)\n      818,170,826      instructions              #    0.13  insn per cycle\n                                                 #    0.30  stalled cycles per insn  (83.34%)\n      155,602,840      branches                  #    2.544 M/sec                    (83.34%)\n        1,701,804      branch-misses             #    1.09% of all branches          (83.33%)\n\n      3.823037998 seconds time elapsed\n显示的内容是从输入了命令之后的相关信息，主要是一些CPU相关的指标， 比如CPU时钟，上下文交换次数，cpu转移，缺页中断等等等等。\n\nperf recordperf record 我常用的命令是这样的， 他会将记录到的指标输出到当前目录的文件中，然后供report命令来进行分析， 这两个一般来说会合用。 \nperf record -a -g -F 1000 -p 123  \nperf record -a -g -F 1000 echo 123\nperf record -a -g -F 1000 \nperf record -a -g -F 1000 -- sleep 60 \n三个命令会记录相关的指标到当前目录的perf.data文件中。 大小和采样的频率，时间的数量有关。\nperf reportperf report 我比较常用的参数就是 使用 \nperf record --stdio\n来直接进行查看， 占用时间百分比比较高的函数，前提是 ，这个命令的运行需要有perf.data.\n\nperf schedperf sched 通常是用来查看cpu调度延时的， 这个用的确实不多， 毕竟cpu调度现在基本上都是cfq， 改的人毕竟还是少数， 所以实际的使用比较少。这个指令常用的如下： \nperf sched record \nperf sched latency \nperf sched report\n上面的这些都是我比较常用的命令， 临时抓出来看下。\n\n\nperf命令输出火焰图perf 命令输出火焰图需要的是Github上面的一个项目， 这个项目的作者也是写性能之巅的作者。\n具体的处理流程如下： \ngit clone --depth 1 https://github.com/brendangregg/FlameGraph.git\nsudo perf script > out.perf\nFlameGraph/stackcollapse-perf.pl out.perf > out.folded\nFlameGraph/flamegraph.pl out.folded > out.svg\n最后输出的out.svg就是结果了，可以通过浏览器来查看。至于查看的方法，其实是看函数所占有的面积， 面积越大说明函数运行的时间越长；那么还有说法是说， 越靠近顶端的应该越尖，如果有顶端比较大的平顶说明可能是有问题的， 这个答案还在求证中。\n","categories":["Linux"],"tags":["Linux"]},{"title":"Kubernetes day2","url":"/2021/10/02/Linux/Linux_Kubernetes-day2/","content":"对于etcd的操作和备份 \n\n\netcd的操作 - etcdctletcd的规划\n最好用 固态盘， Pod数量比较多的情况下会非常非常慢， 内存要大.\n类似于redis 或者 Zookeeper， KV的存储.\n支持watch机制，可以通知给node节点的数据变化.\netcd consul zookeeper 的区别\n\n\n\n\n名称\n优点\n缺点\n接口\n一致性算法\n\n\n\nzookeeper\n1.功能强大，不仅仅只是服务发现2.提供watcher机制能实时获取服务提供者的状态3.dubbo等框架支持\n1.没有健康检查2.需在服务中集成sdk，复杂度高3.不支持多数据中心\nsdk\nPaxos\n\n\nconsul\n1.简单易用，不需要集成sdk2.自带健康检查3.支持多数据中心4.提供web管理界面\n1.不能实时获取服务信息的变化通知\nhttp&#x2F;dns\nRaft\n\n\netcd\n1.简单易用，不需要集成sdk2.可配置性强\n1.没有健康检查2.需配合第三方工具一起完成服务发现3.不支持多数据中心\nhttp\nRaft\n\n\n\n\n\n\n\n\n\netcdctl 的命令\n查看etcd的成员清单\n]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --write-out=table --endpoints=\"192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379\" member list\n\n查看etcd的节点状态\n]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints=\"192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379\" endpoint status -w table\n\n查看etcd存储的数据 \n]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints=\"192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379\" get /registry/ --prefix --keys-only | head\n查看etcd中的pod信息\n\n\n]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints=\"192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379\" get /registry/ --prefix --keys-only | grep pod\n\n其他的操作\n\n\nget &#x2F; put &#x2F; del 等基础操作\n\nwatch机制watch机制是通过不断的查看数据，发生变化就主动的通知客户端，v3支持watch固定的key,也可以watch一个范围的数据。\n# watch 一个pod的信息， 然后手动delete这个pod ， 查看etcd 的watch行为和输出的结果。\n]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints=\"192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379\" watch /registry/pods/monitoring/node-exporter-889hf\n\n数据备份恢复和WAL日志WAL： watch ahead log - 预写日志， 可以通过预写日志来进行数据库的恢复。WAL记录了整个数据变化的过程，在操作写入数据之前先进行wal日志的写入。\netcd v2 的时候直接复制和备份目录，备份文件的方案etcd v3 的备份和恢复， 使用快照的方式。\n备份使用的命令和恢复的命令不太一样， etcdctl  vs   etcdutl \n可以写脚本来进行数据进行备份和恢复。\n]$ etcdctl snapshot save\n]$ etcdctl snapshot restore\n]$ etcdctl snapshot status\n\n]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints=\"192.168.31.21:2379\" snapshot save snap-20211002.db\n\n# 测试尝试恢复到临时的目录，测试用。 目录地址用的是tmp下面的。\n]$ etcdutl snapshot restore ./snap-20211002.db --data-dir /tmp/etcd-restore\n2021-10-02T12:01:13+08:00\tinfo\tsnapshot/v3_snapshot.go:251\trestoring snapshot\t&#123;\"path\": \"./snap-20211002.db\", \"wal-dir\": \"/tmp/etcd-restore/member/wal\", \"data-dir\": \"/tmp/etcd-restore\", \"snap-dir\": \"/tmp/etcd-restore/member/snap\", \"stack\": \"go.etcd.io/etcd/etcdutl/v3/snapshot.(*v3Manager).Restore\\n\\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/snapshot/v3_snapshot.go:257\\ngo.etcd.io/etcd/etcdutl/v3/etcdutl.SnapshotRestoreCommandFunc\\n\\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/etcdutl/snapshot_command.go:147\\ngo.etcd.io/etcd/etcdutl/v3/etcdutl.snapshotRestoreCommandFunc\\n\\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/etcdutl/snapshot_command.go:117\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/home/remote/sbatsche/.gvm/pkgsets/go1.16.3/global/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:856\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/home/remote/sbatsche/.gvm/pkgsets/go1.16.3/global/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:960\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/home/remote/sbatsche/.gvm/pkgsets/go1.16.3/global/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:897\\nmain.Start\\n\\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/ctl.go:50\\nmain.main\\n\\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/main.go:23\\nruntime.main\\n\\t/home/remote/sbatsche/.gvm/gos/go1.16.3/src/runtime/proc.go:225\"&#125;\n2021-10-02T12:01:13+08:00\tinfo\tmembership/store.go:119\tTrimming membership information from the backend...\n2021-10-02T12:01:13+08:00\tinfo\tmembership/cluster.go:393\tadded member\t&#123;\"cluster-id\": \"cdf818194e3a8c32\", \"local-member-id\": \"0\", \"added-peer-id\": \"8e9e05c52164694d\", \"added-peer-peer-urls\": [\"http://localhost:2380\"]&#125;\n2021-10-02T12:01:13+08:00\tinfo\tsnapshot/v3_snapshot.go:272\trestored snapshot\t&#123;\"path\": \"./snap-20211002.db\", \"wal-dir\": \"/tmp/etcd-restore/member/wal\", \"data-dir\": \"/tmp/etcd-restore\", \"snap-dir\": \"/tmp/etcd-restore/member/snap\"&#125;\n]$ ls /tmp/etcd-restore/\nmember\n\n数据恢复的流程：\n\n创建新的etcd集群\n停止kubernetes以及其他的依赖etcd 的服务。 \n停止空白的新的集群\n使用备份的文件进行集群的恢复\n使用在集群的每个节点恢复相同的备份文件\n每个节点启动etcd的集群并且进行验证\n启动Kubernetes的相关集群和组件。\n查看恢复的结果，验证各个组件的相关服务是否已经正常恢复。\n\netcd节点的维护\netcdctl add-etcd  \netcdctl del-etcd\n\n资源清单以及API相关的外部服务接口\nContainer Runtime Interface - CRI\n\n\nrunc\nRKT\n\n\nContainer Storage Interface - CSI\nContainer Network Interface - CNI\n\nnode的相关操作\ncordon\nuncordon\ndrain\ntaint\n\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"Harbor Http 安装部署","url":"/2021/09/27/Linux/Linux_HarborInstallation/","content":" Harbor的部署记录。\n\n\nHarbor Info\nHarbor 项目地址\n\nHarbor HTTP部署因为是临时使用， 所以直接给了HTTP的权限， 为的是不走公网部署 CEPH Cluster， CEPH在BootStrap之后会默认去公网的镜像仓库尝试Pull镜像并且校验镜像和服务，所以给一个私有的仓库， 直接去找私有仓库就免了公网访问卡集群的正常启动的步骤。\n下载解压~]$ wget https://github.com/goharbor/harbor/releases/download/v2.3.2/harbor-offline-installer-v2.3.2.tgz\n~]$ mv harbor-offline-installer-v2.3.2.tgz /opt \n~]$ tar zxvf /opt/harbor-offline-installer-v2.3.2.tgz\n\n复制修改配置文件~]$ cp /opt/harbor/harbor.yml.tmpl /opt/harbor/harbor.yml\n\n配置默认的存储位置# 注释掉https的部分，如果需要https的话签发一个证书写路径在配置文件中\n# 修改默认的存储位置\ndata_volume: /opt/harbor/image_store\n指定Harbor对外提供服务的域名# 修改Harbor的域名或者主机名(需要对应的解析)，也可以直接使用IP地址\n# The IP address or hostname to access admin UI and registry service.\n# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.\nhostname: harbor.local\n设置harbor admin的密码可以登录Dashboard 或者 Pull镜像\n# Remember Change the admin password from UI after launching Harbor.\nharbor_admin_password: Harbor12345\n\n创建harbor存储镜像的目录# 创建Harbor的存储目录， 可以远程指定到Cephfs上面\n~]$ mkdir /opt/harbor/image_store\n\n\n配置Docker-ce 清华的镜像源这个配置是给Centos &#x2F; RHEL来使用的，来自清华的Repo Help\n# 添加repo文件，和修改配置到Tsinghua repo\n~]$ wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo\n~]$ sudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n\n# 安装Docker-ce\n~]$ sudo yum install docker-ce\n\n# CENTOS8 STREAM 特殊的配置，需要卸载 podman 和 Buildah\n~]$ dnf install -y docker-ce --allowerasing\n\n# 开机启动\n~]$ sudo systemctl restart docker &amp;&amp; sudo systemctl enable docker\n\n# 安装Docker-compose ，因为CentOS8 默认是没有Docker-compose的 ， 按照官网的流程走就可以。\n~]$ sudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n~]$ sudo chmod +x /usr/local/bin/docker-compose\n~]$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n安装harbor~]$ cd /opt/harbor\n/opt/harbor]$ ./install.sh\n\n启动Harbor直接使用Docker-compose启动即可，如果需要的话可以在配置文件中指定镜像扫描器，来进行镜像的漏洞扫描。\n# 进入Harbor运行所在的目录\n~]$ cd /opt/harbor\t\n\n# 使用Docker-compose的启动命令，适用于服务停止 或者 Docker 重启的时候， 容器没有正常运行。\n~]$ docker-compose up -d -f /opt/harbor/docker-compose.yaml\n\n# 查看容器的启动状态\n/opt/harbor]$ docker-compose ps\n/opt/harbor]$ watch -n 1 docker-compose ps\n\n# 通过Docker-compose停止harbor\n/opt/harbor]$ docker-compose down\n\n验证\n打开浏览器，访问harbor的地址 默认的端口80 \n在所有docker节点上配置不安全的私有仓库，docker login [HARBORIP:PORT]\n在所有podman节点上配置不安全的私有仓库，podman login [HARBORIP:PORT]\n提示Login Successed， 登录成功，可以正常pull镜像了\n\nCephadm Bootstrap编辑cephadm文件，修改如下的镜像名称，和仓库的前缀这里其实还是需要再测试的， 按照这个脚本的逻辑， 应该会把所有的镜像都从指定的仓库Pull下来，但是我执行的时候只有ceph&#x2F;ceph:v16一个镜像下来了， 感觉还是有点儿问题的。\nDEFAULT_IMAGE = 'harbor.local/ceph/ceph:v16'\nDEFAULT_IMAGE_IS_MASTER = False\nDEFAULT_IMAGE_RELEASE = 'pacific'\nDEFAULT_PROMETHEUS_IMAGE = 'harbor.local/ceph/prometheus:v2.18.1'\nDEFAULT_NODE_EXPORTER_IMAGE = 'harbor.local/ceph/node-exporter:v0.18.1'\nDEFAULT_ALERT_MANAGER_IMAGE = 'harbor.local/ceph/alertmanager:v0.20.0'\nDEFAULT_GRAFANA_IMAGE = 'harbor.local/ceph/ceph-grafana:6.7.4'\nDEFAULT_HAPROXY_IMAGE = 'harbor.local/ceph/haproxy:2.3'\nDEFAULT_KEEPALIVED_IMAGE = 'harbor.local/ceph/keepalived'\nDEFAULT_REGISTRY = 'harbor.local'   # normalize unqualified digests to this\n\ncephadm 使用私有仓库bootstrap~]$ cephadm bootstrap --mon-ip 192.168.1.211 --allow-overwrite \\\n  --registry-url harbor.local \\\n  --registry-username admin \\\n  --registry-password Harbor12345\n\n查看并且清除ceph-bootstrap的历史记录~]$ ls /etc/systemd/system/ceph*\n~]$ ls /usr/lib/systemd/system/ceph*\n\n~]$ rm -rf /etc/systemd/system/ceph*\n~]$ rm -rf /usr/lib/systemd/system/ceph*\n~]$ docker stop `docker ps -a -q`  \n\n\n\n\n\n\n\n\n\n\n","categories":["Linux"],"tags":["Harbor"]},{"title":"Kubernetes day1","url":"/2021/09/11/Linux/Linux_Kubernetes-day1/","content":"云原生的定义\n\n\n云原生一些概念十二因素应用\n基准代码： 一份基准代码，多次部署（用同一个代码库进行版本控制）\n依赖： 显式的声明和隔离相互之间的依赖\n配置： 在环境中存储配置(配置中心，携程的apollo)\n后端服务： 后端服务作为一个附加的资源，数据库，中间件等等\n构建，发布，运行： 对程序执行构建或者打包，严格分离构建和运行,打镜像编译等等，构建和运行严格的分离\n进程： 使用一个或者多个无状态进程运行应用\n端口绑定： 通过端口绑定提供服务\n并发： 通过进程模型进行扩展\n易处理： 快速的启动，优雅的终止，最大程度的保持健壮性。\n开发环境与线上环境等价： 尽可能保持Dev，Prelive，Live环境的一致性\n日志： 将所有运行中的进程和后台服务的输出流按照的时间顺序统一收集存储和展示(ELK,Fluend,Logstash,Filebeat等等)\n管理进程： 一次性管理类型的进程(Onetime Job)应该和正常的常驻进程使用同样的运行环境\n\nMaster - APIserverAPIserver提供了Kubernetes各类资源对象的CRUD以及Watch等等的HTTP REST接口，对象包括Pods，Services, ReplicationsControllers等等, 为RESTful操作提供服务，并且为集群状态提供前端，所有的组件都通过前端进行交互。\n\n特点： RESTful风格\n\nAPIserver的请求最后会同步到Etcd。\n请求APIserver在权限的鉴定完成之后， 可以查看大部分的信息。\n端口默认是 6443 可通过启动参数（–secure-port）来进行调整\n绑定的IP地址可以通过 –bind-address 在启动的时候指定\n端口用于接受客户端，dashboard的外部HTTPBase的请求\nToken 以及证书的HTTPbase的校验\n基于指定策略的授权\n\n\n功能： \n\n身份认证\n验证权限\n验证指令\n执行操作\n返回结果\n\n\n\nMaster - Kube-scheduler负责将Pod指定到节点上。\n\n取出Pod需要分配的信息， 先排除不可调度的Node，在可用的Node列表之中选择一个合适的Node，将信息写入etcd。等待Node上面的kubelet进行生成Pod。\nNode的Kubelet通过APIserver监听到Pod的相关信息，然后获取Pod的清单，下载镜像，启动Pod， Kubelet每秒Watch-APIserver的信息。\n三个默认策略： \nLeastRequestedPriority - CPU+MEM直接评分，选择资源目前较低\nCalculateNodeLanelPriority - 先匹配标签，在进行评分\nBalancedResourceAllocation - 优先分配各项资源的使用率最均衡的节点\n\n\n队列： PodQueue ， NodeList\n\nMaster - Kuber-Controller-Manager提供不同的控制器，例如： 集群内的Node, Pod ReplicaCounts, EndPoint, Namespace, ServiceAccount, ResourceQuota等等资源的控制内容，如果发现异常的时候提供自动化的修复流程。确保所有的服务是在预期的状态下运行。\n\n5s检查一次Node的状态\n包括多种不同的控制器\n检查所有的控制器 和 Node是否符合预期\n如果Node 不可达之后 40s会将节点标记为无法访问\n标记为无法访问5min之后， 将删除这个节点，同时在其他的节点重建需要的Pod。\n\n\nPod的高可用机制：    1.  NodeMonitorPeriod: 监视周期    2.  NodeMonitorGracePerios: 节点监视观察期    3.  PodEvictionTimeout: 超时驱逐的区间\nKube-ProxyKubernetes网络代理，反映了Node上面的Kubernetes中的服务对象的变化，通过管理IPVS或者IPTABLES的规则来进行网络层的实现。\n\n可以通过配置文件来指定IPVS的调度算法， 一般默认是RR, ipvsadm -ln\n直接指定kubelet的配置文件选项ipvs:\n  scheduler: sh\n\nKubelet\n汇报节点的状态\n监听API server 上面的pod信息变化，并调用Docker 和 Containerd 创建容器\n准备Pod所需要的数据卷\n返回Pod的运行状态\n在Node节点上进行容器的健康检查\n\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"Docker基础知识二周目","url":"/2021/09/04/Linux/Linux_Docker-again/","content":"Docker Container 基础 。 again ….. \n \n\nDocker使用的六个名称空间\nMNT Namespace - 提供文件的挂载和文件系统的隔离\nIPC Namespace - 提供进程间通信\nUTS Namespace - 提供主机名隔离\nPID Namespace - 提供进程以及进程号隔离\nNet Namespace - 提供网络隔离\nUser Namespace - 提供用户以及用户id的隔离\n\n容器的进程ID关系Host -------------------------------- | container ------------------------|\nsystemd -- containerd -- container-shim -- | nginx-master -- nginx-worker\n               `      -- container-shim -- | nginx-master -- nginx-worker\n               `      -- container-shim -- | mysql-master -- mysql-worker\n           dockerd -- docker-proxy -- | docker-proxy\n               `   -- docker-proxy -- | docker-proxy\n               `   -- docker-proxy -- | docker-proxy\n\ncontainerd负责控制容器的进程，主机，用户，挂载的隔离。dockerd 启动的docker-proxy用来管理iptables进行网络的隔离。docker.service 里面指定了调用Containerd的socket生成容器的过程钟，API调用过程使用的是grpc。无论dockerd还是containerd最终使用的都是runc（container runtime） ，一个基于Go语言的容器创建工具， 根据OCI标准来创建容器。\ndocker-daemon 限制可以限制生成日志的大小\n\n限制日志的大小使用的是 docker的daemon config， 针对的日志的范围，是docker Container 里面的标准输入输出的日志， 这部分日志的存储是在机器的硬盘上， 位置是（&#x2F;opt&#x2F;docker-storage&#x2F;containers ， 也就是&#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;[CONTAINER_ID]&#x2F;*.json），\n\n cpu 大约可以分配 1:4  ， mem不建议超过物理内存的分配\n压测镜像： lorel&#x2F;docker-stress-ng \ndocker的网络结构使用的是宿主机的docker0桥，同时在Host上面生成虚拟网卡veth多个接口 ， 在容器中生成eth0的虚拟网卡对， 虚拟机中的两个容器之间使用的是物理地址进行寻址访问，所以这个部分可以通过arp命令来进行验证。\nDocker容器的资源限制 - Cgroup可以限制 CPU，MEM，DISKIO，NETIO, PRI，PAUSE&#x2F;RESUME。 对于Kubernetes可以对集群的Namespace来进行限制资源的规划。\n查看内核可以支持的cgroup Flag. \n┌─[hayden@HaydenArchDesktop] - [~] - [Sun Sep 05, 00:14]\n└─[$] &lt;> zcat /proc/config.gz | grep CGROUP\nCONFIG_CGROUPS=y\nCONFIG_BLK_CGROUP=y\nCONFIG_CGROUP_WRITEBACK=y \nCONFIG_CGROUP_SCHED=y\nCONFIG_CGROUP_PIDS=y\nCONFIG_CGROUP_RDMA=y\nCONFIG_CGROUP_FREEZER=y\nCONFIG_CGROUP_HUGETLB=y\nCONFIG_CGROUP_DEVICE=y\nCONFIG_CGROUP_CPUACCT=y\nCONFIG_CGROUP_PERF=y\nCONFIG_CGROUP_BPF=y\nCONFIG_CGROUP_MISC=y\n# CONFIG_CGROUP_DEBUG is not set\nCONFIG_SOCK_CGROUP_DATA=y\nCONFIG_BLK_CGROUP_RWSTAT=y\nCONFIG_BLK_CGROUP_IOLATENCY=y\nCONFIG_BLK_CGROUP_IOCOST=y\n# CONFIG_BFQ_CGROUP_DEBUG is not set\nCONFIG_NETFILTER_XT_MATCH_CGROUP=m\nCONFIG_NET_CLS_CGROUP=m\nCONFIG_CGROUP_NET_PRIO=y\nCONFIG_CGROUP_NET_CLASSID=y\n\n┌─[hayden@HaydenArchDesktop] - [~] - [Sun Sep 05, 00:14]\n└─[$] &lt;> zcat /proc/config.gz | grep MEM | grep CG\nCONFIG_MEMCG=y\nCONFIG_MEMCG_SWAP=y\nCONFIG_MEMCG_KMEM=y\nCPUKubernetes 限制是1&#x2F;1000Core ，Docker是1&#x2F;10Core, 只需要通过 –cpus 选项指定容器可以使用的 CPU 个数就可以了，并且还可以指定如 1.5 之类的小数。\nCPU OverCommit 1:8\nMEMMEM的限制单位是M； 同时可以限制Swap的使用，但是限制Swap的使用需要内核的支持，大部分时候还是会关闭掉SWAP， 虽然避免了OOM，但是会影响服务的质量。\nMEM 不建议OverCommit\n\n一般情况下 ： 分配0.5&#x2F;1个CPU，mem 2G&#x2F;4G； 根据业务的不同， 一般会是高可用的部署，两个一起对外提供服务；所以会启动两个容器，不需要提供太高的硬件，符合要求即可。\n\n关于OOM的问题和可以调整的参数(&#x2F;proc&#x2F;[PID]&#x2F;)：\n\noom_adj 取值范围 -17 to +15 ， 为了兼容旧程序保留的方式\noom_score 一般是自动计算出来的结果，综合计算的结果, 参考 ： CPU时间，存活时间，oom_adj计算之后的结果。\noom_score_adj OOM分数的偏移量，-1000 to +1000, 可以设置-1000表示永远不会被Kill\n\nDISKIODisk OverCommit : 1:1.2\nPAUSE\\RESUME","categories":["Linux"],"tags":["Linux"]},{"title":"Ceph Cluster 04 - CRUSH算法","url":"/2021/09/01/Linux/Linux_Ceph04/","content":"ceph笔记04\n\n\n修改CRUSH算法的分配方式基础概念：五种运行图： MON服务维护\nMonitor Map - 监控的运行图 MON的状态\nOSD Map - OSD的状态， 每隔六秒钟汇报一次状态\nPG Map - PG的运行图， PG的状态和映射关系\nCRUSH Map - 一致性hash算法，和数据块和osd的分配关系， 动态更新，当客户端请求一个文件的时候，会通过CRUSH算法会根据osd的map创建PG组合的来对文件的存储进行负载和分配。假如有20个osd，创建32重组合分配对应的pg和osd的对应关系，选主从，选分布方式和节点的同步关系，叫做CRUSH Map。\nMDS Map - Metadata Map ，元数据和数据文件的映射关系。\n\n5种算法对节点的选择方式\nUniform\nList \nTree\nStraw - 早期的版本，分布不是特别的均衡，抽签算法\nStraw2 - 目前已经发展中的版本， 抽签算法 （Default）\n\n对PG的动态调整默认情况下是动态调整的，但是可以手动调整为给予权重，设置PG分配的倾向，例如1T权重是1，等等等等\n查看状态以及调整方式ceph osd df需要关注的值有两个：\n\nweight - 根据磁盘的空间进行的调整，默认自动计算， 可调。\nreweight - 磁盘的所有权重相加之后， 单个osd所占用的比例，由于默认的分配是相对概率的平衡，所以分配可能还是会有一些不均衡，通过这个可以进行再次的平衡。\n\n调整的效果就是希望立刻重新平衡PG的数量(需要注意在业务负载低的时候执行) 数据均衡分配。迫使算法动态的更新PG的位置。\n\n调整WEIGHT的值ceph osd crush reweight osd.10 {WEIGHT} # 调整weight值， 越大权重越高，分配的PG数量越大。\n\n调整REWEIGHTceph osd crush reweight {OSD_ID} {REWEIGHT} # 范围是 0 - 1， 也是越大权重越高。\n\n\n对运行图进行操作\n创建一个保存运行图的目录mkdir &#x2F;cephmap&#x2F;ceph -pceph osd getcrushmap -o &#x2F;cephmap&#x2F;ceph&#x2F;crushmap  # 同时可以做备份使用，将运行图导出到文件中，当需要的时候可以通过这个文件还原。\n转换map的二进制格式为文本，Ubuntu 需要安装 crushbase 包。crushtool -d &#x2F;cephmap&#x2F;ceph &gt; &#x2F;cephmap&#x2F;ceph&#x2F;crushmap.txt\n编辑这个文件主要编辑type部分， 按照不同的调度需求，按照不同的osd，不同主机调度，不同的机架，不同的机柜，不同的PDU，不同的房间，不同的数据中心， 不同的区域城市， 顶层。下面是主机的配置部分。每个主机的算法，osd的分配情况，权重。Rules 副本池的规则里面定义了step take default  基于default配置端里面的规则来进行osd的分配。step chooseleaf firstn 0(按照顺序选择，先选到的就是) type host（按照什么类别进行选择高可用的类型）max_size 副本池配置文件之中可以定义副本数\n转换成二进制文件crushtool -c &#x2F;cephmap&#x2F;ceph&#x2F;crushmap.txt -o &#x2F;cephmap&#x2F;ceph&#x2F;crushmap_new \n导入查看是否生效ceph osd setcrushmap -i &#x2F;cephmap&#x2F;ceph&#x2F;crushmap_new\n查看是否生效ceph osd crush rule dump\n创建一个存储池ceph osd pool create magedu-ssdpool 32 32 magedu_ssd_ruleceph pg ls-by-pool  magedu-ssdpool | awk ‘{print $1,$2,$15}’\n\n分机械盘和固态盘调整运行图的配置","categories":["Linux"],"tags":["Ceph"]},{"title":"Linux_Ranger_Usage","url":"/2021/08/26/Linux/Linux_RangerUsage/","content":"ranger 备忘\n\n\ng/ Go root \ngh Go home\ngg Go to top \nG  Go bottom\n\n\n\n# 文件操作 复制、剪切、删除、粘贴 （针对当前文件或则选择的内容）\nyy 复制\ndd 剪切\npp 粘贴\nF5 复制\nF5 剪贴\nF8 删除\nDelete 删除\n\n# 书签\nmX 把当前目录做一个书签 (X 表示任何字符)\n&#39;X 跳到X书签代表的目录\n\n# 标签 不同标签可以复制、粘贴、移动\ngn 新建一个标签\nAlt+N 跳转到N号标签 (代表一个数字)\ngt,gT 跳转到前个标签，后个标签\n\n# 排序 对文件和目录列表进行排序，以便查看。\not 根据后缀名进行排序 (Type)\noa 根据访问时间进行排序 (Access Time 访问文件自身数据的时间)\noc 根据改变时间进行排序 (Change Time 文件的权限组别和文件自身数据被修改的时间)\nom 根据修改进行排序 (Modify time 文件自身内容被修改的时间)\nob 根据文件名进行排序 (basename)\non 这个好像和basename差不多(natural)\nos 根据文件大小进行排序(Size)\n\n# 重命名 修改文件名有两种模式：当前文件和批量改名\ncw 新文件名 -- 修改当前文件名\nA -- 在当前文件名后追加文字\nI -- 在当前文件名前追加文字\n:bulkrename --针对mark过的文件批量改名\n\n# 执行shell命令\n! -- 进入命令模式执行shell命令\ns -- 同上\n# -- 同！，但结果输出到一个pager。相当于 cmd | less\n@ -- 同！，但会把选择的文件作为参数放在最后。\nS -- 进入一个新的shell。exit后回到当前的ranger\n\n","categories":["Linux"],"tags":["Ranger"]},{"title":"Ceph Cluster 03 - CephFS","url":"/2021/08/24/Linux/Linux_Ceph03/","content":"ceph笔记03\n\n\ncephfs的使用cephfs的使用条件\n当我们需要多个服务来挂载和实时的同步的时候， 使用到CEPHFS，可以实现文件系统的共享。内核里面现在这个时间已经内置cephfs的挂载模块， 可以直接挂载不需要安装。\ncephfs运行需要MDS服务，用来存储缓存的文件信息。总体需要创建两个存储池，单独创建一个存储MDS信息的存储池， 同时需要创建一个数据池来提供存储空间。\n启用mds的服务 ceph orch mds 2\n创建ceph的存储池ceph mds stat\n# 创建一个cephfs的metadata池\nceph osd pool create metadata 32 32 \n# 创建一个cephfs的data池\nceph osd pool create cephfsdata 64 64 \n# 创建ceph的状态\nceph osd pool ls \nceph -s \n# 创建cephfs的文件系统\nceph fs new defaultfs metadata cephfsdata\n\n# 新的版本里面已经不需要手动创建mds和两个对应的存储池了， 只是需要一条命令就可以自动创建。\n[ceph: root@ceph01 /]# ceph fs volume create test\n[ceph: root@ceph01 /]# ceph fs volume ls\n[ceph: root@ceph01 /]# ceph mds stat\n# 需要提前获取挂载的Token： \n[root@HaydenArchDesktop ceph]# sudo scp root@ceph01:/etc/ceph/ceph.client.admin.keyring /etc/ceph/\n[root@HaydenArchDesktop ceph]# mount -t ceph :/ /mnt -o name=admin\n[root@HaydenArchDesktop mnt]# mount | grep ceph\n192.168.31.11:6789,192.168.31.12:6789,192.168.31.13:6789:/ on /mnt type ceph (rw,relatime,name=admin,secret=&lt;hidden>,acl)\n\n#\n# journalctl -f > /mnt/journal.log   \n# 查看cephfs的写入文件的动作过程。 \n# 同时使用 tail -f 在另一个窗口中查看文件的内容。 测试写入和查看内容的差距。 \n\n\n用户权限MDS可以启动多个实例，多个实例会自动来负载不同资源的文件元数据的缓存。客户端通过MON节点进行授权 和 获取MDS的的位置。MDS现在最新的版本默认已经是一主一备， 自动会生成高可用的模式。（之前的手动部署多个MDS服务器， 然后指定MDS的角色）\n创建一个普通用户来进行身份的验证# 创建一个测试的用户\nceph auth add client.testuser mon 'allow r' mds 'allow rw' osd 'allow rwx pool=cephfsdata'\n# 测试所建立的用户的权限，获取认证的keyring\nceph auth get client.testuser\n# 导出用户的Keyring， 用来做集群的校验\nceph auth get client.testuser -o ceph.client.testuser.keyring\n# 导出用户的key ， Kubernetes的挂载会使用到\nceph auth print-key client.testuser > testuser.key\n# 验证用户是否可以获取到集群的状态\nceph --user testuser -s \n\n# 将挂载点添加到fstab自动开机挂载\nMON01:6789,MON02:6789,.........:/ /mnt ceph defaults,name=testuser,secretfile=/etc/ceph/testuser.key,_netdev 0 0\nDashboard权限# 启用或者禁用账户\n[ceph: root@ceph01 /]# ceph dashboard ac-user-enable admin\n\n# 重置Dashboard用户的密码\n# Old Version\n[ceph: root@ceph01 /]# ceph dashboard set-login-credentials admin -i /etc/ceph/dashboard_password\n\n# New version command.\n[ceph: root@ceph01 /]# ceph dashboard -h | grep ac-user\ndashboard ac-user-add-roles &lt;username> [&lt;roles>...]         Add roles to user\ndashboard ac-user-create &lt;username> [&lt;rolename>] [&lt;name>]   Create a user. Password read from -i &lt;file>\ndashboard ac-user-del-roles &lt;username> [&lt;roles>...]         Delete roles from user\ndashboard ac-user-delete [&lt;username>]                       Delete user\ndashboard ac-user-disable [&lt;username>]                      Disable a user\ndashboard ac-user-enable [&lt;username>]                       Enable a user\ndashboard ac-user-set-info &lt;username> &lt;name> [&lt;email>]      Set user info\ndashboard ac-user-set-password &lt;username> [--force-         Set user password from -i &lt;file>\ndashboard ac-user-set-password-hash &lt;username>              Set user password bcrypt hash from -i &lt;file>\ndashboard ac-user-set-roles &lt;username> [&lt;roles>...]         Set user roles\ndashboard ac-user-show [&lt;username>]                         Show user info\n\nMDS 高可用# 提升默认的主节点的数量， 来提高MDS服务的吞吐量\nceph fs set defaultfs max_mds 2 \nceph fs get defaultfs\n# 变成两主两备， （设置Rank）\n# 参数： mds_standby_replay true \n         mds_standby_for_name: MDS_NAME\n         mds_standby_for_rank: 备份指定级别的mds\n         mds_standby_for_fscid: 指定文件系统ID，会联合rank配置生效，如果指定了rank就是指定文件系统的rank会进行主备，如果未指定就是指定文件系统的所有Rank。\n\n如果是一对一的高可用 ， 需要对每个mds进行独立的配置。配置样例： # 配置的结果是mds1主， mds2 备； mds3 主， mds4 备.\n[mds.ceph-mds1]\nmds_standby_replay = true \nmds_standby_for_name = ceph-mds2\nmds_standby_for_fscid = defaultfs \n\n[mds.ceph-mds3]\nmds_standby_replay = true\nmds_standby_for_name = ceph-mds4\n\n","categories":["Linux"],"tags":["Ceph"]},{"title":"Ceph Cluster 02 - OSD/RBD","url":"/2021/08/23/Linux/Linux_Ceph02/","content":"Ceph的使用笔记。\n\n\nUSAGE创建存储池# 创建一个PG为64 ，PGP为64的存储池。\n[ceph: root@ceph01 /]# ceph osd pool create test-64 64 64\npool 'test-64' created\n\n# 创建一个自动识别的大小的存储池。\n[ceph: root@ceph01 /]# ceph osd pool create test\npool 'test' created\n\n# 查看已经存在的存储池。\n[ceph: root@ceph01 /]# ceph osd pool ls\ndevice_health_metrics\ntest\ntest-64\n\n# 查看存储池的PG 和 PGP 的信息和关系。\n[ceph: root@ceph01 /]# ceph pg ls-by-pool test\n\n# 查看OSD的状态。\n[ceph: root@ceph01 /]# ceph osd tree\nID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF\n-1         0.58557  root default\n-5         0.14639      host ceph01\n 2    hdd  0.04880          osd.2        up   1.00000  1.00000\n 7    hdd  0.04880          osd.7        up   1.00000  1.00000\n11    hdd  0.04880          osd.11       up   1.00000  1.00000\n-7         0.14639      host ceph02\n 3    hdd  0.04880          osd.3        up   1.00000  1.00000\n 6    hdd  0.04880          osd.6        up   1.00000  1.00000\n10    hdd  0.04880          osd.10       up   1.00000  1.00000\n-9         0.14639      host ceph03\n 1    hdd  0.04880          osd.1        up   1.00000  1.00000\n 5    hdd  0.04880          osd.5        up   1.00000  1.00000\n 9    hdd  0.04880          osd.9        up   1.00000  1.00000\n-3         0.14639      host ceph04\n 0    hdd  0.04880          osd.0        up   1.00000  1.00000\n 4    hdd  0.04880          osd.4        up   1.00000  1.00000\n 8    hdd  0.04880          osd.8        up   1.00000  1.00000\n\n# 测试上传一个文件并且查看文件的状态和信息。\n[ceph: root@ceph01 rpm]# pwd\n/var/lib/rpm\n[ceph: root@ceph01 rpm]# ls -lh ./Packages\n-rw-r--r-- 1 root root 20M Jul  8 17:56 ./Packages\n\n[ceph: root@ceph01 rpm]# rados put msg1 ./Packages --pool=test\n[ceph: root@ceph01 rpm]# rados put msg1 ./Packages --pool=test-64\n\n[ceph: root@ceph01 rpm]# rados ls --pool=test\nmsg1\n\n[ceph: root@ceph01 rpm]# ceph osd map test msg1\nosdmap e68 pool 'test' (2) object 'msg1' -> pg 2.c833d430 (2.10) -> up ([9,3,0], p9) acting ([9,3,0], p9)\n\n[ceph: root@ceph01 rpm]# ceph osd map test-64 msg1\nosdmap e68 pool 'test-64' (3) object 'msg1' -> pg 3.c833d430 (3.30) -> up ([2,0,3], p2) acting ([2,0,3], p2)\n\n# 删除上传的文件\n[ceph: root@ceph01 rpm]# rados rm msg1 --pool=test\n[ceph: root@ceph01 rpm]# rados rm msg1 --pool=test-64\n[ceph: root@ceph01 rpm]# rados ls --pool=test\n[ceph: root@ceph01 rpm]# rados ls --pool=test-64\n\n# 删除刚刚添加的存储池。(临时的解决方案是使用如下的命令， 永久生效的配置写入 /etc/ceph/ceph.conf)\n[ceph: root@ceph01 ~]# ceph tell mon.* injectargs '--mon-allow-pool-delete=true'\nmon.ceph01: &#123;&#125;\nmon.ceph01: mon_allow_pool_delete = 'true'\nmon.ceph02: &#123;&#125;\nmon.ceph02: mon_allow_pool_delete = 'true'\nmon.ceph03: &#123;&#125;\nmon.ceph03: mon_allow_pool_delete = 'true'\n\n# 执行删除命令就不会报错了。\n[ceph: root@ceph01 ~]# ceph osd pool rm test test --yes-i-really-really-mean-it\npool 'test' removed\n[ceph: root@ceph01 ~]# ceph osd pool rm test-64 test-64 --yes-i-really-really-mean-it\npool 'test-64' removed\n\n# 创建一个KVM的块设备池。\n[ceph: root@ceph01 ~]# ceph osd pool create kvm 256 256\npool 'kvm' created\n\n# 设置Application enable flag\n[ceph: root@ceph01 ~]# ceph osd pool application enable kvm rbd\nenabled application 'rbd' on pool 'kvm'\n# 初始化池。\n[ceph: root@ceph01 ~]# rbd pool init -p kvm\n# 创建一个可挂载的镜像。\n[ceph: root@ceph01 ~]# rbd create disk01 --size 5G --pool kvm\n# 查看镜像的相关信息\n[ceph: root@ceph01 ~]# rbd ls --pool kvm\ndisk01\n  size 5 GiB in 1280 objects\n  order 22 (4 MiB objects)\n  snapshot_count: 0\n  id: 1703d21a8f2ba\n  block_name_prefix: rbd_data.1703d21a8f2ba\n  format: 2\n  features: layering, exclusive-lock, object-map, fast-diff, deep-flatten\n  op_features:\n  flags:\n  create_timestamp: Mon Aug 23 09:13:58 2021\n  access_timestamp: Mon Aug 23 09:13:58 2021\n  modify_timestamp: Mon Aug 23 09:13:58 2021\n\n# 安装Ceph的客户端以及rbd命令。\n[root@HaydenArchDesktop hayden]# sudo pacman -S ceph\n[root@HaydenArchDesktop hayden]# rbd -p kvm map disk01\n/dev/rbd0\n[root@HaydenArchDesktop hayden]# lsblk\nNAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nrbd0        254:0    0     5G  0 disk\n\n# 格式化文件系统。 \n[root@HaydenArchDesktop test]# mkfs.xfs /dev/rbd0\n# 挂载。\n[root@HaydenArchDesktop test]# mount /dev/rbd0 /opt/test/\n# 移除挂载\n[root@HaydenArchDesktop test]# umount /opt/test\n# 取消这个映射关系。\n[root@HaydenArchDesktop test]# rbd unmap /dev/rbd0\n","categories":["Linux"],"tags":["Ceph"]},{"title":"Ceph Cluster 01 - Installation","url":"/2021/08/21/Linux/Linux_Ceph01/","content":"Ceph Installation Record.\n\n\nINITINIT OSOS Version: Fedora 34 ServerCEPH Version: v15.2.0 (Octopus) +DOCKER Version: 20.10.8\nCONFIG hostname and IPvim /etc/hosts\n  192.168.122.121 ceph01 ceph01.liarlee.site\n  192.168.122.122 ceph02 ceph02.liarlee.site\n  192.168.122.123 ceph03 ceph03.liarlee.site\n  192.168.122.124 ceph04 ceph04.liarlee.site\n\n[root@ceph01 ~]$ ssh 192.168.122.121 echo \"ceph01.liarlee.site\" > /etc/hostname\n[root@ceph01 ~]$ ssh 192.168.122.122 echo \"ceph02.liarlee.site\" > /etc/hostname\n[root@ceph01 ~]$ ssh 192.168.122.123 echo \"ceph03.liarlee.site\" > /etc/hostname\n[root@ceph01 ~]$ ssh 192.168.122.124 echo \"ceph04.liarlee.site\" > /etc/hostname\n\n[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.121/24\n[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.122/24\n[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.123/24\n[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.124/24\n\n[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.gateway 192.168.122.1\n[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.dns 192.168.122.1\n[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.method manual\n[root@ceph01 ~]$ nmcli conn up enp1s0\n\n[root@ceph01 ~]$ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld\n[root@ceph01 ~]$ vim /etc/config/selinux # change it to disabled\n\nCONFIG dnf repo# config docker\n[root@ceph01 ~]$ wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/fedora/docker-ce.repo\n[root@ceph01 ~]$ sudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo\n# Add GPG Key.\n[root@ceph01 ~]$ sudo rpm --import 'https://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc'\n# update cache and install cephadm.\n[root@ceph01 ~]$ dnf makecache -y &amp;&amp; dnf install curl wget htop -y\n[root@ceph01 ~]$ dnf install cephadm -y\n\nINSTALL Docker[root@ceph01 ~]$ sudo dnf remove docker \\\n                  docker-client \\\n                  docker-client-latest \\\n                  docker-common \\\n                  docker-latest \\\n                  docker-latest-logrotate \\\n                  docker-logrotate \\\n                  docker-selinux \\\n                  docker-engine-selinux \\\n                  docker-engine\n[root@ceph01 ~]$ sudo dnf install docker-ce docker-ce-cli containerd.io -y\n[root@ceph01 ~]$ sudo systemctl start docker containerd\n[root@ceph01 ~]$ sudo systemctl enable docker containerd\n\n# Config proxy for docker daemon.\n[root@ceph01 ~]$ mkdir /etc/systemd/system/docker.service.d/\n[root@ceph01 docker.service.d]$ cat http-proxy.conf\n  [Service]\n  Environment=\"HTTP_PROXY=http://192.168.31.199:7890/\"\n  Environment=\"HTTPS_PROXY=http://192.168.31.199:7890/\"\n\nINSTALL CEPHCephadm tools were default in Fedora Repo, No need to change the repo to tsinghua or aliyun. Just install. WOW ~ Fedora YYDS.\nBOOTSTRAP ceph[root@ceph01 ~]$ cephadm bootstrap --mon-ip 192.168.122.121 --allow-fqdn-hostname\nPlease notice the output, context include username and password and dashboard address.\nCeph Dashboard is now available at:\n\n       URL: https://localhost.localdomain:8443/\n      User: admin\n  Password: 20jrekw4ko\n\nEnabling client.admin keyring and conf on hosts with \"admin\" label\nYou can access the Ceph CLI with:\n\n  sudo /usr/sbin/cephadm shell --fsid e8997974-029f-11ec-a59a-525400c06f36 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring\n\nUSE ceph shell# temprary use\n[root@ceph01 ~]$ cephadm shell -- ceph -s\n\n# start a interactive shell\n[root@ceph01 ~]$ cephadm shell\n\n# check the ceph status \n[ceph: root@ceph01 /]$ ceph -s\n\n# list ceph hosts\n[ceph: root@ceph01 ceph]$ ceph orch host ls --format yaml\n\n# general a new ssh key for cephadm\n[ceph: root@ceph01 ceph]$ cephadm get-pub-key > /etc/ceph/ceph.pub\n\n# copy new key to hosts\n[ceph: root@ceph01 ceph]$ ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.122.122\n[ceph: root@ceph01 ceph]$ ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.122.123\n[ceph: root@ceph01 ceph]$ ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.122.124\n\nMAINTAIN HostsADD hosts[ceph: root@ceph01 ceph]$ ceph orch host add ceph02.liarlee.site 192.168.122.122\n[ceph: root@ceph01 ceph]$ ceph orch host add ceph03.liarlee.site 192.168.122.123\n[ceph: root@ceph01 ceph]$ ceph orch host add ceph04.liarlee.site 192.168.122.124\n\n# set mon sub-network \n[ceph: root@ceph01 /]$ ceph config set mon public_network 192.168.122.0/24\n\n\nADD osd# auto-detect available devices (need time to sync the status 1 by 1)\n# NOTE: Strangely enough, the command automatically recognizes all devices, including the ZRAM!  QAQ.....\n[ceph: root@ceph01 /]$ ceph orch apply osd --all-available-devices\n\n# list devices\n[ceph: root@ceph01 /]$ ceph orch device ls\nHostname             Path        Type  Serial  Size   Health   Ident  Fault  Available\nceph01.liarlee.site  /dev/vdb    hdd           21.4G  Unknown  N/A    N/A    Yes\nceph01.liarlee.site  /dev/vdc    hdd           21.4G  Unknown  N/A    N/A    Yes\nceph01.liarlee.site  /dev/vdd    hdd           21.4G  Unknown  N/A    N/A    Yes\nceph01.liarlee.site  /dev/zram0  ssd           2071M  Unknown  N/A    N/A    No\n\n# MANUAL ADD OSD\n# It is not necessary. \n# INIT DISK in MON node\n[ceph: root@ceph01 /]$ ceph orch daemon add osd ceph01.liarlee.site:/dev/vdb\nCreated osd(s) 0 on host 'ceph01.liarlee.site'\n[ceph: root@ceph01 /]$ ceph orch daemon add osd ceph01.liarlee.site:/dev/vdc\nCreated osd(s) 1 on host 'ceph01.liarlee.site'\n[ceph: root@ceph01 /]$ ceph orch daemon add osd ceph01.liarlee.site:/dev/vdd\nCreated osd(s) 2 on host 'ceph01.liarlee.site'\n\n# MANUAL DELETE OSD\n# It is not necessary. \n[ceph: root@ceph01 /]$ ceph orch osd rm 0\nScheduled OSD(s) for removal\n[ceph: root@ceph01 /]$ ceph orch osd rm 1\nScheduled OSD(s) for removal\n[ceph: root@ceph01 /]$ ceph orch osd rm 2\nScheduled OSD(s) for removal\n\n# Enable the device scan enhencement, show the infomation about Health, Ident, Fault. \nceph config set mgr mgr/cephadm/device_enhanced_scan true\n\n# OSD memory auto tune, for performance maybe.\nceph config set osd osd_memory_target_autotune true\n\n# mark the auto manage to true.\n[ceph: root@ceph01 /]$ ceph orch apply osd --all-available-devices --unmanaged=true\n[ceph: root@ceph01 /]$ ceph orch device ls --wide\n[ceph: root@ceph01 /]$ ceph osd status\n\n# Check Deleting Status\n[ceph: root@ceph01 /]$ ceph orch osd rm status\nOSD_ID  HOST                 STATE                    PG_COUNT  REPLACE  FORCE  DRAIN_STARTED_AT\n0       ceph01.liarlee.site  done, waiting for purge  0         False    False  None\n1       ceph01.liarlee.site  started                  0         False    False  None\n2       ceph01.liarlee.site  started                  0         False    False  None\n\n# remove the devices and reuse in cluster.\n[ceph: root@ceph01 /]$ ceph orch apply osd --all-available-devices --unmanaged=true\n[ceph: root@ceph01 /]$ ceph orch device zap ceph01.liarlee.site /dev/vdb --force\n[ceph: root@ceph01 /]$ ceph orch device zap ceph01.liarlee.site /dev/vdc --force\n[ceph: root@ceph01 /]$ ceph orch device zap ceph01.liarlee.site /dev/vdd --force\n[ceph: root@ceph01 /]$ ceph orch apply osd --all-available-devices --unmanaged=false\n\nMANAGE services# reduce mon instance to 3\n[ceph: root@ceph01 /]$ ceph orch ls mon\nNAME                       PORTS        RUNNING  REFRESHED  AGE  PLACEMENT\nmon                                         4/5  2m ago     10h  count:5\n\n[ceph: root@ceph01 /]$ ceph orch apply mon 3\nScheduled mon update...\n\n[ceph: root@ceph01 /]$ ceph orch ls mon\nmon                                         4/3  4m ago     39s  count:3\n\n[ceph: root@ceph01 /]$ ceph orch redeploy mon\nScheduled to redeploy mon.ceph01.liarlee.site on host 'ceph01.liarlee.site'\nScheduled to redeploy mon.ceph02 on host 'ceph02.liarlee.site'\nScheduled to redeploy mon.ceph03 on host 'ceph03.liarlee.site'\n\nCOMPLETED[ceph: root@ceph01 /]$ ceph orch ls\nNAME                       PORTS        RUNNING  REFRESHED  AGE  PLACEMENT\nalertmanager               ?:9093,9094      1/1  0s ago     10h  count:1\ncrash                                       4/4  9m ago     10h  *\ngrafana                    ?:3000           1/1  0s ago     10h  count:1\nmgr                                         2/2  9m ago     10h  count:2\nmon                                         3/3  9m ago     5m   count:3\nnode-exporter              ?:9100           4/4  9m ago     10h  *\nosd.all-available-devices                 12/16  9m ago     20m  *\nprometheus                 ?:9095           1/1  0s ago     10h  count:1\n\n[ceph: root@ceph01 /]$ ceph pg stat\n1 pgs: 1 active+clean; 0 B data, 82 MiB used, 240 GiB / 240 GiB avail\n\n[ceph: root@ceph01 /]$ ceph osd status\nID  HOST                  USED  AVAIL  WR OPS  WR DATA  RD OPS  RD DATA  STATE\n 0  ceph01.liarlee.site  7364k  19.9G      0        0       0        0   exists,up\n 1  ceph01.liarlee.site  7364k  19.9G      0        0       0        0   exists,up\n 2  ceph01.liarlee.site  7300k  19.9G      0        0       0        0   exists,up\n 3  ceph02.liarlee.site  7364k  19.9G      0        0       0        0   exists,up\n 4  ceph03.liarlee.site  6980k  19.9G      0        0       0        0   exists,up\n 5  ceph04.liarlee.site  6532k  19.9G      0        0       0        0   exists,up\n 6  ceph02.liarlee.site  7236k  19.9G      0        0       0        0   exists,up\n 7  ceph03.liarlee.site  6596k  19.9G      0        0       0        0   exists,up\n 8  ceph04.liarlee.site  6404k  19.9G      0        0       0        0   exists,up\n 9  ceph02.liarlee.site  7108k  19.9G      0        0       0        0   exists,up\n10  ceph04.liarlee.site  6596k  19.9G      0        0       0        0   exists,up\n11  ceph03.liarlee.site  7172k  19.9G      0        0       0        0   exists,up\n\nIMAGE\n","categories":["Linux"],"tags":["Ceph"]},{"title":"Ubuntu 18.04 内核编译初试","url":"/2021/07/01/Linux/Linux_Ubuntu-kernel-Complie/","content":"Ubuntu 18.04 上编译内核，生成deb安装包的过程和遇到的问题。 \n使用工具： Linux-tkg \n系统版本： Ubuntu18.04 \n内核版本： 5.12-muqss-6ms-skylake\n\n\n背景说明看到B站大佬的教学视频， 使用Linuxtkg进行内核的编译打包和添加muqssCPU调度器， 想自己尝试一下， 同时熟悉一下Ubuntu常见的工具链，所以就开始做了这个事情。\n\n首先是需要对系统进行初始化， 需要一些底层的工具包。 在Ubuntu上面还是有些问题的， 这个坑自己踩了。\n]$ sudo apt install zstd git wget sudo bc rsync kmod cpio libelf-dev build-essential fakeroot libncurse5-dev libssl-dev ccache bison flex qtbase5-dev kernel-package\n\nNOTE: 这里面我遇到少了pkg但是没有明确报错的是：zstd &amp; kernel-package ，这两个如果没有正确的安装报错是比较模糊的， 完全不能定向到问题是缺少了这两个包。 （而官方的手册中也没有写需要安装kernel-package就比较坑爹\n\n之后就是下载Linux-tkg ，建议直接从Github下载， Linux-tkg official address\n]$ git clone https://github.com/Frogging-Family/linux-tkg.git\n\n在clone完成之后， 执行下面的.&#x2F;install.sh config, 按照提示回答相关的问题。 \n\n或者可以直接编辑其中的customization.cfg来设置默认的参数。（这个文件中都是有说明的，还算是比较详细， 一般可以直接看懂。 \n\n回答完成问题之后会生成对应的config文件。\n\n执行.&#x2F;install.sh install， 之后会有几个不同的发行版的选择， 只需要选择自己需要的即可， 如果是Deb系的会自动启动编译命令 make -j32 deb-pkg ,自动生成的软件包在.&#x2F;DEBS&#x2F;*.deb.直接安装即可。如果是rpm也会生成RPMS。\n\n如果中途报错 ， 我也不知道具体的处理方式。目前发现的问题如下：\n\n不同版本的ubuntu 表现是不同的， 震惊！ 同样的设置参数 ，同样的内核代码和补丁，在Ubuntu 18.04 和 20.04 ， 20.04 会成功。\n不同环境的结果是不同的， 在一台服务器上面 起Docker容器进行初始化之后编译 会成功的。 在笔记本上不一定会成功。 \n目前没有总结出任何的相关规律， 太任性了只能说。 \n如果是不打包成deb，目前的成功率是100%（包括直接将文件释放到系统中， 和打成rpm）。\n\n\n\n\n现在能想到的差不多这些吧 ， 反正编译出错就是dpkg-package ERROR 2 ， 我现在满脑子都是ERROR 2了， 告辞。\n","categories":[],"tags":["Linux"]},{"title":"Java连接数据库报错No subject alternative names present","url":"/2020/11/16/Mongo_JavaException-MangoDB-ssl/","content":"配置监控的时候需要配置和获取MongoDB的信息，使用华为云的MongoDBPaaS服务，如果开启了SSL就无法正常连接。\n\n\n报错如下： \nTimed out after 30000 ms while waiting to connect. Client view of cluster state is &#123;type=UNKNOWN, servers=[&#123;address=192.168.1.1:8635, type=UNKNOWN, state=CONNECTING, exception=&#123;com.mongodb.MongoSocketWriteException: Exception sending message&#125;, caused by &#123;javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative names present&#125;, caused by &#123;java.security.cert.CertificateException: No subject alternative names present&#125;&#125;]\n\n网上搜索到的结果基本上都是从开发的角度解决这个错误， 如果是从运维的方向上 ， 没有什么有效的方法可以解决这个问题么？比如通过一些参数或者设置。\n有的 ： \n\n在连接串里加上sslinvalidhostnameallowed&#x3D;true即可, 设置是直接允许无效的hostname证书， 算是不解决证书问题一个解决方案吧。\n\n所以 最后的连接参数变成： \nmongodb:&#x2F;&#x2F;192.168.1.1:8635&#x2F;test?authSource&#x3D;admin&amp;ssl&#x3D;true&amp;sslinvalidhostnameallowed&#x3D;true\n\n","categories":["Linux"],"tags":["Linux, JAVA"]},{"title":"Linux性能调优笔记","url":"/2020/08/20/Linux/Linux_PerformanceNote/","content":"应该是一个性能调优的书的笔记， 记不清了， 存货了属于是。\n\n\nLinux的性能调优CPU性能释放Process生命周期\n建立一个新的进程, 表示为父进程， 父进程进入Wait状态。\n父进程Fork()系统调用出来的一个子进程。\n子进程调用exec()对操作进行执行。\n子进程执行结束退出exit()。\n子进程变为Zombie进程。\n等待父进程回收，更新父进程的运行状态。\n\n进程与线程线程是可以在同一个进程下并发执行的执行单位，他们共享相同的地址，数据和运行空间。线程也叫做（LWP） - 轻量的进程。两者的区别在于，进程在同一个CPU上不能并发执行，且两个进程间不是共享资源的方式进行数据处理的。其他的地方， 进程和线程并无太大的区别，Linux的内核将使用一直的Manner对进程和线程进行调度和处理。\nThere are several thread implementations avaliable in linux operation system.\n\nLinux threads\n传统的Linux进程\n\nNative POSIX thread library\n内核 2.6 以后由红帽开发的进程模型。\n\nNext Generation POSIX Thread\nIBM开发的新的进程模型。\nNote：Linux系统中的环境变量 LD_ASSUME_KERNEL 。\n\n\n进程优先级\n优先级(控制调度的先后顺序): 优先级的范围是： 0 - 139。\nNice值(控制运行时间的长短): 每个进程的nice值，19 到 -20 ，nice值越大 分配的时间越长。19最低 ， -20最高。默认的nice值是0。\n\n进程上下文切换每一个进程有独立的数据存储，当多个进程在同一颗CPU切换时，内核会对进程需要的数据空间进行重定向，这个行为叫做上下文切换。Context Swtiching，负责进程的上下文切换以及调度。在多个进程切换的过程中， 每次切换都会触发一次上下文的切换，是导致性能下降的主要原因。\n中断CPU的终端控制通常由 硬中断 和 软中断为主， 硬中断通常见于硬件设备，鼠标键盘网卡硬盘设备。软中断常见于TCP&#x2F;IP协议操作，SCSI协议操作。\nNote： 中断的信息显示在： /proc/interrepts。\nNote：在多个CPU的系统中，可以将中断集中绑定在某一颗物理的CPU上，可以有效的改善系统的性能。\n进程的状态\nRUNNING（正常的进程）\nSTOPPED（已经停止的进程）\nUNINTERREPTIBLE（Disk I&#x2F;O）\nINTERREPTIBLE（Keyboard I&#x2F;O）\nZOMBIE（只能通过结束或重启父进程来回收僵尸进程）\n\n进程的内存空间进程的内存地址空间：(由顶至低顺序为)Text Segment， Data Segment，Heap Segment， Stack Segment\n\nText Segment ： 存储进程可执行代码的部分，只读模式。\nData Segment： 包括三个部分\nData： 数据片段， 初始数据的存储，例如静态变量。\nBSS：填零数据的存储。数据初始化为0。\nHeap（堆内存）： malloc()按需分配的动态内存，堆内存向更高的地址发展。\n\n\nStack Segment： 栈内存。存储本地变量， 方法参数，方法返回地址。栈内存向更低的地址发展。\n\nNOTE：可以使用命令pmap查看一个用户空间进程的地址分配情况。可以使用ps命令查看总的内存分配情况。\nCPU的NUMA Node\n一颗物理CPU 8逻辑核，两个NUMA节点。\n\n4CPU为一组， 在同一个NUMA节点中。\n\nHT技术提供了一个物理两个逻辑CPU。\n\n在 1 情况下，一般不触发跨越NUMA节点的负载均衡，除非子节点过载。\n在 2 情况下，提供了调度器tick的 时间调度 和 时间片 调度。\n在 3 情况下， 提供调度器tick时间的负载均衡调度。\n\n\n\n\n内存性能Linux内存的结构\n物理内存物理的内存的分配分别有32位和64位的不同，\n\n32位系统中只能Linux内核可以直接管理的内存空间只有第一个 1 GB（扣除预留的部分只有896M） ， 剩下的空间需要映射到前面的1GB空间中。这种映射会使性能下降，但是对于应用程序来说是透明且无感的。\n64位系统中将ZONE-NORMAL的区域扩展到了64GB - 128GB， 就不需要这种映射操作了。\n\n虚拟内存虚拟内存的地址布局\n\n32位架构中，一个进程可以使用的虚拟内存的空间最大只能有4GB。内存被分为了 3GB的用户空间和1GB的内核空间。\n64位架构中完全没有这种限制，每个进程都可以使用全部的内存空间。\n\n虚拟内存的管理物理内存通常对于应用或者用户是不可见的，Linux内核会将任何内存自动映射到虚拟内存。\n应用不直接使用物理内存而是向内核申请一个确切空间的虚拟内存映射，并在虚拟内存中接收并处理内存的映射关系。\n并且，虚拟内存不是必须映射到物理内存，还可以映射到在硬盘子系统中的Swap文件。\n应用通常也不直接写入硬盘子系统，而是写入数据到Cache&#x2F;Buffer中。pdflush内核线程在合适的时间负责将Cache和Buffer中的数据刷写到硬盘。\nLinux虚拟内存管理器分配全部的未使用虚拟内存作为磁盘的cache，其他的操作系统只使用内存中的一部分。\n同样的，交换的空间的管理同样也是如此，事实上交换空间占用程度并不代表系统的瓶颈所在，相反证明了linux系统资源调度上的高效。\n页帧的分配 - 内存分页\nPage - 页是在物理内存（Page Frame） 或者 虚拟内存中的连续线性地址空间。\nLinux通过控制页单元来控制内存的分配，一页的大小是 4Kb。\n内核了解那些分页是可用的，以及他们的确切位置。\n\nBuddy SystemLinux使用这种机制来进行空闲页的管理和维护，并对申请进行分配，始终试图保持内存分页的连续性。当内存的分配失败的时候，会内存的页帧回收。\nPage Frame Reclaiming当所有的页已经处于不可用的状态（unavalible）,会触发内存的回收机制，将暂时不在使用中的，或者在使用中但是优先级低的内存重新分配，这种机制叫做内存回收。内核线程pswapd和内核函数try_to_free_page()负责执行这个动作。\n\n文件系统Linux可以支持多种多样的文件系统，这得益于内核的VFS技术，VFS是介于用户进程和文件系统之间的抽象层。由于VFS的存在，那么用户进程无需知道文件系统的类型，就可以直接进行文件系统的使用。\n用户进程调用文件系统的流程：\nUser Process –&gt; System Call –&gt; VFS –&gt; Variety of supported file system\n日志：\nExt2简单的文件系统，无日志记录\nExt2 文件系统（BlockGroup）结构：\n\nSuperBlock： 存储信息，在每一个BlockGroup的前面都有一个SuperBlock。\n\nBlockGroupDesciptor： 存储BlockGroup的信息。\n\nData Block Bitmaps： 空闲的数据块管理。\n\ni-node Bitmap： 空闲的Inode管理\n\ni-node Tables： inode的table存储位置。记录了文件的基本信息，例如： uid，gid, atime, ctime, mtime, dtime,指向数据块的位置。\n\nData blocks: 实际用户数据的存储位置。\n文件系统查找文件的过程： 先在&#x2F;下查看inode信息，查看所找文件的信息，按照路径持续查找，直到找到了文件的inode信息，通过inode提供的信息去数据块读取数据。\n\n\nExt3ext3文件系统是ext2 的升级版，主要的变更是支持了文件系统的日志，\next3支持的日志模式有三种：\n\njournal - 全量记录，数据也元数据都通过日志记录。\nOrdered - 只有元数据会记录日志。\nWriteback - 记录了元数据的操作，无法保证数据的一致性，突然终止会导致旧数据的出现。\n\nxfsxfs是新一代的日志文件系统，性能和稳定性都不错。其他资料待补全。\n\nIO子系统进程是如何使用IO子系统进行数据交换的？\n\n进程通过write()系统调用，请求一个文件的写入。\n内核更新Pagecache映射到文件。\npdflush内核线程处理pagecache到硬盘。\n文件系统层将每个Blockbuffer组成一个Bio结构，提交和写入到块设备层。\n块设备层得到上层的请求，执行IO电梯算法操作将数据推入写入队列。\n存储驱动接管并执行写入。\n硬盘硬件设备执行写入到盘片。\n\n关于缓存(cache)?\n由于CPU和硬盘的速度差距太大，因此需要缓存来进行临时的数据存储。\n速度递减的结构是：\nCPU –&gt; cache –&gt; RAM –&gt; Disk\n块层 Block Layer关键的数据结构就是bio结构，bio结构是一种接口，存在于 文件系统层 和 块层之间。\nbio就是将相邻的block buffer块整合到一起，发送bio到块层。\n块大小直接影响服务器性能的设置。如果需要存储的文件尺寸比较大，那么设置大的blocksize性能更好。\nIO电梯算法\nAnticipatory\nCFQ\nBFQ\nDeadline\nMQ-deadline\nNOOP\nNONE\n\n存储驱动\nSCSI\nRAID\n\n\n网络子系统netstat -n | awk &#39;&#x2F;^tcp&#x2F; &#123;++S[$NF]&#125; END &#123;for (a in S) print a, S[a]&#125;&#39;\n# 查看所有的连接状态\n\n\n\n当一个应用需要发送网络数据流程：\n\n应用打包自己的数据。\n应用通过套接字接口写入数据。\nsocket buffer用于处理需要被发送的数据。缓冲区引用了数据，并贯穿所有的层。\n在每个层中，执行相应的操作，例如数据的封包，添加数据的报文首部。\n从网卡的物理接口发出。\n以太网帧抵达对端的网卡接口。\n如果MAC地址匹配对端网卡的地址，数据帧移动到物理网卡的Buffer。\n移动数据包到socket buffer中，同时发出一次CPU硬中断。\n数据包逐层解包，直到抵达可以解释的应用层。\n\n\n通过&#x2F;proc&#x2F;sys&#x2F;net目录可以进行网络子系统的调优。\n​\t&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;rmem_max​\t&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;rmem_default​\t&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;wmem_max​\t&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;wmem_default​\t&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_mem​\t&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_rmem​\t&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_wmem\nNetwork API(NAPI)NAPI是一种新的网络处理方式，每次对数据包的操作都会触发系统中断，新的方式使用的是poll的方式。\nNAPI 是 Linux 上采用的一种提高网络处理效率的技术，它的核心概念就是不采用中断的方式读取数据，而代之以首先采用中断唤醒数据接收的服务程序，然后 POLL 的方法来轮询数据，\nhttps://www.ibm.com/developerworks/cn/linux/l-napi/\nNetfilterLinux有使用了内核的Netfilter模块，可以通过Iptables进行管理和控制。\nNetfilter提供了默认的集中功能：\n\n通过匹配规则，进行网络数据包的过滤。\n通过匹配规则，提供修改数据包地址信息。\n\n\nNetfilter可以使用的过滤属性有；\n\n􏰀  Network interface\n􏰀  IP address, IP address range, subnet\n􏰀  Protocol\n􏰀  ICMP Type\n􏰀  Port\n􏰀  TCP flag\n􏰀  State\n\n\nPrerouting – Routing – Forward – Postrouting\n​\t\t\t\t\t\t  -                              -\n​           \t\t\t Input                     Output\n\nNetfilter可以使用执行的操作有：\n\nACCEPT\nDROP\nREJECT\nLOG\nMASQUERADE, SNAT, DNAT, REDIRECT\n\nNetfilter可以检测的状态；\n\nNEW\n\nESTABLISHED\n\nRELATED\n\nINVALID\n还可以单独使用独立的模块用来进行分析和过滤相关的数据包。\n\n\nTCPIPconnection establishment - 连接维持，在应用的数据被传送之前，TCP一直属于维持的状态，创建连接的时候叫做三次握手。断开连接的时候进行四次挥手。\n三次握手：\n\n客户端发布SYN数据包，请求对端连接。\n服务端收到SYN，回复SYN+ACK\n客户端发布ACK，开始维持连接。\n\n四次挥手：\n\n客户端 发送 FIN\n服务端发送ACK\n服务端发送 FIN\n客户端发送 ACK\n\n\nTCPIP流量控制\n\n滑动窗口： 双方协商传输数据包大小。\n\n负载\n\n校验和offload\n用于校验数据的字段计算的负载。\n\nTCP段Offload（TSO）\n用于计算大于MTU的数据包的消耗。\n\n\n网卡聚合： Bonding module\n网卡的聚合可以将多个网卡绑定为一个物理网卡使用， Windows下叫做 Teaming。能够提供比较初级的负载均衡和容错。可以直接提高系统的性能表现。\n性能指标在调试系统之前，需要了解的是相关性能指标的含义，由于是开源的系统，所以可用的工具多种多样，但是监控的指标是一致的。\nCPU性能指标\nCPU使用率 - 每一颗处理器的利用率，直观的指标。\n\nUser Time： CPU运行在用户空间的时间，包括Nice Time，运行在用户空间的时间越长越好。\n\nSystem Time： CPU用于处理内核级别操作的时间，包括IRQ和Softirq time。维持在较高的水平说明系统的瓶颈可能是网络或者驱动栈。通常情况下，内核时间越短越好。\n\nWaiting： 等待IO的时间。类似Blocked的数值，等待的时间越短越好。如果太多时间花在等待IO，你需要排查IO子系统的性能。\n\nIdle Time： 系统的等待任务的空闲时间，描述CPU空闲的百分比。\n\nNice Time： CPU百分比，重新分配进程Nice值所占的百分比。\n\nLoad Average： Rolling average of the sum of the followings: \n\n进程队列等待被执行的数量。\n不可中断任务等待被完成的数量。（IO操作的进程）\n\n\nRunable Processes：描述准备执行的进程，在持续的时间中不应该超过物理CPU核心数量的10倍。超过就是CPU瓶颈。\n\nBlocked： 等待IO完成的进程，直接指向IO瓶颈。\n\nContext Switch： 表示大量的线程切换，数值过高且大量系统中断可能代表了信号驱动或应用问题。这个参数通常不期望过高，因为CPU的缓存应该可以处理进程间切换的数据，但是某些是必要的。\n\nInterrupts： 中断的值包括硬中断和软中断，硬中断更不利于系统的性能释放，高中断值是软件瓶颈的迹象，同时也存在于内核 或 驱动。 要注意的是 CPU的中断值可能是包括CPUclock导致的。\n\n\n内存指标\nFree Memory： 和其他的操作系统来进行比较，Linux的内存使用量不是主要的关心指标，因为Linux内核采用的虚拟内存管理器，会将未使用的内存作为文件系统的cache，所以，减除Buffers和Cache才是真正未使用的内存量。 Free &#x3D; Used - Buffer - cache\n\nSwap Usage： 交换空间的使用量，使用量不是主要的指标，相比较更应该查看Swap IN&#x2F;OUT，才可以看出是否为内存瓶颈。数值在200-300+ page&#x2F;s 并且状况持续才说明是内存的瓶颈。\n\nBuffer and Cache:  Cache分配给文件系统和块设备缓存。？ 块设备被写入之前和成Bio结构的时候使用的buffer空间？\n\nSlabs： 内核使用的内存空间。值得注意的是内核的Pages不能被Pageout到硬盘。\n\nActive versus Inactive Memory： 提供数据的内存空间叫做活动内存。未活动的内存类似于候选，等待被kswapd进程交换到硬盘。\n\n\n网络指标\nPackets Received and sent : 数据包的发送量和接收量。关于网络的状况。\nBytes Received and sent： 数据字节数的发送量和接收量。\nCollisions per second： 数据包冲突的数量。一般来讲不是服务器的问题，是网络设备的问题，常见的是在Hub网络中。\nPackets Dropped： 内核丢弃的数据包，通常情况下和防火墙规则以及网络缓冲区溢出有关，默认较少。\nOverruns: 展示了缓冲区溢出的次数。 结合 Packets Dropped来分析可能的瓶颈，缓冲区 或者 网络数据队列长度。\nErrors： 数据包错误。网络数据无法完成校验或者网线的损坏。\n\n块设备指标\nIOwait ： IOwait表示CPU在等待IO操作的完成，持续的数值偏高说明是IO的瓶颈。\nAverage queue length： 大量IO请求未完成，通常情况下在 2 - 3 之间是最佳的值。 如果高于说明是IO瓶颈。\nAverage Wait： 从请求到得到服务的平均时间。包括队列等待的时间和请求执行的时间。\n\n\n观察性能的工具\ntop\n通过top命令可以查看所有的进程， 观察使用CPU高的进程，可以kill命令停止它；或者观察TIME过低的进程，使用renice命令提高进程在CPU上的调度时间。\n常用的命令：\nt  – 显示Summary信息。关闭或者不同的风格\nm – 显示内存的信息。 关闭或者使用不同的风格。\nA – 显示多种不同的参数来观察进程。 Def ， Job ， Mem， Usr， 四个维度；默认， 任务，内存， 用户。\nf – 交互式配置界面。\no – 交互式的选择排序方式。\nr – renice\nk – kill\n\nvmstat\nvmstat 提供的信息是从硬件和物理参数的角度。vmstat 第一次的数据显示的是从上一次重启到现在的平均值，所以不应该使用。\n显示的参数如下：\n[root@izbp14rk86kli4eecj39swz ~]# vmstat 2\nprocs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----\n r  b   swpd   free   buff  cache   si   so    bi    bo   in   cs us sy id wa st\n 2  0      0 1074952 125680 544184    0    0    20   366   58  136  0  0 99  0  0\n 0  0      0 1074952 125680 544224    0    0     0     0   70  122  0  0 100  0  0\n 0  0      0 1074936 125680 544228    0    0     0     0   69  119  0  0 99  0  0\n 0  0      0 1074952 125688 544224    0    0     0    10   78  128  0  0 100  0  0\n 0  0      0 1074952 125688 544232    0    0     0     0  105  209  1  1 99  0  0\n\n上面的第一行就不在解释了， 分别是 进程 ， 内存， 交换 ， IO， 系统 ， 和 CPU。直观且易懂。\n详细参数如下：\nr - 等待运行的进程数量\nb - Uninterreptable Sleep的进程数量\nswpd： VirtualMemory的值 （KB）\nfree： IdleMemory的值（KB）\nbuff： 被buffer使用的内存量（KB）\ncache： 被cache使用的内存量（KB）\nsi： Swapin 内存从硬盘换入的数据量 （KBps）\nso： Swapout 内存交换到硬盘的数据量 （KBps）\nbi： blockin， 发送到块设备的块数量（blocks&#x2F;s）\nbo： blockout ， 从块设备接收的块数量（blocks&#x2F;s）\nin： 包括时钟在内的那每秒中断数\ncs： 进程上下文每秒切换的次数\nus：非内核代码使用的CPU百分比， 包括用户时间和Nice时间。\nsy：内核代码使用的CPU时间。\nid： CPU的空闲时间。\nwa：等待IO操作花费的时间。\nst：虚拟化层操作花费的时间。\n其他可用的命令：\nVmstat -m – 显示内核占用内存的分配情况。\nVmstat -a – 将内存的使用量分为活动内存和非活动内存。\nvmstat -n 2 10 – 展示vmstat的结果10次， 每两秒一次。\n\nuptime OR w\n观察系统的负载状况和当前登陆到系统的用户。 三个load average的值，1 5 15 分钟的系统负载情况， 主要是观察状况， 前面介绍了 ， 计算的方式是CPU任务队列长度 和 不可中断进程的数量（IO操作的影响）。\n\nps OR pstree\n观察系统进程的列表和系统进程列表树状图。\n在ps -elFL命令中，有几个不常用的列 ： \n\nWCHAN： 休眠中的进程使用的内核函数名称。 - 表示进程正在运行；* 表示进程有多线程或者ps无法详细显示。\nRSS ： 常驻内存集。进程使用的非交换物理内存。单位是： KB\nPSR： 进程运行所在的CPU编号。\n\n\nfree\nfree命令是查看内存的详细信息使用。\nfree的命令在最新的操作系统已经直观的显示内存的每个参数及细节。直接能看懂了， 老版本看 used下的Buffer&#x2F;Cache行数值，即使已经使用的内存；看free下的Mem行， 即使未使用的内存值。其余直接查看即可。\n\n\n\nCPU整合的不同架构：\nNUMA，SMP，MPP的概念：\nhttps://www.cnblogs.com/yubo/archive/2010/04/23/1718810.html\n调优方法安装的考虑安装系统之前就应该考虑做什么使用， 采用什么样的配置，CPU（数量），内存（大小， 多通道），硬盘（容量，RAID），网络（带宽，是否聚合，是否需要Binding）等等问题。\n通过这些考虑可以避免很多后续的问题。\n\n收集配置\ndmesg\nulimit\n关闭不用的守护进程\n改变Runlevel\n是否需要SELinux\n是否需要编译内核\n\n\n\n\n更改内核参数\n系统的内核参数在&#x2F;proc&#x2F;sys目录下，有如下的几个方面：\n\nvm\nnet\nfs\nabi\nKernel\n\n可以通过sysctl命令进行调节， 也可以写入配置文件&#x2F;etc&#x2F;sysctl.conf\n\n\n进程子系统SMT结构\n更改进程的优先级renice，使业务进程更多的得到CPU cycle。\n变更系统的中断处理，一般有两种方式，一种是将中断绑定给一颗物理CPU集中处理。还有一种是让物理CPU自动控制中断。\n\nNUMA结构\n使用numactl软件包中的numastat来查看NUMA节点的状态。\n如果numa_miss 的数值过高，考虑使用NUMA节点的亲和调节或者renice进程的数值。\n\n\n内存子系统内核行为\nvm.swappiness的调整\n内存交换的频度\n\n设置为0 ， 如果内存没有耗尽就不使用交换分区。\n设置为100，积极的使用交换分区。\n\n\nvm.dirty_background_ratio\n脏数据回写到硬盘的比例，按照百分比设置，触发pdflush内核进程的回写。\n\n\n交换分区Linux推荐使用多个硬盘，每个硬盘上建立交换分区，Linux会自动调度不同的交换分区来合理的使用空间。可以在&#x2F;etc&#x2F;fstab文件中进行交换分区的配置，可以调整优先级，使系统优先使用某几个分区。 (32767 is the highest and 0 is the lowest).\n/dev/sda2\tswap\tswap sw \t0 0\n/dev/sdb2\tswap\tswap sw \t0 0\n\n/dev/sda2\tswap\tswap sw,pri=1 \t0 0\n/dev/sdb2\tswap\tswap sw,pri=3 \t0 0\n\nhugeLTBfs该值对 虚拟机 和 数据库 这种需要使用大量的内存空间的应用 是非常有用的参数。\n&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;nr_hugepages 设置HugePages的容量。\ncat &#x2F;proc&#x2F;meminfo | grep huge  查看相关的信息。\n如果应用需要使用Hugepage，可以使用文件系统挂载的方式。\nmount -t hugetlbfs none /mnt/hugepages\n\n硬盘子系统安装的考量\n是否需要Raid ，如何组建Raid， 硬件Raid或者软件Raid。\n硬盘的性能计算。IOs PerSecond * 4kB &#x3D; 800kB\n\n分区的规划分区的规划， 规则和细节见： FHS\n内核调度算法常见的调度算法：\n\nAnticipatory\n\nCFQ\n\nBFQ\n\nDeadline\n\nMQ-deadline\n\nNOOP\n\nNONE\n硬盘的调度算法可以单独设置某个硬盘，也可以在系统启动的内核参数重设置。\nCat &#x2F;proc&#x2F;block&#x2F;sda&#x2F;queue&#x2F;schedular 查看sda的调度算法。\nelevator&#x3D;noop 在Grub启动配置内核文件的行中添加。\n\n\n文件系统\n文件系统的日志记录模式\n/dev/sdb1 /testfs ext3 defaults,data=writeback 0 0\n\n文件系统的块大小\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"ElasticSearch 02","url":"/2020/06/27/Linux/Linux_Elasticsearch-02/","content":"记录一下自己的集群安装过程和常见的命令。\n\n\nElastic search三节点的安装\n节点的名称和相关参数：\n\n\n\nHost-name\nIP\nCluster Name\nRole\n\n\n\nelk01\n192.168.122.101\nliarlee-elk\nElasticsearch\n\n\nelk02\n192.168.122.102\nliarlee-elk\nKibana\n\n\nelk03\n192.168.122.103\nliarlee-elk\nFilebeat\n\n\n\n使用清华的repo\n [elasticsearch]\nname&#x3D;Elasticsearch repository for 7.x packages\n#baseurl&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;packages&#x2F;7.x&#x2F;yum\nbaseurl&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;elasticstack&#x2F;7.x&#x2F;yum&#x2F;\ngpgcheck&#x3D;0\ngpgkey&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;GPG-KEY-elasticsearch\nenabled&#x3D;1\nautorefresh&#x3D;1\ntype&#x3D;rpm-md\n\n三个服务器都需要安装Elasticsearch并设置开机启动。\n # 安装\nyum makecache fast &amp;&amp; yum install -y elasticsearch \n# 编辑配置文件\nvim /etc/elasticsearch/elasticsearch.yml\n  node.master: true[只有其中的两台是主节点即可，剩下的03可以设置为False]\n  node.name: elk01 [节点的名称，可以自定义，但是一个集群的内部节点名称不能相同]\n  network.host: 192.168.122.101[每个机器的外部IP]\n  http.port: 9200\n  node.data: true [三个节点都可以存储数据]\n  cluster.name: liarlee-elk [集群的名称三个机器必须一致]\n  cluster.initial_master_nodes: [\"elk1.hayden.cluster\"]\n  discovery.zen.ping.unicast.hosts: [\"elk01\", \"elk02\", \"elk03\"]\n  discovery.zen.minimum_master_nodes: 1 [最少的master节点需要有一个]\n# 复制到02\nscp /etc/elasticsearch/elasticsearch.yml root@elk02:/etc/elasticsearch/elasticsearch.yml\n# 复制到03\nscp /etc/elasticsearch/elasticsearch.yml root@elk03:/etc/elasticsearch/elasticsearch.yml\n# 去对应的机器上修改机器名和相关字段，结束， 尝试启动。 \n\n确认三个节点的服务是否正常启动。\n # 使用curl命令访问节点的restfulAPI\ncurl http://elk01:9200/_cluster/health?v\ncurl http://elk01:9200/_cluster/health?pretty\ncurl http://elk02:9200\ncurl http://elk03:9200\n\n在02上安装kibana，并且开机启动。\n # 使用yum安装\nyum makecache fast &amp;&amp; yum install -y kibana \nsystemctl enable kibana &amp;&amp; systemctl start kibana\n\n访问Kibana的WebUI，可以正常的使用。\n # 通过API检查\ncurl http://elk02:5601\n# 通过WebUI检查\nfirefox http://elk02:5601/\n\n配置filebeat 无脑收集&#x2F;var&#x2F;log&#x2F;messages, 并查看上报的状态。\n # 安装filebeat， 开机启动\nDownload from ELK offical website : filebeat-7.7.1-linux-x86_64.tar.gz\ncd /opt/filebeat/filebeat-7.7.1-linux-x86_64\nvim ./filebeat.yml\n  filebeat.inputs:\n  - type: log\n    enabled: true\n    paths:\n          - /var/log/messages\n  output.elasticsearch:\n  \t\thosts: [\"192.168.122.101:9200\"]\n  setup.kibana:\n  \t\thost: \"192.168.122.102:5601\"\n# 运行\ncd /opt/filebeat/filebeat-7.7.1-linux-x86_64\nsudo nohup ./filebeat -e -c filebeat.yml >/dev/null 2>&amp;1 &amp;\n\n观察集群的日志的收集状况，我在这个时候已经没有其他的问题了…. Over.\n\n\n有些问题\n我自己的虚拟机我是经常性的暴力关机的，所以遇到了节点的状态同步不正确，这导致我的ES集群启动的时候总是只有一个节点在线 ，其他的节点无法加入， 可以在ES的日志中观察到，节点试图添加，但是无法成功加入， 这个时候只需要删除无法加入节点的Node数据即可。\n\n","categories":["Linux"],"tags":["ELK"]},{"title":"记一次旧LVM硬盘挂载失败","url":"/2020/05/30/Linux/Linux_Lvm2-member-mounterror/","content":"我之前的硬盘上是fedora默认的LVM分区，我换了硬盘之后，弄了一个硬盘盒，把旧的硬盘放进去，连到电脑上试图把旧的数据取出来。发现系统已经正确的识别了PV，VG，LV，但是不能挂载， 提示无法读取硬盘的Superblock 和提示 mount: unknown filesystem type ‘LVM2_member（这个提示说明你是直接挂载的&#x2F;dev&#x2F;sdx，LVM需要你挂载的应该是逻辑卷，不是物理设备）。\n\n\n问题是这样的HaydenArchLinux$ lvs\n  LV   VG                    Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert\n  home fedora_localhost-live -wi------- &lt;118.77g\n  root fedora_localhost-live -wi-------   70.00g\n  swap fedora_localhost-live -wi-------    7.75g\n  \n---\nHaydenArchLinux$ vgs\n  VG                    #PV #LV #SN Attr   VSize    VFree\n  fedora_localhost-live   1   3   0 wz--n- &lt;196.52g    0\n\n---\nHaydenArchLinux$ vgchange -ay /dev/fedora_localhost-live\n  device-mapper: create ioctl on fedora_localhost--live-swap LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFIj295MyZpNmiMsazbwuGuLsrDgl7u509 failed: Device or resource busy\n  device-mapper: create ioctl on fedora_localhost--live-home LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFqvyrcOAarRr7ol6o2DbieN2mIRCnqi0m failed: Device or resource busy\n  device-mapper: create ioctl on fedora_localhost--live-root LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFkTeSg9t8t8dy1jqHhzD6BRcOoJ7dG0H2 failed: Device or resource busy\n  0 logical volume(s) in volume group \"fedora_localhost-live\" now active\n  \n---\nHaydenArchLinux$ lvdisplay\n --- Logical volume ---\n  LV Path                /dev/fedora_localhost-live/home\n  LV Name                home\n  VG Name                fedora_localhost-live\n  LV UUID                qvyrcO-AarR-r7ol-6o2D-bieN-2mIR-Cnqi0m\n  LV Write Access        read/write\n  LV Creation host, time localhost-live, 2020-04-05 03:07:47 +0800\n  LV Status              NOT available\n  LV Size                &lt;118.77 GiB\n  Current LE             30405\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n\n可以注意看上面的vgchange的时候显示LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFqvyrcOAarRr7ol6o2DbieN2mIRCnqi0m failed: Device or resource busy， lvdisplay显示lv status是NOT avaliable。\n去Google了一下，大部分复制粘贴来的答案都是：\n两种方法，第一种是直接mount &#x2F;dev&#x2F;fedora_localhost-live&#x2F;home &#x2F;mnt;第二种是格式化硬盘。\n我不需要这种粗鲁的处理方式，第一种和第二种其实都是废话。\n\n处理方式是这样的简单说明一下，使用dmsetup命令，dmsetup是一个偏向底层的逻辑卷管理工具，可以对现在已经有的逻辑卷进行更改。我的初步怀疑是Arch自动识别挂载了旧硬盘的逻辑卷但是，使用了错误的参数，导致设备被占用但是我无法再次挂载使用。使用dmsetup remove 参数将系统现在已经识别出来的逻辑卷移除， 之后手动使用vgchange重新读取，问题解决了。\nHaydenArchLinux$ dmsetup remove /dev/fedora_localhost-live/home\nHaydenArchLinux$ vgchange -ay\n  3 logical volume(s) in volume group \"fedora_localhost-live\" now active\nHaydenArchLinux$ lvdisplay\n  --- Logical volume ---\n  LV Path                /dev/fedora_localhost-live/home\n  LV Name                home\n  VG Name                fedora_localhost-live\n  LV UUID                qvyrcO-AarR-r7ol-6o2D-bieN-2mIR-Cnqi0m\n  LV Write Access        read/write\n  LV Creation host, time localhost-live, 2020-04-05 03:07:47 +0800\n  LV Status              available\n  # open                 0\n  LV Size                &lt;118.77 GiB\n  Current LE             30405\n  Segments               1\n  Allocation             inherit\n  Read ahead sectors     auto\n  - currently set to     256\n  Block device           254:1\nHaydenArchLinux$ mount /dev/fedora_localhost-live/home /mnt\n\n如上才是正确的处理方式。\n","categories":["Linux"],"tags":["Linux"]},{"title":"为虚拟机开启内存大页","url":"/2020/05/02/Linux/Linux_Hugepage-configuration/","content":"Huge Pages是从Linux Kernel 2.6后被引入的。目的是使用更大的内存页面（memory page size） 以适应越来越大的系统内存，让操作系统可以支持现代硬件架构的大页面容量功能。透明大页（Transparent Huge Pages）缩写为THP，这个是RHEL 6（其它分支版本SUSE Linux Enterprise Server 11, and Oracle Linux 6 with earlier releases of Oracle Linux Unbreakable Enterprise Kernel 2 (UEK2)）开始引入的一个功能。具体可以参考官方文档。\n\n\n这两者有啥区别呢？\n\n这两者的区别在于大页的分配机制，标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式。\n\n使用大页的目的：\n\n增加内存寻址的命中率，如果使用旧的内存分页方式，操作系统需要管理很多很多的小的内存页面，查找和命中的效率比较低。\n想象一下， 你有一本1000页的书，你需要找到其中的第782页的第20行中的一个“我”字，那么计算机会从第一页开始翻动一页一页的看是否符合要求；现在我将书藉的每100页合成1页，那我们只需要顺序查看10次就可以找到这个字符所在的范围了，之后再去查看这个字符所在的具体位置，速度就会比之前一页一页找快得多。\n\n\n如何启用Hugepage：\n\n设置操作系统可用的最大内存值\nvim &#x2F;etc&#x2F;security&#x2F;limits.conf\n* soft memlock 8192\n* hard memlock 8192\n\n设置sysctl.conf\n# vim &#x2F;etc&#x2F;sysctl.conf\nvm.nr_hugepages &#x3D; 72708\n# sysctl -p\n\n根据我的测试 ，需要重启才会生效，Hugepage部分的内存会一直属于使用中的内存，并且一直被占用。我的配置用了16G内存之中的8G，所以开机之后，我的8G内存是一直使用中的状态， 无论你的应用是否真正的开始使用了这部分Hugepage.\n\n\n\n下面举例了如何查看Hugepages的信息：\n$ cat &#x2F;proc&#x2F;meminfo | grep Huge\nAnonHugePages:         0 kB\nShmemHugePages:        0 kB\nFileHugePages:         0 kB\nHugePages_Total:    4096\nHugePages_Free:     4096\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:         8388608 kB\n\n我的需求试使用Hugepage来分配给KVM中的虚拟机， 加速虚拟机的内存使用效率，所以当然是在虚拟机的配置文件中配置，编辑虚拟机&#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;HaydenGentoo.xml, 改成如下的样子：\n&lt;memory unit&#x3D;&#39;KiB&#39;&gt;4194304&lt;&#x2F;memory&gt;\n&lt;currentMemory unit&#x3D;&#39;KiB&#39;&gt;4194304&lt;&#x2F;currentMemory&gt;\n&lt;memoryBacking&gt;\n  &lt;hugepages&#x2F;&gt;\n&lt;&#x2F;memoryBacking&gt;\n\n之后重启虚拟机即可。\n可以观察到如下改变说明已经在使用了。\n$ cat &#x2F;proc&#x2F;meminfo | grep Huge    \nAnonHugePages:         0 kB\nShmemHugePages:        0 kB\nFileHugePages:         0 kB\nHugePages_Total:    4096\nHugePages_Free:     2048\nHugePages_Rsvd:        0\nHugePages_Surp:        0\nHugepagesize:       2048 kB\nHugetlb:         8388608 kB","categories":["Linux"],"tags":["Linux"]},{"title":"正见——佛陀的证悟（一）诸行无常(选摘)","url":"/2020/04/10/Books_%E6%AD%A3%E8%A7%81-%E4%BD%9B%E9%99%80%E7%9A%84%E8%AF%81%E6%82%9F-%E8%8A%82%E9%80%89/","content":"这是从《正见-佛陀的证悟》这书中，摘出来的句子，我认为有意义，或者对四法印中的诸行无常有理解的部分， 一直放在QQ空间中， 前两天拿出来看了一下， 觉得还是有不少的体会，所以把它搬过来了， 后续的内容是不是会更新还是要看我懒不懒了。\n\n\n​\t\t只是对老与死的厌恶，并不足以让太子离开王宫而踏入未知的世界；悉达多会采取这么激烈的行动，是因为他实在无法合理地解释所有已生和将出生的一切众生命运就是如此而已。如果所有生者都必须衰朽死亡，那么花园中的孔雀、珍宝、华盖、熏香、音乐，放拖鞋的金质拖盘、进口的玻璃水瓶、他与耶输陀罗和罗睺罗的感情、家庭、国家，都变得毫无意义。这一切的目的到底是什么？\n​\t\t完全不凭借任何科学工具，悉达多太子以吉祥草为垫，坐在一棵菩提树下，探索人类的本性。经过了长时间的思维，他终于了悟到一切万有，包括我们的血肉、我们所有的情绪和我们所有的觉受，都是由两个以上的元素组合而成。当两种或多种元素和合在一起，新的现象就会产生；钉子和木头产生了桌子；水和叶子产生了茶；而恐惧、虔诚和救世主，就产生了神。这些最终的产物，并没有独立于其各别元素的存在。相信它真实独立存在，是最大的骗局。而在和合的同时，各个元素也起了变化。只因接触和合，它们的性质也随之改变了。\n​\t\t一切万有，没有一样是以独立、恒常、纯粹的状态存在。你手上的书不是，原子不是，甚至神祇也不是。\n​\t\t因此悉达多发现，无常并不像一般人以为的就是意味死亡，而是意味变化。任何事物和另一个事物之间的位置或关系转变了，即使是非常细微的变动，都要依循无常的法则。\n​\t\t如果没有盲目的期待，就不会有失望。如果能了解一切都是无常，不会攀缘执著；如果不攀缘执著，就不会患得患失，也才能真正完完全全地活着。\n​\t\t悉达多从恒常的幻相中觉醒，因此我们称他为佛陀、觉者。到现在还是没有人可以长生不老，每个人终究会死，而且每天大概有二十五万人死亡。我们亲近的人不是已经死亡就是将会死亡。然而当亲人死去的时候，我们还是会震惊和悲伤；我们还是继续寻找青春之泉，或是长寿的秘方。\n​\t\t悉达多太子不再需要或渴求长生不老药了。由于了悟到一切事物皆是和合而成，解构无止境，而且一切万有的各个成分，没有一项是以独立、恒常与纯粹的状态存在的，他因此获得解脱。一切和合之物(现在我们知道这是指一切事物)与其无常的本质是合而为一、不可分割的。\n​\t\t当悉达多看到一个人走过，即使他很健康，悉达多所看到的是此人的生与灭同时发生。你也许会认为这样的人生观不太有趣，但在生命的旅程中能够同时看到一体的两面，可以是非常奇妙，而且可能会有很大的满足感。\n​\t\t也就表示现在有个“假设者”，悉达多会同意，只要有[假设者]，就会有上帝存在；但如果没有假设者，就不会有上帝存在。如果没有纸，就不会有书。如果没有水，就不会有冰。如果没有开始，就不会有结束。一件事物的存在，亟需依赖其它事物的存在，因此没有什么是真正独立的。尽管我们以为可以控制变化，但事实上大多是不可能的，因为无法察觉的影响因素太多了。也因为这种相互依存性，一切事物不可避免地会从目前或原始状态中解体。每一个变化中都蕴藏着死亡的因素。今日就是昨日之死。\n​\t\t大部分的人都接受一切生者终将死亡。然而我们对“一切”与“死亡”的定义或许不太一样。对悉达多来说，生指的是一切万有，不仅仅是花朵、蘑菇、人类，而是一切生成或和合的事物。而死亡指的是任何的解体或是解构。无常纯粹是一个简单实在的事实。不可能有一天，某个突发的和合事物会突然变得恒常，更难想象我们能证明这样的事。但是在今天，我们不是将佛陀奉为神明，就是想用科技证明自己比佛陀更高明。\n​\t\t佛陀教导我们，至少我们心中要保持着无常的概念，不要故意去隐藏它。我们借着不断地觉察和合的现象，便会了知因缘相依。认识因缘相依，我们就会认识无常。而当我们知道一切事物皆无常，才不会被种种假设、僵化的信条(不论宗教的或世俗的)、价值体系和盲目信仰所奴役。我们的觉察力可以让我们免于受限于个人的、政治的和感情的戏码之中。我们还可以将这种觉察力导向大至想象之极，小至次原子层次。\n​\t\t由于我们对自己的道德原则感到自豪，而且常强加于别人身上，因此道德观还是具有少许价值。然而，在整个人类历史当中，道德的定义也随着时代精神而一直在改变。美国人度量政治正确性或不正确性的仪表起伏不定，令人迷惑。不管如何称呼种族或文化群体，总是有人会被冒犯，游戏规则一直在改变。\n​\t\t当悉达多提到“一切和合的事物”，他所指的不只是像DNA、你的狗、艾菲尔铁塔、卵子和精子等具体可认知的现象而已。心、时间、记忆和上帝，也是和合而成。而每一和合的成分，又依赖更多不同层次的和合而成。    当悉达多教导无常时，他也超越了一般“结束”的想法，像是那种认为死亡只发生一次就完了的概念。死亡从生、从创造的那一刻开始，就没有停过。每一个变化，都是死亡的一种形式，因此每一个生都包含了另一个事物的死亡。\n​\t\t拿煮鸡蛋来做例子。如果没有不断的变化，蛋就煮不熟；煮好蛋的这个结果，需要某些基本的因缘。很显然的，你要有一颗蛋、一锅水，和一些加热的元素。另外有些非必要的因和缘，像是厨房、灯光、计时器，还有一只把蛋放进锅子的手。另外一个重要的条件，就是没有像是电力中断或是山羊跟进来打翻锅子之类的干扰。此外，每一个条件，例如母鸡，都需要有另一套具足的因缘条件。需要有另一只母鸡生下蛋才能孵出它，还要有安全的地方，有食物才能让它成长。鸡的食物也要有适合的地方生长，并且要能让它吃进去才行。我们可以将非必要和必要条件一直分析到小于原子的程度，而在这个分析的过程中，各种形态、形状、功能和标签也会不断地增加。\n​\t\t当无数的因缘和合在一起，而且没有障碍与干扰，结果是必然的。许多人误以为这是注定的或是运气所致。然而，到了一个程度以后，即使我们祈求蛋不要煮熟，它还是会熟的。\n​\t\t就像蛋一样，所有的现象是由无数的成分所组成，因此它们是可变的。这些无数的成分几乎都不是我们所能控制的，所以会让我们的期待落空。这种不可预料性，遍在于所有的物质、感受、想象、传统、爱情、信任、不信任、怀疑论，甚至上师和弟子以及人与神之间的关系。\n​\t\t信仰，怀疑论以及所有和合的现象一样，都是无常的。\n​\t\t有些人到现在还认为马克·查普曼(Mark Chapman)是谋杀约翰·蓝侬(John Lennon)唯一的罪犯。当我们能了解一个病态而饱受折磨的心是如何形成，并且知道它是在什么样的情况下运作，就比较能够理解并宽恕世界上众多的马克”查普曼。当条件成熟，就像蛋煮熟了一样，即使我们祈祷暗杀事件不要发生，它还是避免不了。超过了某个时间点，我们要改变条件的企图和行为就会徒劳无功了。\n​\t\t恐惧和焦虑是人类心智中主要的心理状态。恐惧的背后是对确定性不断的渴求。我们对未知感到恐惧。人心对肯定的渴望，是根植于我们对无常的恐惧。\n​\t\t但我们常常忘记自己的来日一直都是有限的。即使理智上知道有生必有死，一切和合终将分散，我们的情绪状态还是常常会回到相信恒常的模式，完全忘记相互依存性。这种习气会造成各种负面的情况，像是偏执、寂寞、罪恶感等等。我们会觉得被欺骗、被威胁、被虐待、被冷落，仿佛这个世界只对我们不公平。\n​\t\t佛陀不是一个悲观者、也不是末日论者，他是重视实际者，而我们却多是逃避现实者。当他说一切和合皆是无常，他并不认为那是坏消息，而是简单、科学的事实。我们能认清因缘的不稳定，就会了解自己有力量转化障碍，并且完成不可能的任务。生活中的各个层面都是如此。如果你现在没有一台法拉利，你完全有可能创造出因缘而拥有一台。只要世上有法拉利，你就有机会去拥有它。同样的，如果你想活久一点，可以选择不抽烟和多运动。合理的希望是存在的。而绝望，和它的反面—模一样，都是相信恒常的结果。\n","categories":["Books"],"tags":["书"]},{"title":"less命令占用内存过高","url":"/2020/04/06/Linux/Linux_Cat-and-less/","content":"在华为的月度报告中， 发现了奇怪的问题，有一台机器在每天都会内存使用率飙到100%，每天如此。在查找问题的过程中，发现机器上每天的2，4，6，8点会自动运行处理日志的脚本，并统计关键字的数量。但是为什么会导致占用高呢？\n\n\n这个脚本的过滤日志思路是， 使用less 打开昨天的所有日志文件，并且读取内容，过滤其中的关键字，命令类似如下：\n\nless yesterday*.log.gz | grep -e “xxxxx” -e “xxxxx” | sort | uniq | wc -l\n\n关于less, more, cat, zcat这个四个命令可以分成两组，\nless and more 的命令是为了个人类查看文件的内容设计的程序；\ncat, zcat 是一次性输出内容到屏幕的；\n两组命令的唯一区别是是否提供前后反复查看的功能。但是就是这个功能，导致了命令执行的逻辑其实是完全不同的。\nless命令的执行方式将查看到的数据放入内存中（USED），所以 如果是一个10G的日志，就会占用10G的内存， 如果不够，系统就会进行交换和将旧数据换出的操作。\ncat的执行模式是将数据直接输出到屏幕，同时操作系统会将数据存储到Cache中，而不占用内存（USED）的空间，就不会触发告警。\n结论由于两个命令的处理方式不同,因此直接将命令中输出gz日志的命令换成了zcat，即可保证内存的使用不会变多；\n这里同时提高了系统执行命令的效率，非常nice。\n所以在脚本中一定一定使用正确的命令和方式处理数据，才能保障性能和效率的兼顾。\n最后，优化过的命令为：\n\nzcat yesterday*.log.gz | grep -e “xxxx” -e “xxxx” | sort -u | wc -l\n\n最后的执行时间，从原来的半个小时起步，变成了10分钟即完成。\n后续的处理已知我们的问题是不需要系统自动cache住我们过滤的日志数据，为了加快后续的性能，cache住更多有用的信息，我们需要手动释放一下已经完全被cache的信息：\nsync &amp;&amp; echo 3 > /proc/sys/vm/drop_caches\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"Kubernetes集群的学习笔记(7)","url":"/2019/11/06/Linux/Linux_k8s-basic-7/","content":"Kubernetes的Dashboard 和 分级认证权限。\n\n\nDashboard的简介Dashboard算是k8s的一个管理的web界面。不做具体的操作，登录的时候使用的是K8S提供的用户名和密码。\nDashboard的部署只需要从github进行apply资源清单即可。\nDashboard的登录使用Token登录\n获取系统中默认的admin的token，或者创建一个需要登录和管理的ServiceAccount，然后Binding到Role或者 ClusterRole,进行权限的控制\n\n查看集群中自动创建的dashboard的secret， 系统部署完成之后自动创建了一个可管理全部集群的secret\n\n\n   ~]$ kubectl get secret -n kube-system | grep dashboard\ndashboard-admin-token-g85h7                      kubernetes.io&#x2F;service-account-token   3      109d\n\n\n在这个secret中有Token相关的信息\n~]$ kubectl describe secret dashboard-admin-token-g85h7 -n kube-system\n\n其中的Token字段的内容就是可以用来登录的令牌。复制到dashboard中粘贴即可\n\n\n使用config文件登录\n创建ServiceAccount，Binding到role\n获取Secret，然后查看他的token\n制作config文件\n使用config文件登录\n\nKubernetes的管理方式\n命令\ncreate , run ,expose , delete ,edit ….\n\n命令式配置文件\ncreate -f , delete -f , replace -f \n\n声明式配置文件\napply -f , patch\n\n\n一般不混合使用，1.2 使用的是替换，但是3是立刻修改，立刻生效，所以还是比较危险的。\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"Kubernetes集群的学习笔记(6)","url":"/2019/11/06/Linux/Linux_k8s-basic-6/","content":"K8S的认证部分， ServiceAccount以及RBAC 。\n\n\n授权插件\nNode\nABAC\nRBAC\nWebhook\n\n常用的授权插件就是RBAC\n基于角色的授权和访问控制基于角色的访问控制，就是将权限授予Role，而不是User。 将权限的控制授予Role， 将User分配到Role。默认的是拒绝全部， 无法也无需定义拒绝权限，定义的Permission是许可访问的权限。\n\n角色，User Accouts OR Service Accounts.\n许可， Permission分为两个部分：Operation 以及 Object.\n\n角色以及角色绑定Role – RoleBinding\nRole 和 RoleBinding 是建立和控制 NameSpace 级别的权限。\n集群角色以及集群角色绑定ClusterRole – ClusterRoleBinding\nCluster 和 ClusterBinding 是建立和控制 Cluster 级别的权限。\nNOTE： 特殊的情况，可以对名称空间级别的\n特殊的绑定方式ClusterRole – RoleBinding\n可以使用 RoleBinding 绑定 ClusterRole， 那么这种情况下，Role只是具有NameSpace的权限。解释一下：雷在同一个Cluster中有多个不同的NameSpace，我需要对每个NameSpace都授权相同的权限，这种场景下:\n\n如果使用RoleBinding 去绑定一个Role，那么每一个NameSpace都需要建立各自的 RoleBinding，并且都要各自绑定到Role。\n\n如果建立一个ClusterRole, 使用RoleBinding绑定到ClusterRole上面，那么我只需要定义一个即可在全部集群范围内生效这个权限。这样的就相当于批量的进行Role的授权。\n\n\n相关命令命令不做过多的解释，所有的资源可以通过explain获取，主要是记录逻辑和思路。\n~]$ kubectl create role testrole --verb&#x3D;get,list,watch --resources&#x3D;pods -o yaml\n~]$ kubectl get role\n~]$ kubectl describe role pods-reader\n~]$ kubectl create rolebinding test-rolebinding --role&#x3D;testrole --user&#x3D;testuser -o yaml\n~]$ kubectl explain rolebinding\n~]$ kubectl config use-context USERNAME@kubernetes\n~]$ kubectl create clusterrole test-clusterrole --verb&#x3D;get,list,watch --resource&#x3D;pods -o yaml \n~]$ kubectl explain clusterrole\n~]$ kubectl get clusterrole\n~]$ \n\n其他补充在RBAC中可以存在三类组件：\n\nuser\ngroup\nservice account\n\n创建Pod的过程中可以指定一个值叫做ServiceAccountName，如果授权ServiceAccount高等级的权限，那么，Pod会以这个Account运行，那么Pod中的应用程序也会有ServiceAccount的权限。也就是提高了Pod应用程序的等级，使得Pod可以对K8S的相关资源进行管理和配置。\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"i3wm的简单配置","url":"/2019/10/22/Linux/Linux_ChangeGnometoi3wm/","content":"将自己的桌面环境迁移到了i3。Gnome好用是好用的，但是体量还是有点儿大了，吃资源有点多。\n\n\n安装i3-gaps安装的过程比较简单,pacman 就完事儿了。\npacman -Syyu \npacman -S i3-gaps\n  这里有两个可以选择，一个是i3-wm,  还有一个是i3-gaps, 我用了i3gaps， 好看一些。\npacman -S alacritty \npacman -S polybar\npacman -S compton (最新的软件改名了，叫picom,但是安装的方式不变的。)\npacman -S picom\npacman -S dmenu\npacman -S rofi\npacman -S feh \npacman -S variety\npacman -S unzip-natspec  \n#unzip-natspec貌似是在archlinuxcn里面的，可以处理zip解压的时候乱码的问题，不用考虑手动指定解压的-O了。\n\n配置i3在已经有Gnome环境的条件下，i3 的配置还是比较轻松的。\nI3的使用说明i3默认的操作是使用Windows&#x2F;Super&#x2F;Mod这个按键(或者随便怎么叫吧，后面都说Super，和Gnome的口径一致)，简单列举一下频率较高的使用方式\n\n先说重载配置文件，使用 Super + Shift + r \n退出到DM，使用Super + Shift + e， DM就是你的登录界面。\ni3提供了10个虚拟桌面，切换虚拟桌面方式是可以通过Super + 1 - 10 快速切换\n在默认的Tiling排列模式中切换排列位置Vertical和Horizontal. 通过Super V 或者 Super H\n调整容器布局：\n使用Stacking – Super + s 所有的标签堆叠显示最上方提供标签进行切换\n使用Tabbed – Super + w 标签页方式显示所有窗口\n使用Toggle – Super + e 平铺窗口模式，可切换不同的布局方式\n\n\n打开程序可以通过terminal开启，或者Super + d 使用dmenu开启（可将 dmenu 替换成 rofi）\n关闭程序可以通过Super + Shift + q关闭，或者鼠标点击x，不是所有的窗口都有x\n在窗口间移动可使用 Super + {jkl;}四个按键；或者使用Super + {up,down,left,right}的arrowkey\n可将窗口变更为Floating， 使用Super + Shift + Space\nFloating窗口调整位置可长按Super 使用鼠标拖动调整位置\n全屏程序使用 Super + f\n\ni3的自定义配置配置文件的位置配置文件的路径： $HOME&#x2F;.config&#x2F;i3&#x2F;config,所有的配置在这个文件下修改\n自定义界面的设置默认的i3启动的时候还是挺丑的，我有四个设置：\ngaps inner 5  # 设置i3窗口间的空隙大小，单位是像素。\nnew_window 1pixel # 设置新的窗口的边界宽度，效果是不显示窗口的title。\nnew_float 1pixel\t# 新的浮动窗口的边界宽度，同上。\nsmart_borders on \t# 在只有一个窗口的情况下自动最大化当前的窗口，不处理窗口的Gap。\n\n默认terminal程序更改设置默认使用 Super + Enter 打开alacritty\nbindsym $mod+Return exec --no-startup-id alacritty\n\nalacritty 是一个使用GPU进行渲染的terminal模拟器，它有自己的配置文件，我的配置文件路径在:$HOME&#x2F;.config&#x2F;alacritty&#x2F;alacritty.conf\n*NOTE： 关于Alacritty的问题目前已经解决的差不多了。 还有还有一个问题是关于ssh过去之后不能正确的识别终端类型的BUG，解决方案在后面。\n默认打开Firefox我的设置是Super + p\nbindsym $mod+p exec --no-startup-id firefox\n\n开机启动一些程序我的开机启动了 Compton, Variety, Remmina,Aria2c, Polybar, ibus-daemon，剩下的看自己喜欢什么就安装什么就OK了。\nexec_always --no-startup-id compton --config &#x2F;home&#x2F;liarlee&#x2F;.config&#x2F;compton&#x2F;compton.conf -b\nexec_always --no-startup-id variety \nexec_always --no-startup-id remmina\nexec_always --no-startup-id aria2c --conf-path&#x3D;&#x2F;home&#x2F;liarlee&#x2F;.aria2&#x2F;aria2.conf\nexec_always --no-startup-id sh &#x2F;home&#x2F;liarlee&#x2F;.config&#x2F;polybar&#x2F;polybar_startup.sh\nexec_always --no-startup-id ibus-daemon -dr\n\n截屏功能快捷键默认的i3不能用PrintScreen我觉得有点儿难受，所以自己添加了快捷键和保存位置\nbindsym --release Print exec &quot;scrot -b -m &#x2F;home&#x2F;liarlee&#x2F;Pictures&#x2F;Scort_ScreenShot&#x2F;screenshot.png&quot;\n\n\n更新：\n之前的这个方式可以作为截图的快捷方式，但是当你需要连续的截图的时候就会特别的难受，因为新截图会自动覆盖掉旧的截图。更新一下新的方式，写一个脚本，当我们按下PrintSc的时候触发这个脚本，就可以对文件重命名了。\n\n也许脚本名称可以叫 - screenshot.sh\n\n#!/bin/bash\n\nsnapdate=`date \"+%Y%m%d_%H%M%S\"`\ngnome-screenshot -f /home/hayden/screenshot/Screenshot-$snapdate.png\n\n\n更新i3的配置文件，添加快捷键的管理。\n\nbindsym --release Print exec \"/home/liarlee/Scripts/System/screenshot.sh\"\n\n\n这样再截图就会保存到指定的位置，并且用时间命名区分开不同的截图，不会覆盖了。\n\n调节音量快捷键由于已经有了PulseAudio， 所以i3自动增加了笔记本Func-key的调整，但是如果没有功能键的话，还是要按照如下自定义的。我调整音量用的Super + F2&#x2F;F3, 也可以改其他的\n# Use pactl to adjust volume in PulseAudio.\nbindsym XF86AudioRaiseVolume exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ +10% &amp;&amp; $refresh_i3status\nbindsym $mod+F3 exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ +10% &amp;&amp; $refresh_i3status\nbindsym $mod+F2 exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ -10% &amp;&amp; $refresh_i3status\nbindsym XF86AudioLowerVolume exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ -10% &amp;&amp; $refresh_i3status\nbindsym XF86AudioMute exec --no-startup-id pactl set-sink-mute @DEFAULT_SINK@ toggle &amp;&amp; $refresh_i3status\nbindsym XF86AudioMicMute exec --no-startup-id pactl set-source-mute @DEFAULT_SOURCE@ toggle &amp;&amp; $refresh_i3status\n\n拼音输入法我在Gnome下使用的是Ibus-rime框架的小鹤双拼，Gnome下开箱即用，但是i3需要做一些简单的配置，更改一些环境变量。\n\n在自己的家目录 : touch .xprofile\n\n添加内容如下：\nexport GTK_IM_MODULE=ibus\nexport QT_IM_MODULE=ibus\nexport XMODIFIERS=@im=ibus\n\n使用source读取配置文件中的环境变量，使环境变量生效。尝试输出echo $XMODIFIER查看是否已经生效，输出空值的话ibus还是无法使用。\n\n结合上面的i3配置文件，使用ibus-daemon -dr启动，不要XIM。如果你使用了-x启动，在浮动的输入法工具栏中会显示一个禁止的标志，说明拼音未正常工作。\n\n\n配置Polybar我基本上使用的是默认的Bar配置文件，还比较方便。\n\n复制一个配置文件的模板到家目录的文件夹下：\ncp -p /usr/share/doc/ploybar/config .config/polybar/config\n\n编辑一个启动脚本\ntouch polybar_startup.sh\n\n写入如下内容\n#!/bin/bash\n## kill all old process of ploybar\nkillall -q polybar\nwhile pgrep -u $UID -x polybar > /dev/null; do sleep 1; done\n\necho \"---\" | tee -a /tmp/polybar.log\npolybar example &amp; /tmp/polybar.log 2>&amp;1 &amp;\necho \"Polybar launched\"\n\n结合上面的i3配置进行开机自动运行即可。我觉得默认的这个还行，可以日常使用了，不调整了，太费劲了。\n\n\n目前还未解决的问题无法在terminal中输入中文目前中文的输入法还是有些问题，无法在alacritty中输入中文，可能和ibus的关系比较大，我尝试使用fctix完全没有任何问题，所有的地方都可以输入中文，所以选择什么方式输入中文，各有所好吧。但是，Gnome环境和i3的环境是会冲突的，所以同时安装ibus和fcitx要考虑一下。\n\nAlacritty中无法输入中文的问题已经解决了！\n\n那么解决的方法如下：\n​\t在&#x2F;etc&#x2F;profile文件中（或者.zshrc中），添加如下的环境变量\nexport GTK_IM_MODULE=ibus\nexport XMODIFIERS=@im=ibus\nexport QT_IM_MODULES=ibus\n\n之后 ，\nsource /etc/profile\nsource ~/.zshrc \n\n重新启动alacritty就可以使用了。\n我想root cause是：我的alacritty启动的时候使用的是zshrc的变量，而没有读取系统的变量，不知道是不是这个原因，但是现在已经可以使用了，完美。\n\n\nSSH之后不能正确的识别Alacritty的终端类型ssh登录其他机器的时候会报错： Error opening terminal: alacritty.\n\n这个问题其实是因为Alacritty毕竟是小众的Terminal Emulator， 所以 大部分的机器里面并没有这个类型的终端的信息， SSH在登录之后会试图把登录用户的Terminal设置为Alacritty，这个问题其实配置文件中已经有了解决方法，直接注释配置文件的如下部分，就可以恢复SSH正常使用了。\nenv:\n    TERM: xterm-256color\n\n解决~\n\n放几张最后的结果图大约就是这个样子啦～　　收工！\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"Kubernetes集群的学习笔记(5)","url":"/2019/10/18/Linux/Linux_k8s-basic-5/","content":"K8S集群的存储卷笔记。\n\n\n整体的存储卷调用结构：\n在k8s的集群中，Pod声明自己需要存储卷资源，同时创建自己的PVC，PVC绑定到集群中已经注册的PV资源，例如 已经建立的NFS网络空间。PV资源通过存储系统的分配，直接提供给Pod来使用。\nPVC属于名称空间级别，PV属于集群资源。\n存储卷的类型\nEmptyDir：只在Node上存在的存储卷，Pod删除的时候存储卷也会被移除，无法持久存储，叫做EmptyDir，做临时存储和缓存使用，可以使用Node的内存。\n\nHostPath： Docker的存储卷类型，Node节点上的目录。\n\n网络存储： SAN， NAS; 分布式存储(Glusterfs，Cephfs，rbd); 云存储(EBS， Azure Disk，特定的托管在云上的服务)\nkubectl explain pod.spec.volumes\n\n 可以查看支持那些存储。\n\n\nPVC对于用户来说，无法掌握所有的存储系统的知识和技能，因此，创建了PVC的逻辑层，在定义需要使用存储卷的Pod中只需定义需要的空间以及存储的类型，不需要详细的考虑后端存储的信息和配置。\nPVC被定义在Pods的配置中，一个 PVC可以被多个Pod同时访问。\nPV存储类： Gold Storage Class, Sliver Storage Class, Bronze Stroage Class.三个不同级别的存储类。存储类直接接受PVC的内容并对PVC定义的容量等信息进行分配。\nPV与存储服务的提供者直接绑定且需要在存储服务提供方配置完成。PV与PVC具有一一对应的关系。\nPersistentVolumegitRepo仓库的使用：\ngitRepo存储卷建立在EmptyDir的基础上，在Pod内建立空目录，同步Git的内容到空目录，Pod运行的过程中不会更改存储卷上的内容。也就是说，git同步是pod建立的时候同步的数据，不会对git项目的数据进行更改。Pod更改了存储卷中的数据不会自动推送到Git上（可以使用Sidecar进行推送和配置）。\nPV资源的定义---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv001\n  labels:\n    name: pv001\nspec:\n  nfs:\n    path: &#x2F;data&#x2F;volumes&#x2F;v1\n    server: storage1.liarlee.com\n  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;]\n  capacity:\n    storage: 20Gi\n\n使用apply命令进行应用即可。\n查看所有的PV使用： kubectl get pv ， 其中显示了PV的名称，大小，访问模式，回收策略，状态，建立时间等。\nPersistentVolumeClaim使用的过程中，在Pod中定义PVC；PVC和PV是一一对应的，但是PVC可以被多个Pod调用和挂载。一个PV会Binding一个PVC，PVC在Pod中被定义。\nPVC以及PV的状态，未绑定的状态叫做Pending，绑定后叫做Bound。\nPVC的定义---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: pvc001\n  namespace: default\nspec:\n  accessModes: [&quot;ReadWriteMany&quot;]\n  resource:\n    request:\n      storage: 6Gi\n  \n\nConfigMap资源configmap是明文资源，secret是base64的编码资源，因此安全程度提高了。configmap相当于外挂的配置文件，当Pods启动的时候直接挂载Configmap读取自己需要的配置内容。\n配置应用容器化的方式\n自定义命令行参数\n配置文件直接放入镜像中\n通过环境变量进行配置\n存储卷\nCloud Native应用通过环境变量直接配置\n通过EntryPoint脚本预处理环境变量作为配置问文件中的信息\n\n\ndocker config命令行方式\n\nconfigmap的创建configmap为了将配置文件中镜像中解耦，configmap可以直接注入到容器中直接使用，注入的方式可以使用存储卷，或者使用EntryPoint来处理。\n通过命令直接传递键值：\nkubectl create configmap cm-nginx --from-literal=nginx_server_name=nginx.liarlee.com --from-literal=nginx_server_port=80\nkubectl create configmap cm-nginx --from-file=./nginx.conf # 直接传递文件到ConfigMap\nkubectl get cm cm-nginx -o yaml # 用YAML的格式查看ConfigMap\n\n通过YAML文件：\n---\napiVersion: v1\nkind: ConfigMap\ndata: \n  nginx.conf................................\nmetadata:\n  name: cm-nginx\n  namespace: default\n\n创建Pod时ConfigMap使用环境变量方式：\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nginx\n  namespace: default\n  labels:\n    app: pods-nginx\n    tier: frontend\n  annotations:\n    liarlee.com&#x2F;created-by: &quot;cluster admin&quot;\nspec:\n  containers:\n  - name: pod-nginx\n    image: nginx:latest\n    ports:\n    - name: http\n      containerPort: 80\n    env:\n    - name: NGINX_SERVER_PORT\n      valueFrom:\n      \tconfigMapKeyRef:\n      \t\tname: cm-nginx\n      \t\tkey: nginx_port\n    env:\n    - name: NGINX_SERVER_NAME\n    \tvalueFrom:\n    \t\tconfigMapKeyRef:\n    \t\t\tname: cm-nginx\n    \t\t\tkey: nginx_server_name\n\n创建Pod的时候使用挂载卷的方式：\n通过挂载的方式可以通过修改configmap的方式，同步修改容器内部的配置。\n---\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-nginx\n  namespace: default\n  labels:\n    app: pods-nginx\n    tier: frontend\n  annotations:\n    liarlee.com&#x2F;created-by: &quot;cluster admin&quot;\nspec:\n  containers:\n \t- name: pod-nginx\n   \t  image: nginx:latest\n      ports:\n      - name: http\n      \tcontainerPort: 80\n \t  volumeMounts:\n \t  - name: nginxconf\n \t  \tmountPath: &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;\n \t  \treadOnly: true\n \tVolumes:\n \t- name: nginxconf\n \t  configMap: \n \t  \tname: cm-nginx \n\n创建Secret的类型kubectl create secret –help\n\ndocker-registry – 连接到Docker私有仓库的密钥\ngeneric – 通用的服务密钥\ntls – 创建证书\n\nStatefulSet的建立和使用在一定程度上实现了有状态的管理，但是依旧需要写成管理脚本，注入到Pod中。\nCoreOS – 提供了 Operator相关的功能用来完善有状态的应用的部署。\n两类不同的Pod\n\nCattle\n\nPet\n\n\n最早叫做PetSet， 后面改成StatefulSet。\nStatefulSet一般用于管理以下特性的组件：\n\n稳定且有唯一的网络标识符；必须保持Pod的名称和地址稳定持久有效；\n\n稳定且持久的存储；\n\n有序平滑(Graceful)的部署及扩展；启动的时候Pod1 - Pod8；\n\n有序平滑(Graceful)的终止和删除；关闭的时候Pod8 - Pod1；\n\n有序的滚动更新；有顺序的进行Pod的更新；\n\n\n有三个主要的部分；\n  1. Headless Service\n     - 类似于Redis，提供一个headless的服务，来确保每一个请求直达后端的pod，可保持pod的接入点稳定且不发生变化。\n  2. StatefulSet Controller\n     - 需要一个控制器来进行Pod的管理和控制，保持Pod的生命周期，即使Pod被终止也需要在启动后保持和之前一样的Pod信息。\n  3. Volume Claim Template\n     - 由于有状态的服务不能同时使用同一个PV，所有的节点存储的数据各不相同，所以不能提供一个PVC&amp;PV。因此提供了申请PV的模板，每个Pod提供一个独立的存储卷用来做独立存储。\n\n对于StatefulSet来说，确保所有的Pod名称稳定有效不可变动。大多数有状态的副本都会使用持久存储，多个Pod能不能共用同一个存储？ 不能，每个Pod必须使用不同的存储。\n 所以，需要定义PV，定义PVC模板，定义Pod，定义Stateful控制器，定义headless服务这几种。\n定义StatefulSet的YAML文件---\napiVersion: v1\nkind: Service \nmetadata:\n  name: sts-headless-svc\n  labels:\n    Service: sts-headless-svc\nspec:\n  ports:\n  - port: 80\n    name: sts-headless-svc\n  clusterIP: None\n  selector: \n    service: sts-headless-svc\n\n---\napiVersion: app&#x2F;v1\nkind: StatufulSet\nmetadata:\n  name: sts\nspec:\n  serviceName: sts\n  replicas: 3\n  selector:\n    matchlabels:\n      service: sts-headless-svc\n  template:\n    metadata:\n      labels:\n        service: sts-headless-svc\n   \tspec: \n   \t\tcontainers: \n   \t\t- name: sts-container\n   \t\t  image: ikubernetes&#x2F;SOMEIMAGE&#39;SNAME\n   \t\t  ports:\n   \t\t  - containerPort: 80\n   \t\t    name: sts-headless-svc\n   \t\t  volumeMounts:\n   \t\t  - name: sts-pv\n   \t\t    mountPath: &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html\n  volumeClainTempates:\n  - metadata:\n    name: sts-vct\n  spec:\n    accessModes: [ &quot;ReadWriteOnce&quot; ]\n    resources:\n      requests:\n        storage: 5Gi\n        \n\n\nPod的dns命名规则：\nPod_Name+Service_Name+NameSpace_Name.svc.cluster.local\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"Kubernetes集群的学习笔记(4)","url":"/2019/10/14/Linux/Linux_k8s-basic-4/","content":"K8S Service资源的笔记以及Ingress资源的笔记。\n\n\nk8s的serviceservice的模型： userspace(kube-proxy), iptables, ipvs \nService的类型请求发送的过程：\n\nClient –&gt; Node’s IP:Node’s Port –&gt; Cluster’s IP:ServicePort –&gt; PodIP: ContainerPort\n\nService的类型：\n\nClusterIP是将提供服务的Pods统一建立一个集群内部的可访问接口，为集群内部的服务提供入口，PodsPort to ClusterIP:Port.\n\nNodePort是将Node的端口映射出去的方式，每个节点各自对外提供一组IP:Port用来对外提供服务 - PodsPort to NodePort.\n\nLoadBalancer使用LBaaS的方式对外提供服务，共有云环境可用，例如阿里云。\n\nExternalname可将外部的服务引入到集群的内部，需要提供的字段是外部网络的真正的DNS服务的CNAME（FQDN）。\n\nHeadlessService是不提供ClusterIP，可将ServiceName直接解析到PodsIP。\n\n\n上面的每一个类型的服务都是顺序增强的，也就是说，基础的模式是 ClusterIP 。\nHTTPS的处理和思路\n启动一个Pod对HTTPS进行卸载和LB\n如果需要对HTTPS进行配置，Kubernetes本身提供的Service不能提供7层协议的卸载(将HTTPS卸载为HTTP与后端做通信以及数据交换),那么可行的方案是，建立一个新的Pod，例如nginx，由nginx-pod进行HTTPS的代理和卸载操作，但是这样的话就会有如下的流程： \nUser Request –&gt; LBaaS(LB) –&gt; Service - NodePort(LB) –&gt; Pod - nginx proxy(LB) –&gt; BackendPods\nPROBLEM: 在这种情况下，用户的请求需要进行两次负载的转换才可以到达Pod，开销太大。\n\n将Nginx的Pod与Node的Net名称空间进行共享\n使得Nginx的Pod直接工作在Node的网卡级别上。免去了K8S的Service中间的一次转发。为了避免单点的故障，可将DeamonSet将Pod运行在一部分节点上。\n\n\nIngress Controller可以用于处理和卸载HTTPS协议的负载均衡器Pod控制器。\nK8S四个附件： DNS， DashBoard， Ingress Controller,  heasper。\n如上的那种NginxPod，衍生为Ingress Controller， 独立运行的应用程序组成，不属于Controller Manager。通常由4种选择：HAProxy，Nginx，Envoy，Traefik。微服务使用更多的是Envoy。\nIngress资源定义IngressController如何建立一个期望的前端（Nginx URL Rewrite），同时定义了期望的后端(Upstream servers)，更新后端负载Pod的信息。\nIngress类型Ingress资源的定义格式使用NginxServer的方式进行配置IngressPod\napiVersion: Extensions&#x2F;v1beta1\nkind: Ingress\nmetadata:\n  name: ingress-nginx\n  namespace: default\n  annotations: \n    kubernetes.io&#x2F;ingress.class: &quot;nginx&quot;\nspec:\n  rules:\n  - host: nginx.test.local\n    http:\n    \tpaths:\n    \t- path:\n    \t  backend:\n    \t\tserviceName: svc-nginx\n    \t\tservicePort: 80 \t\t \t\n\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"Linux及vim的技巧","url":"/2019/10/10/Linux/Linux_VIM_tricks/","content":"Linux的使用技巧已经vim的常用设置及插件。\n\n\n1. 删除目录下的所有文件夹，但是保留所有文件find .&#x2F; -type d ! -name . | xargs rm -rf\n2. 设置vim的个性化设置设置自动显示行号，设置VIM自动将tab转化为4个空格\n:set nu\n:set tabstop&#x3D;4\n:set softtabstop&#x3D;4\n:set shiftwidth&#x3D;4\n:set expandtab\n\n3. 已经编辑的文件进行tab空格转换：\nTAB替换为空格：:set ts=4\n:set expandtab\n:%retab!\n空格替换为TAB：:set ts=4\n:set noexpandtab\n:%retab!\n\n4. vim的个性化配置及插件\n自动换行\nset wrap\n\n输入命令时自动提示补完\nset showcmd\n\n显示行号\nset nu\n\n显示当前行号及关联前后行号\nset relativenumber\n\n语法高亮\nsyntax on \n\n光标所在的行高亮\nset cursorline\n\n前后滚动时保留最前和最后的5行\nset scrolloff&#x3D;5\n\n\n","categories":["Linux"],"tags":["Vim"]},{"title":"k8s所有的NS删除的时候都进入Terminating状态","url":"/2019/10/09/Linux/Linux_k8s-namespace-delete-terminating/","content":"集群无法删除Namespace解决方式。\n\n\nnamespace 无法删除  始终处于Teminating强制删除的方法，临时方案。\n将名称空间的配置文件导出。kubectl get namespace testtest -o json &gt; tmp.json\n\n编辑这个临时文件。vim tmp.json\n\n删除spec字段中的值。  &quot;spec&quot; : &#123;      &quot;finalizers&quot; : [\t\t# delete this line.          &quot;kubernetes&quot;\t\t# delete this line.          ]\t\t# delete this line.      &#125;  \t\t\t\n\n使用另一个terminal， 运行本地的proxy， 连接到API server。 kubectl proxy --port=8888\n\n通过ApiServer进行删除 curl -k -H &quot;Content-Type: application/json&quot; -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/[NEEDTODELETENS]/finalize;   \n 这里面http的调用路径在 ： tmp.json的 api 字段中。\n\n运行结果返回NameSpace的相关信息应该就是删除了。\n\n\nNamespace删除卡住的原因\nSolution From Github: https://github.com/kubernetes/kubernetes/issues/60807\n\n是某些服务的问题导致了无法删除掉相关的NS\n\nkubectl get apiservice | grep False\nkubectl api-resources –verbs&#x3D;list –namespaced -o name | xargs -n 1 kubectl get -n [NEEDTODELETENS]\nkubectl delete apiservice v1alpha3.kubevirt.io\n\n其实是这个apiservice影响的，他的状态不正常导致的NS删除的时候卡住，删除这个apiservice就可以了。\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"Linux_Cobbler搭建本地YUM源同步k8s阿里云","url":"/2019/09/30/Linux/Linux_Cobbler-k8s-reposync/","content":"昨天晚上尝试使用阿里云的时候出现问题 ，阿里云的k8s源安装的时候报错，无法正常通过yum安装。内网正好放了一台Cobbler，所以直接从Cobbler同步阿里的repo过来放到内网，防止这个事情再次发生。\n\n\nCobbler是什么\nCobbler是一个免费开源系统安装部署软件，用于自动化网络安装操作系统。 Cobbler 集成了 DNS, DHCP,[1][2]软件包更新， 带外管理以及配置管理， 方便操作系统安装自动化。Cobbler 可以支持PXE启动, 操作系统重新安装, 以及虚拟化客户机创建，包括Xen, KVM or VMware. Cobbler透过koan程序以支持虚拟化客户机安装。Cobbler 可以支持管理复杂网路环境，如创建在链路聚合以太网的桥接环境。 FROM Wikipedia\n\nCobbler repo的建立k8s的源，Cobbler直接建立的同步不可以是因为k8s的目录结构和一般软件源的结构不同。（开始以为阿里云会一直保持带有Pool文件夹的那个结构， 今天早上看到结构已经和普通的yumrepo一样了，记录一下出现这种问题怎么办好了。）其实解决的方案就是手动同步，使用Cobbler进行源的发布。其实也就是httpd发布出去。\n\n建立阿里云的源[root@cobbler &#x2F;]# cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo\n[kubernetes]\nname&#x3D;Kubernetes\nbaseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;\nenabled&#x3D;1\ngpgcheck&#x3D;1\nrepo_gpgcheck&#x3D;1\ngpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg\nEOF\n[root@cobbler &#x2F;]# mv kubernetes.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo\n[root@cobbler &#x2F;]# yum clean all\n[root@cobbler &#x2F;]# yum makecache\n[root@cobbler &#x2F;]# yum repolist\n# 在repolist中记录repoid\n手动同步源[root@cobbler &#x2F;]# reposync -n --repoid&#x3D;kubernetes -p &#x2F;var&#x2F;www&#x2F;cobbler&#x2F;repo_mirror&#x2F; --allow-path-traversal\n手动将已经同步好的目录创建为repo[root@cobbler &#x2F;]# createrepo &#x2F;var&#x2F;www&#x2F;cobbler&#x2F;repo_mirror&#x2F;kubernetes&#x2F;\n最后编辑一下自己的kubernetes.repo源文件，只指向本地的源就可以了。\n","categories":["Linux"],"tags":["Cobbler"]},{"title":"Kubernetes集群的学习笔记(3)","url":"/2019/09/24/Linux/Linux_k8s-basic-3/","content":"k8s笔记YAML格式定义资源。\n\n\n通过YAML定义PodsapiVersionkubectl api-versions查看所有可用的组名apiVersion:[Group&#x2F;Version]\nkind 资源类别metadata 元数据\nname: uniq Key\nnamespace\nlabels Key-Value\nannotations \nSelfLink: 资源引用的链接API格式：&#x2F;api&#x2F;group&#x2F;verison&#x2F;namespaces&#x2F;namespace&#x2F;type&#x2F;name\n\n\n\nspeckubectl explain pods.spec可使用命令查看：定义用户期望的目标状态。\nstatus自动维护即可 ，不需要更改。\n简单的YAML实例apiVersion: v1 \nkind: Pod\nmetadata:\n    name: myapp-pod\n    labels:\n        app: myapp\n        version: v1\nspec: \n    containers:\n    - name: app\n      image: nginx\n   - name: php-fpm\n      image: php-fpm\n\n对Pods进行标签操作\n查看标签kubectl get pods –show-labelskubectl get pods -l LABEL_NAME –show-labels\n\n增加标签kubectl label pods pod-demo KEY VALUE\n\n修改标签kubectl label pods pod-demo KEY VALUE –overwirte\n\n指定selector选择标签 - 等值关系， 集合关系的标签选择器。kubectl get pods -l release&#x3D;stablekubectl get pods -l release!&#x3D;stablekubectl get pods -l “release in (v1, v2, v3)”kubectl get pods -l “release notin (v1, v2, v3)”\n许多资源支持内嵌字段，matchLabels(直接给定键值) ， matchExporession(基于给出的表达式进行选择)。常用的操作符号，In ; Notin ； Exists；NotExists；\n\n\nPod的生命周期\n初始化容器init c(初始化主容器的执行环境)，可以有多个，串行执行，直到最后一个init c执行结束。\nmain c在所有的初始化完成之后开始启动，在容器的运行时间与main c的执行时间基本一致；main c刚刚启动之后，可以手动执行Poststart；结束之前可以进行Prestop；\n在Pod运行的过程中，提供Pod的Liveness probe； 提供Pod的Readiness probe。\n\n常见的Pod状态：\n\nPending： 挂起，调度尚未完成；\nRunning： 运行状态；\nFailed： 失败；\nSucceeded： 成功；\nUnknown: kubelet失去联络或者无法获取Pod信息时；\n\n创建Pod的阶段：\n\n创建提交请求给API server，目标状态保存到etcd;\nAPI Server 请求 Schduler 进行Pod的调度；\nAPI取得Pod的调度结果后，将信息记录到etcd；\nNode节点获取到API server上的Pod状态更新后，开始按照调度的信息进行Pod的建立。\n\nPod生命周期中的重要行为：\n\n初始化容器： init container ; \n容器探测： liveness ,readiness;\n\n容器的重启策略： restartPolicyAlways, OnFailure, Never; 三种策略中默认的设置时Always.\nPod的终止过程：\n\n发送term信号， 默认等待30s，如果30s还未终止就强制终止。\n\n存活检测可通过三种类型进行探测： ExecAction, TCPSocketAction, HTTPGetAction;\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"K8S将loop-lvm改为direct-lvm说明","url":"/2019/09/24/Linux/Linux_k8s-changelooplvm-directlvm/","content":"对k8s集群进行存储驱动的调整，从loop-lvm 切换到direct-lvm。\n\n\nk8s的几种不同的存储驱动\nAUFS - 这是一个经过时间检验的存储驱动\nDeviceMapper - Redhat系默认的驱动，有loop和direct两种不同配置\nBtrfs - 我…. 这个文件系统的快照真的是贼好用，但是性能什么的….我倒觉得都一般\nZFS - 还没用过\nVFS - 还没用过\nOverlay2 - 简单的接触了一下，docker目前推荐的存储驱动\n\n关于存储驱动选择的相关博客及文章\nDocker引擎 - 选择存储驱动Docker五种存储驱动原理及应用场景和性能测试对比Docker系统八：Docker的存储驱动 \n\nloop-lvm这是docker默认安装之后的选择，因为这样可以out-of-box，但是据说稳定性不佳，我没遇到稳定性的问题，但是遇到了IO高导致的整个虚拟机运行缓慢。Loop-LVM其实使用了linux中的使用loop设备我之前安装的一套k8s默认是使用overlay2的存储，可能是内核的版本过低导致无法使用其他的存储驱动，所以我觉得默认使用了loop-lvm。\n\nloop-lvm的工作模式是，默认在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;devicemapper&#x2F;devicemapper&#x2F;目录下生成data&amp;metadata两个稀疏文件（我目前还不知道什么叫做稀疏文件），并将两个文件挂载为loop设备做为块设备来使用。按照这个说法的话确实如果直接对裸设备的想能和稳定性都应该更强。所以下面可以动手啦~\n\ndirect-lvm这里直接放一个官方文档的链接好了。Device-Mapper-driver\n切换loop-lvm到direct-lvm想要切换的原因其实是已经安装好的这个k8s的master节点在跑了很久之后，总是被zabbix监控到报警，硬盘负载高；cpu进程数高。cpu的进程数量高可以理解，毕竟监控了如此多的容器。硬盘负载高这个报警在系统中发现是&#x2F;dev&#x2F;loop2这个设备。对应去查找 docker info中的信息，发现这是k8s的存储所使用的。进而搜索到了关于loop-lvm&amp;direct-lvm的相关问题，发现使用loop设备的方式应该实在性能上有影响的，k8s不推荐生产环境使用，所以考虑切换过来，今后毕竟还是要长期使用的。值得注意的是：切换一定会导致之前的容器无法使用。而且目前来看关键的数据是不能恢复的，所以最好是在之前已经做好了计划。\n自动托管配置自动托管的配置主要是两部分，首先是建立一个空的LVM，不需要挂载，只要系统识别到即可。之后是更改配置文件及重启docker服务。\n\n在虚拟机中加入一块新的硬盘，fdisk中识别为&#x2F;dev&#x2F;sdb；\n关闭docker。systemctl stop docker.\n在配置文件中加入如下的配置，注意格式不要错，不要丢下末尾的逗号：&#123;\n  &quot;storage-driver&quot;: &quot;devicemapper&quot;,   # 告诉docker应用使用的存储驱动\n  &quot;storage-opts&quot;: [\n    &quot;dm.directlvm_device&#x3D;&#x2F;dev&#x2F;sdb&quot;,\t# 指定使用的块设备，不需要格式化，不需要分区。在这里指定了设备docker会自动完成创建LVM等等操作。\n    &quot;dm.thinp_percent&#x3D;95&quot;,\t# 指定Thinpool占用的百分比\n    &quot;dm.thinp_metapercent&#x3D;1&quot;,\t# 指定Thinpool Meta数据使用的百分比\n    &quot;dm.thinp_autoextend_threshold&#x3D;80&quot;,\t# 指定自动扩容的阈值\n    &quot;dm.thinp_autoextend_percent&#x3D;20&quot;,\t# 指定自动扩容的比例\n    &quot;dm.directlvm_device_force&#x3D;false&quot;\t# 是否强制格式化设备，默认是false。如果使用dockerd启动的时候出现了提供需要强行格式化设备的提示，就改为True。\n  ]\n&#125;\n重新启动docker服务。如果正常启动可通过docker info 查看是否已经切换过来。\n如果没有能成功启动，尝试重启虚拟机；尝试使用dockerd命令直接启动，根据dockerd的日志信息进行相应的修改。\n常见错误有：Sep 23 18:38:03 k8s-master dockerd: time&#x3D;&quot;2019-09-23T18:38:03.931876136+08:00&quot; level&#x3D;warning msg&#x3D;&quot;[graphdriver] WARNING: the devicemapper storage-driver is deprecated, and will be removed in a future release&quot;\n# 存储驱动将会在未来的版本被移除的警告。这不会导致docker无法运行。\nSep 23 18:25:52 localhost dockerd: Error starting daemon: error initializing graphdriver: &#x2F;dev&#x2F;sdb is already part of a volume group &quot;docker&quot;: must remove this device from any volume group or provide a different device\n# 这个问题说明 docker 认为你的&#x2F;dev&#x2F;sdb上已经被创建了LVM，你需要手动指定，自动托管不会对这个设备进行操作。这会导致docker无法启动。\n# 首先你需要把&#x2F;dev&#x2F;sdb这个设备从LVM里面移除，lvdelete,pvdelete,vgdelete, 将设备还原为默认的状态，之后重启docker，将设备的所有操作控制都交给docker来做，就不会有这个错误了。\n\n总结在更换之后目前性能稳定，IO的负载也下来了。总体来看还是不错的。不过随着kubernetes的发展，我觉得这种问题应该会越来越少。还是推荐在安装的时候直接调整，不然数据的随时确实带来了一些麻烦。\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"RabbitMQ_SysV风格管理脚本模板","url":"/2019/09/23/Linux/Linux_RabbitMQ-Problems/","content":"需要自己写一个RabbitMQ的SysV脚本，所以找了一个模板，如果需要的时候可以改改就用了。\n\n\nSysV脚本#!/bin/sh\n#\n# rabbitmq-server RabbitMQ broker\n#\n# chkconfig: - 80 05\n# description: Enable AMQP service provided by RabbitMQ\n#\n### BEGIN INIT INFO\n# Provides:          rabbitmq-server\n# Required-Start:    $remote_fs $network\n# Required-Stop:     $remote_fs $network\n# Description:       RabbitMQ broker\n# Short-Description: Enable AMQP service provided by RabbitMQ broker\n### END INIT INFO\n# Source function library.\n. /etc/init.d/functions\n# 一些加注释的位置是需要修改的参数\nPATH=/sbin:/usr/sbin:/bin:/usr/bin:/data/erlang/bin  # 更改PATH指向Erlang的路径\nNAME=rabbitmq-server # 服务的名称，可以和脚本名称一致\nDAEMON=/data/rabbitmq_server-3.6.13/sbin/$&#123;NAME&#125;\t# 启动为守护进程的命令所在绝对路径\nCONTROL=/data/rabbitmq_server-3.6.13/sbin/rabbitmqctl\t# 制定rabbitmqctl程序的所在位置， 绝对路径\nDESC=rabbitmq-server\t# 目标服务\nUSER=root\t# 运行时的用户， 线上服务都使用了root用户\nexport HOME=/data/rabbitmq_server-3.6.13/\t# 指定RabbitMQ的HOME目录，默认是在安装目录下；也有可能是在运行RabbitMQ用户的家目录下\nROTATE_SUFFIX=\n# INIT_LOG_DIR=/usr/local/rabbitmq/var/rabbitmq\nINIT_LOG_DIR=/data/rabbitmq_server-3.6.13/var/log/rabbitmq\t# 指出log的目录\n# PID_FILE=/var/run/rabbitmq/pid\t\nPDI_FILE=/data/rabbitmq_server-3.6.13/var/lib/rabbitmq/mnesia/rabbit@ean-online-dubbo-zk-rmq-server-209.pid # 指出当前RabbitMQ的PID文件所在目录\nSTART_PROG=\"daemon\"\nLOCK_FILE=/var/lock/subsys/$NAME \ntest -x $DAEMON || exit 0\ntest -x $CONTROL || exit 0\nRETVAL=0\nset -e\n[ -f /etc/default/$&#123;NAME&#125; ] &amp;&amp; . /etc/default/$&#123;NAME&#125;\n[ -f /etc/sysconfig/$&#123;NAME&#125; ] &amp;&amp; . /etc/sysconfig/$&#123;NAME&#125;\nensure_pid_dir () &#123;\n    PID_DIR=`dirname $&#123;PID_FILE&#125;`\n    if [ ! -d $&#123;PID_DIR&#125; ] ; then\n        mkdir -p $&#123;PID_DIR&#125;\n        chown -R $&#123;USER&#125;:$&#123;USER&#125; $&#123;PID_DIR&#125;\n        chmod 755 $&#123;PID_DIR&#125;\n    fi\n&#125;\nremove_pid () &#123;\n    rm -f $&#123;PID_FILE&#125;\n    rmdir `dirname $&#123;PID_FILE&#125;` || :\n&#125;\nstart_rabbitmq () &#123;\n    status_rabbitmq quiet\n    if [ $RETVAL = 0 ] ; then\n        echo RabbitMQ is currently running\n    else\n        RETVAL=0\n        # RABBIT_NOFILES_LIMIT from /etc/sysconfig/rabbitmq-server is not handled\n        # automatically\n        if [ \"$RABBITMQ_NOFILES_LIMIT\" ]; then\n                ulimit -n $RABBITMQ_NOFILES_LIMIT\n        fi\n        ensure_pid_dir\n        set +e\n        RABBITMQ_PID_FILE=$PID_FILE $START_PROG $DAEMON \\\n            > \"$&#123;INIT_LOG_DIR&#125;/startup_log\" \\\n            2> \"$&#123;INIT_LOG_DIR&#125;/startup_err\" \\\n            0&lt;&amp;- &amp;\n        $CONTROL wait $PID_FILE >/dev/null 2>&amp;1\n        RETVAL=$?\n        set -e\n        case \"$RETVAL\" in\n            0)\n                echo SUCCESS\n                if [ -n \"$LOCK_FILE\" ] ; then\n                    touch $LOCK_FILE\n                fi\n                ;;\n            *)\n                remove_pid\n                echo FAILED - check $&#123;INIT_LOG_DIR&#125;/startup_\\&#123;log, _err\\&#125;\n                RETVAL=1\n                ;;\n        esac\n    fi\n&#125;\nstop_rabbitmq () &#123;\n    status_rabbitmq quiet\n    if [ $RETVAL = 0 ] ; then\n        set +e\n        $CONTROL stop $&#123;PID_FILE&#125; > $&#123;INIT_LOG_DIR&#125;/shutdown_log 2> $&#123;INIT_LOG_DIR&#125;/shutdown_err\n        RETVAL=$?\n        set -e\n        if [ $RETVAL = 0 ] ; then\n            remove_pid\n            if [ -n \"$LOCK_FILE\" ] ; then\n                rm -f $LOCK_FILE\n            fi\n        else\n            echo FAILED - check $&#123;INIT_LOG_DIR&#125;/shutdown_log, _err\n        fi\n    else\n        echo RabbitMQ is not running\n        RETVAL=0\n    fi\n&#125;\nstatus_rabbitmq() &#123;\n    set +e\n    if [ \"$1\" != \"quiet\" ] ; then\n        $CONTROL status 2>&amp;1\n    else\n        $CONTROL status > /dev/null 2>&amp;1\n    fi\n    if [ $? != 0 ] ; then\n        RETVAL=3\n    fi\n    set -e\n&#125;\nrotate_logs_rabbitmq() &#123;\n    set +e\n    $CONTROL rotate_logs $&#123;ROTATE_SUFFIX&#125;\n    if [ $? != 0 ] ; then\n        RETVAL=1\n    fi\n    set -e\n&#125;\nrestart_running_rabbitmq () &#123;\n    status_rabbitmq quiet\n    if [ $RETVAL = 0 ] ; then\n        restart_rabbitmq\n    else\n        echo RabbitMQ is not runnning\n        RETVAL=0\n    fi\n&#125;\nrestart_rabbitmq() &#123;\n    stop_rabbitmq\n    start_rabbitmq\n&#125;\ncase \"$1\" in\n    start)\n        echo -n \"Starting $DESC: \"\n        start_rabbitmq\n        echo \"$NAME.\"\n        ;;\n    stop)\n        echo -n \"Stopping $DESC: \"\n        stop_rabbitmq\n        echo \"$NAME.\"\n        ;;\n    status)\n        status_rabbitmq\n        ;;\n    rotate-logs)\n        echo -n \"Rotating log files for $DESC: \"\n        rotate_logs_rabbitmq\n        ;;\n    force-reload|reload|restart)\n        echo -n \"Restarting $DESC: \"\n        restart_rabbitmq\n        echo \"$NAME.\"\n        ;;\n    try-restart)\n        echo -n \"Restarting $DESC: \"\n        restart_running_rabbitmq\n        echo \"$NAME.\"\n        ;;\n    *)\n        echo \"Usage: $0 &#123;start|stop|status|rotate-logs|restart|condrestart|try-restart|reload|force-reload&#125;\" >&amp;2\n        RETVAL=1\n        ;;\nesac\nexit $RETVAL\n\n一些问题错误1在Work目录下运行这个脚本的时候可以正常使用，但是在&#x2F;etc&#x2F;init.d&#x2F;目录下不正常？ 报错如下：\nservice rabbitmq-server status  \nStatus of node rabbit@HOSTNAME ...  \nError: unable to perform an operation on node 'rabbit@HOSTNAME'. Please see diagnostics information and suggestions below.  \n  \nMost common reasons for this are:  \n  \n * Target node is unreachable (e.g. due to hostname resolution, TCP connection or firewall issues)  \n * CLI tool fails to authenticate with the server (e.g. due to CLI tool's Erlang cookie not matching that of the server)  \n * Target node is not running  \n  \nIn addition to the diagnostics info below:  \n  \n * See the CLI, clustering and networking guides on http://rabbitmq.com/documentation.html to learn more  \n * Consult server logs on node rabbit@HOSTNAME \n  \nDIAGNOSTICS  \n===========  \n  \nattempted to contact: ['rabbit@HOSTNAME']  \n  \nrabbit@HOSTNAME:  \n  * connected to epmd (port 4369) on HOSTNAME  \n  * epmd reports node 'rabbit' uses port 25672 for inter-node and CLI tool traffic  \n  * TCP connection succeeded but Erlang distribution failed  \n  \n  * Authentication failed (rejected by the remote node), please check the Erlang cookie \n  \n  \nCurrent node details:  \n * node name: 'rabbitmq@HOSTNAME'  \n * effective user's home directory: /data/rabbitmq-$version  \n * Erlang cookie hash: fhaluhadgahlfhlashdfag \n\n因为RabbitMQ的认证是通过文件 ： .erlang.cookie这个文件，但是这个文件不是在系统的一个位置有，如果使用find命令查找的话会发现两个目录都有这个文件，我的查找结果一个是在root的家目录下，还有一个是在安装目录，也就是程序的$HOME下。需要自行判断程序使用的是那个目录下在Erlangcookie文件进行的认证及通信，在SysV脚本中，export $HOME到正确的文件目录下，就可以正常使用service命令了。  \n错误2使用service rabbitmq-server status 的时候， 提示$HOME 需要被设置，就在SysV脚本里面直接export就可以了。\n","categories":["Linux"],"tags":["RabbitMQ"]},{"title":"All this I did without you","url":"/2019/09/19/Books_All%20this%20I%20did%20without%20you/","content":"English Text for exercise the Reading and listening.\n\n\nLetter No. 028\nJuly 31st, 1978\nGerald Durrell, a respected conservationist wrote a love letter to his future wife, and then one of his students taking her PhD at Duke University, Lee McGeorge.\n~\nMy darling McGeorge,\nYou said that things seemed clearer when they were written down. Well, here with is a very boring letter in which I will try and put everything down so that you may read and re-read it in horror at your folly in getting involved with me. Deep breath.\nTo begin with I love you with a depth and passion that I have felt for no one else in this life and if it astonishes you it astonishes me as well. Not I hasten to say, because you are not worth loving. Far from it. It’s just that, first of all, I swore I would not get involved with another woman. Secondly, I have never had such a feeling before and it is almost frightening. Thirdly, I would never have thought it possible that another human being could occupy my waking (and sleeping) thoughts to the exclusion of almost everything else.\nFourthly, I never thought that — even if one was in love — one could get so completely besotted with another person, so that a minute away from them felt like a thousand years.\nFifthly, I never hoped, aspired, dreamed that one could find everything one wanted in a person. I was not such an idiot as to believe this was possible. Yet in you I have found everything I want: you are beautiful, gay, giving, gentle, idiotically and deliciously feminine, sexy, wonderfully intelligent and wonderfully silly as well. I want nothing else in this life than to be with you, to listen and watch you (your beautiful voice, your beauty), to argue with you, to laugh with you, to show you things and share things with you, to explore your magnificent mind, to explore your magnificent mind, to explore your wonderful body, to help you, protect you , serve you, and bash you on the head when I think you are wrong… not to put too fine a point on it I consider that I am the only man outside mythology to have found the crock of gold at the rainbow’s end.\nBut — having said all that — let us consider things in detail. Don’t let this become public but… well, I have one or two faults. Minor ones, I hasten to say. For example, I am inclined to be overbearing. I do it for the best possible motives (all tyrants say that) but I do tend (without thinking) to tread people underfoot. You must tell me when I am doing it to you, my sweet, because it can be a very bad thing in a marriage.\nRight. Second blemish. This, actually, is not so much a blemish  of character  as a blemish of circumstance. Darling I want you to be you in your own right, and I will do everything I can to help you in this. But you must take into consideration that I am also me in my own right and that I have a headstart on you… what I am trying to say is that you must not feel offended if you are sometimes treated simply as my wife. Always remember that what you lose on the swings, you gain on the roundabouts. But I am an established ‘creature’ in the world, and so — on occasions — you will have to live in my shadow. Nothing gives me less pleasure than this but it is a fact of life to be faced.\nThird (and very important and nasty) blemish: jealousy. I don’t think you know what jealousy is (thank God) in the real sense of the word. I know you have felt jealousy over Lincoln’s wife and child but this is what I call normal jealousy, and this — to my regret — is not what I’ve got. What I have got is a black moster that can pervert my good sense, my good humour and any goodness that I have in my make-up. It is really a Jekyll and Hyde situation… my Hyde is stronger than my good sense and defeats me, hard though I try. As I told you, I have always known that this lurks within me, but I couldn’t control it, and my monster slumbered and nothing happened to awake it. Then I met you and I felt my monster stir and become half awake when you told me of Lincoln and others you have known, and with your letter my monster came out of its lair, black, irrational, bigoted, stupid, evil, malevolent. You will never know how terribly corrosive jealousy is; it is a physical pain as though you had swallowed acid or red hot coals. It is the most terrible of feelings. But you can’t help it — at least I can’t, and God knows I’ve tried. I don’t want any ex-boyfriends sitting in church when I marry you. On our wedding day, I want nothing but happiness, for both you and me, and I know I won’t be happy if there is a church full of your ex-conquests. When I marry you I will have no past, only a future: I don’t want to drag my past into our future and I don’t want you to do it , either. Remember I am jealous of you because I love you. You are never jealous of something you don’t care about. OK, enough about jealousy.\nNow, let me tell you something… I have seen a thousand sunsets and sunrises, on land where it floods forest and mountains with honey-coloured light, at sea where it rises and sets like a blood orange in a multi-coloured nest of cloud, slipping in and out of the vast ocean. I have seen a thousand moons: harvest moons like gold coins, winter moons as white as ice chips, new moons like baby swans’ feathers.\nI have seen seas as smooth as if painted, coloured like shot silk or blue as a kingfisher or transparent as glass or black and crumpled with foam, moving ponderously and murderously.\nI have felt winds straight from the South Pole, bleak and wailing like a lost child; winds as tender and warm as a lover’s breath; winds that carried the astringent smell of salt and the death of seaweeds; winds that carried the moist rich smell of a forest floor, the smell of a million flowers. Fierce winds that churned and moved the sea like yeast, or winds that made the waters lap at the shore like a kitten.\nI have known silence: the cold, earthy silence at the bottom of a newly dug well; the implacable stony silence of a deep cave; the hot, drugged midday silence when everything is hypnotized and stilled into silence by the eye of the sun; the silence when great music ends.\nI have heard summer cicadas cry so that the sound seems stitched into your bones. I have heard tree frogs in an orchestration as complicated as Bach singing in a forest lit by a million emerald fireflies. I have heard the Keas calling over grey glaciers that groaned to themselves like old people as they inched their way to the sea. I have heard the hoarse street vendor cries of the mating Fur seals as they sang to their sleek golden wives, the crisp staccato admonishment of the Rattlesnake, the cobweb squeak of the Bat and the belling roar of the Red deer knee-deep in purple heather. I have heard Wolves baying at a winter’s moon, Red Howlers making the forest vibrate with their roaring cries. I have heard the squeak, purr and grunt of a hundred multi-coloured reef fishes.\nI have seen hummingbirds flashing like opals round a tree of scarlet blooms, humming like a top. I have seen flying fish, skittering like quicksilver across the blue waves, drawing silver lines on the surface with their tails. I have seen Spoonbills flying home to roost like a scarlet banner across the sky. I have seen Whales, black as tar, cushioned on a cornflower blue sea, creating a Versailles of fountain with their breath. I have watched butterflies emerge and sit, trembling, while the sun irons their wings smooth. I have watched Tigers, like flames, mating in the long grass. I have been dive-bombed by an angry Raven, black and glossy as the Devil’s hoof. I have lain in water warm as milk, soft as silk, while around me played a host of Dolphins. I have met a thousand animals and seen a thousand wonderful things… but –\nAll this I did without you. This was my loss.\nAll this I want to do with you. This will be my gain.\nAll this I would gladly have forgone for the sake of one minute of your company, for your laugh, your voice, your eyes, hair, lips, body, and above all for your sweet, ever surprising mind which is an enchanting quarry in which it is my privilege to delve.\n","categories":["Books"],"tags":["English"]},{"title":"Gnome快捷键","url":"/2019/09/19/Linux/Linux_Gnome-Tricks/","content":"Gnome桌面环境的快捷键。\n\n\n快速启动一个应用\nSuper \nHot Corner这两个我都用，HotCorner不是一个按键，而是一个动作，是指屏幕的右上角，鼠标指针用力撞过去，撞开所有的应用窗口。在Gnome的环境中，Super是一个超方便的键，当我需要打开vscode的时候，可以在任意时候通过super+code+Enter直接打开vscode应用。当按下Super的时候会自动触发一个全局的搜索，可以通过Super快速查看自己的需要的文件或者应用，这个功能是我最喜欢Gnome的地方。\n\n执行命令\nAlt+F2最常用的就是重启Gnome环境，通过Alt+F2 调出的命令窗口，使用r命令重启Gnome。\n\n应用的切换\nSuper+TAB使用Super+TAB可以在应用之间快速切换。如果Cover-alt-tab的插件，还有三维动画。\nSuper+&#96;这个组合键是切换应用内窗口的，我自己用的不多。\n\n快速显示主屏幕应用菜单\nSuper+a快速显示主屏幕的所有应用程序页。\n\n切换工作区\nSuper+PageUP or PageDown\nCtrl+Alt+UP or DOWN可以在多个工作区之间快速切换，但是我自己常用的是第二种。键位上舒服一些。\n\n移动窗口到其他工作区\nCtrl+Alt+Shift+UP or Down\nSuper+Shift+PageUP or PageDown将当前的焦点窗口移动到前后的工作区，并保持当前窗口的焦点不变。这个也是特别好用的快捷键。\n\n呼出通知中心和日历\nSuper+m这个快捷键最早的Gnome上是通知栏，现在用的比较少了，毕竟Gnome已经不是下方的通知栏了，是上面的日历加上通知中心的方式了。我自己的用的时候大部分是看时间和日历。\n\n截图\nPrintScreen截取所有屏幕内容，保存到文件。\nAlt+PrintScreen截取当前窗口的内容，保存到文件。\nShift+PrintScreen截取选择的区域，保存到文件。\n\nNOTE： 最方便的就是，使用Ctrl+Shift PrintScreen和Ctrl+PrintScreen 可以直接选取的区域截图，将截图保存在剪贴板，截图完成直接粘贴即可。\n最大化最小化和分屏\nSuper+UP最大化到铺满屏幕\nSuper+LEFT靠左占据一般屏幕\nSuper+RIGHT靠右占据一般屏幕\nSuper+H最小化，隐藏\n\n这些基本上就是我觉得常用而且好用的快捷键啦，但是还是可以自定义的，在设置里面，也可以随便设置，但是那样的话就不是拿来直接可以用的啦。\n","categories":["Linux"],"tags":["Linux"]},{"title":"Nginx配置文件中if判断与try_files","url":"/2019/09/19/Linux/Linux_Nginx-if-try-files-Notwork/","content":"Nginx的if判断问题，导致try_files字段未能正常生效。\n\n\n配置文件server &#123;\n  listen 80;\n  server_name liarlee.site;\n\n  set $mobile_rewrite do_not_perform;\n  if ($http_user_agent ~* &quot;(android|bb\\d+|meego).+mobile|avantgo|bada\\&#x2F;|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\&#x2F;|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino&quot;) &#123;\n    set $mobile_rewrite perform;\n  &#125;\n  if ($http_user_agent ~* &quot;^(1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\&#x2F;|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\&#x2F;)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\&#x2F;)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\&#x2F;(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\&#x2F;|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\&#x2F;|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\&#x2F;|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-)&quot;) &#123;\n    set $mobile_rewrite perform;\n  &#125;\n  if ($http_cookie ~ &#39;gotopc&#x3D;true&#39;) &#123;\n    set $mobile_rewrite do_not_perform;\n  &#125;\n\n  location &#x2F; &#123;\n    root &#x2F;var&#x2F;html&#x2F;www&#x2F;[PC_WEB_ROOT];\n     # 问题出现在了这里。\n    if ($mobile_rewrite &#x3D; perform) &#123;\n      root &#x2F;var&#x2F;html&#x2F;www&#x2F;[MOBILE_WEB_ROOT];\n    &#125;\n    index index.html index.htm\n    error_page 404 index.html\n    # 问题就出现在了这里。\n    try_files $uri $uri&#x2F; &#x2F;index.html 404; \n  &#125;\n&#125;\n表现在访问请求来的同时，判断是不是手机访问，如果是PC使用默认的PC_WEB_ROOT,  如果是手机的话，访问到MOBILE_WEB_ROOT。本身逻辑和使用是没有问题的，但是需要使用try_files字段的时候，会导致PC站点的tryfiles可以正常生效；但是手机不会有tryfile的效果。\n可行的方式 -  proxypass看到了一篇文章说到nginx的IF语句，可以正常不出奇怪问题的只有Proxy_pass,Rewrite两个，因为项目无法用rewrite所以选择了Proxypass。配置文件分为两个部分，每个网站放在一个Server下。\nPC配置文件server &#123;\n  listen 80;\n  server_name liarlee.site;\n\n  set $mobile_rewrite do_not_perform;\n  if ($http_user_agent ~* &quot;(android|bb\\d+|meego).+mobile|avantgo|bada\\&#x2F;|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\\&#x2F;|plucker|pocket|psp|series(4|6)0|symbian|treo|up\\.(browser|link)|vodafone|wap|windows ce|xda|xiino&quot;) &#123;\n    set $mobile_rewrite perform;\n  &#125;\n  if ($http_user_agent ~* &quot;^(1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\\-(n|u)|c55\\&#x2F;|capi|ccwa|cdm\\-|cell|chtm|cldc|cmd\\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\\-s|devi|dica|dmob|do(c|p)o|ds(12|\\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\\-|_)|g1 u|g560|gene|gf\\-5|g\\-mo|go(\\.w|od)|gr(ad|un)|haie|hcit|hd\\-(m|p|t)|hei\\-|hi(pt|ta)|hp( i|ip)|hs\\-c|ht(c(\\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\\-(20|go|ma)|i230|iac( |\\-|\\&#x2F;)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\\&#x2F;)|klon|kpt |kwc\\-|kyo(c|k)|le(no|xi)|lg( g|\\&#x2F;(k|l|u)|50|54|\\-[a-w])|libw|lynx|m1\\-w|m3ga|m50\\&#x2F;|ma(te|ui|xo)|mc(01|21|ca)|m\\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\\-2|po(ck|rt|se)|prox|psio|pt\\-g|qa\\-a|qc(07|12|21|32|60|\\-[2-7]|i\\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\\&#x2F;|sa(ge|ma|mm|ms|ny|va)|sc(01|h\\-|oo|p\\-)|sdk\\&#x2F;|se(c(\\-|0|1)|47|mc|nd|ri)|sgh\\-|shar|sie(\\-|m)|sk\\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\\-|v\\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\\-|tdg\\-|tel(i|m)|tim\\-|t\\-mo|to(pl|sh)|ts(70|m\\-|m3|m5)|tx\\-9|up(\\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\\-|your|zeto|zte\\-)&quot;) &#123;\n    set $mobile_rewrite perform;\n  &#125;\n  if ($http_cookie ~ &#39;gotopc&#x3D;true&#39;) &#123;\n    set $mobile_rewrite do_not_perform;\n  &#125;\n\n  location &#x2F; &#123;\n    root &#x2F;var&#x2F;html&#x2F;www&#x2F;[PC_WEB_ROOT];\n    \n    if ($mobile_rewrite &#x3D; perform) &#123;\n      proxy_pass http:&#x2F;&#x2F;127.0.0.1:81; # 更改这里为proxypass；\n    &#125;\n\n    index index.html index.htm\n    error_page 404 index.html\n    \n    try_files $uri $uri&#x2F; &#x2F;index.html 404; \n  &#125;\n&#125;\t\n&#125;\n\nMOBILE配置文件server &#123;\n  listen 81;\n    server_name 127.0.0.1;\n    location &#x2F; &#123;\n    root &#x2F;var&#x2F;www&#x2F;html&#x2F;[MOBILE_WEB_ROOT];\n    index index.html index.htm;\n    error_page 404 index.html;\n    try_files $uri $uri&#x2F; &#x2F;index.html 404; \n  &#125;\n&#125;","categories":["Linux"],"tags":["Nginx"]},{"title":"虚拟机制作模板的步骤及设置","url":"/2019/09/18/Linux/Linux_CentOS7-VM-Template/","content":"换了工作之后接管了这边旧的ESXi和上面的虚拟机，之前的模板不是特别的合适，自己开始动手做模板。最终期望的目标是：1. 修改IP地址；2.yum install 收工。  \n\n\nCentOS虚拟机模板的制作系统的硬件配置系统的硬件规格 – 4cpu; 8G-RAM\nOS的版本系统的版本 – CentOS 7.6 1810 x64 \n安装VMwareToolsyum install -y epel vim wget curl net-tools open-vm-tools htop iotop iftop tree\n\n安装Zabbix-agentrpm -ivh zabbix-agent-$version.rpm \n关闭SELinuxsed -i &#39;s@SELINUX&#x3D;enforcing@SELINUX&#x3D;disabled@g&#39; &#x2F;etc&#x2F;selinux&#x2F;config\n更改hostnamehostname有一些有趣的问题，CentOS6.8 中的hostname可以定义在/etc/sysconfig/network文件中，系统启动的时候先读取/etc/sysconfig/network文件中的定义，如果没有的话读取/etc/hosts文件中的定义；和我自己的之前的记忆不一样。我之前一直都是之间编辑/etc/hostname，直接将主机名echo到这个文件中就可以了，不过向来这些区别都不大，先这样吧。\n清除硬件及网卡的信息将网卡配置文件中的UUID和 HWADDR直接删除或者是注释掉，IPADDR留空。/etc/sysconfig/network-scripts/ifcfg-eth0\n删除udevrm -rf /etc/udev/rules.d/70-*NOTE：命令的效果貌似和sys-unconfig 的效果是一样的。\n关闭防火墙systemctl disable firewalld\niptables -F\niptables -X\niptables -Z\n更改Grub的等待时间虚拟机中的模板大部分是不需要等待grub给出的操作选单时间的，等待操作的时间是5秒，我们给出1秒就够了。\nvim &#x2F;etc&#x2F;default&#x2F;grub\nGRUB_TIMEOUT&#x3D;1\n清理工作rm -rf &#x2F;etc&#x2F;ssh&#x2F;*key*.\nrm -rf &#x2F;root&#x2F;.ssh&#x2F;\nsystemctl stop rsyslog\nsystemctl stop auditd\n&#x2F;usr&#x2F;bin&#x2F;yum clean all\nlogrotate -f &#x2F;etc&#x2F;logrotate.conf \nrm -f &#x2F;var&#x2F;log&#x2F;dmesg.old \nrm -rf &#x2F;var&#x2F;log&#x2F;anaconda*\ncat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;audit&#x2F;audit.log \ncat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;wtmp \ncat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;lastlog \ncat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;grubby \n清除系统的唯一ID&gt; &#x2F;etc&#x2F;machine-id\n清除系统命令历史记录unset HISTFILE\nhistory -c &amp;&amp; rm -rf &#x2F;root&#x2F;.bash_histroy\nsys-unconfig关机执行sys-unconfig等待关机，然后转换为模板。\n","categories":["ESXi"],"tags":["Linux"]},{"title":"通过systemd管理软件和服务","url":"/2019/09/17/Linux/Linux_Systemctl-ManageService/","content":"通过systemctl 来管理系统的服务和软件，但是如果是自己安装的软件就没有办法使用了。其实是可以自己定义systemd的管理脚本的，类似与之前的SysV风格的管理脚本。\n\n\nSystemd是什么\nsystemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system. systemd provides aggressive parallelization capabilities, uses socket and D-Bus activation for starting services, offers on-demand starting of daemons, keeps track of processes using Linux control groups, maintains mount and automount points, and implements an elaborate transactional dependency-based service control logic. systemd supports SysV and LSB init scripts and works as a replacement for sysvinit. Other parts include a logging daemon, utilities to control basic system configuration like the hostname, date, locale, maintain a list of logged-in users and running containers and virtual machines, system accounts, runtime directories and settings, and daemons to manage simple network configuration, network time synchronization, log forwarding, and name resolution.\n\n管得还是挺多的，主要是启动PID为1的进程并启动其他的程序，并行执行，维护挂载点及自动挂载，服务之间的依赖关系，日志进程，\nSystemd的Units文件Systemd默认的文件配置路径有：    - &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;*    - &#x2F;run&#x2F;systemd&#x2F;system&#x2F;*    - &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;*\nSystemd Unit File 的模板有这样的几个模块：\n\n[Unit]\n[Service]\n[Install]\n\nUnitUnit的作用是记录文件的通用信息。\n\nDescripition – 对软件或服务的描述。\nBefore OR After – 定义启动的顺序，在某些服务 启动之前 OR 启动之后 ，在启动这个服务。其中还定义了服务的依赖关系，先后顺序。常用的值有 network.target, Multi-User.target, network.service 等等。\nRequires – 并行启动所指定的其他服务。\nRequireOverrideable – 类似与require，但是不同的是手动启动的时候不会报错。\nRequisite – 只要启动失败了就直接报错停止，强硬版本的requires。\nWants – 启动依赖单元的常用选项，在启动的同时调起其他的Unit，如果其他单元启动失败了也不会影响当前定义的Unit的启动。\nConflicts – 冲突单元，启动的时候发现了Conflict中定义的其他单元就会尝试终止Unit。\n\nService\nType – 对服务类型的定义，通常有如下三种：- simple 默认的类型，启动就启动，停止就结束了。- forking 守护进程的类型，把必要的启动之后留下守护进程。- oneshot 一次性的服务，启动后就结束了。\nExecStart – 启动的时候执行的命令， 这条命令就是服务的主体。\nExecStartPre OR ExecStartPost – ExecStart执行前后的动作。\nExecStop – 指定服务结束的动作，如果未指定直接kill。\nRestart – 定义了重启的条件和动作，常用的参数有： no, on-sucess, on-failure, on-watchdog, on-abort。\nSuccessExitStatus – ExecStart的返回值。 SuccessExitStatus=1 2 8 SIGKILL\n\nInstall\nWantedBy – 定义启动的情景，几种不同的target: multi-user.target | poweroff.target | rescue.target | graphical.target | reboot.target\nAlias – 别名的设置在这里定义。\n\n标准配置文件 - Libvirtd[Unit]\nDescription&#x3D;Virtualization daemon\nRequires&#x3D;virtlogd.socket\nRequires&#x3D;virtlockd.socket\nWants&#x3D;systemd-machined.service\nBefore&#x3D;libvirt-guests.service\nAfter&#x3D;network.target\nAfter&#x3D;dbus.service\nAfter&#x3D;iscsid.service\nAfter&#x3D;apparmor.service\nAfter&#x3D;local-fs.target\nAfter&#x3D;remote-fs.target\nAfter&#x3D;systemd-logind.service\nAfter&#x3D;systemd-machined.service\nDocumentation&#x3D;man:libvirtd(8)\nDocumentation&#x3D;https:&#x2F;&#x2F;libvirt.org\n\n[Service]\nType&#x3D;simple\nEnvironmentFile&#x3D;-&#x2F;etc&#x2F;conf.d&#x2F;libvirtd\nExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;libvirtd $LIBVIRTD_ARGS\nExecReload&#x3D;&#x2F;bin&#x2F;kill -HUP $MAINPID\nKillMode&#x3D;process\nRestart&#x3D;on-failure\n# At least 1 FD per guest, often 2 (eg qemu monitor + qemu agent).\n# eg if we want to support 4096 guests, we&#39;ll typically need 8192 FDs\n# If changing this, also consider virtlogd.service &amp; virtlockd.service\n# limits which are also related to number of guests\nLimitNOFILE&#x3D;8192\n# The cgroups pids controller can limit the number of tasks started by\n# the daemon, which can limit the number of domains for some hypervisors.\n# A conservative default of 8 tasks per guest results in a TasksMax of\n# 32k to support 4096 guests.\nTasksMax&#x3D;32768\n\n[Install]\nWantedBy&#x3D;multi-user.target\nAlso&#x3D;virtlockd.socket\nAlso&#x3D;virtlogd.socket","categories":["Linux"],"tags":["Linux"]},{"title":"ElasticSearch 01","url":"/2019/09/11/Linux/Linux_ElasticSearch-01/","content":"ElasticSearch的安装过程。\n\n\n\n准备源码包\n需要下载的包有三个：网站地址\nElasticSearch- 分布式、RESTful 风格的搜索和分析。&#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch\nLogstash - 采集、转换、充实，然后输出。&#x2F;usr&#x2F;local&#x2F;src&#x2F;logstash\nKibana - 实现数据可视化。在 Elastic Stack 中进行导航。&#x2F;usr&#x2F;local&#x2F;src&#x2F;kibana\n\n\n\n安装 ElasticSearch\n解压下载的安装包\ntar zxvf elasticsearch-7.3.1-linux-x86_64.tar.gz\ntar zxvf kibana-7.3.1-linux-x86_64.tar.gz\ntar zxvf logstash-7.3.1.tar.gz\n\n\n修改系统参数\nvim &#x2F;etc&#x2F;sysctl.conf\nfs.file-max&#x3D;65535\nvm.max_map_count&#x3D;262144\n\n\nsysctl -p : 重新读取配置文件中的参数，更新的条目会显示在命令执行结果中。\nvim &#x2F;etc&#x2F;security&#x2F;limits.conf * soft nofile 65536\n* hard nofile 131072\n* soft noproc 4096\n* hard noproc 4096\t\n保存退出。\n建立elk用户 useradd elk -p &quot;YOUR-PASSWD&quot;\n将&#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch目录的权限给elk用户。 chown -R elk:elk &#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch&#x2F;\n在&#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch&#x2F;elasticsearch&#x2F;config&#x2F;目录下修改配置文件elasticsearch.yml。 cluster.name: CLUSTER_NAME\nnode.name: HOSTNAME &amp; ROLES\nnode.master: true\nnode.data: true\nnetwork.host: YOUR_HOSTNAME\ndiscovery.zen.ping.unicast.hosts: [&quot;YOUR_OTHER_NODE_HOSTNAME &quot;]\n\n切换到elk用户，尝试启动elk su elk cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch&#x2F;elasticsearch&#x2F;bin&#x2F; .&#x2F;elasticsearch 如果没有错误就可以使用 -d 选项将服务启动到后台。\n\n","categories":["Linux"],"tags":["ElasticSearch"]},{"title":"文件系统及文件管理","url":"/2019/07/24/Linux/Linux_FileManagement/","content":"Linux文件管理笔记\n\n\n文件系统默认的规定是遵守FHS规定的。\n\nFHS(Filesystem Hierarchy Standard) defines the directory structure and directory contents in Linux distributions.  FHS_Website\n\n标准的根文件系统，应该具有如下的结构，所有的文件目录均在根文件系统下。\n\n&#x2F;bin  – 单用户模式下可运行的二进制命令。所有用户都可以使用。\n&#x2F;sbin  – 基本的系统二进制文件。\n&#x2F;boot – 一般为BootLoaderFiles，例如内核，ramfs，grub等等。\n&#x2F;etc  – 常用的应用程序的全局配置文件。Host-specific system-wide configruation。\n&#x2F;etc&#x2F;opt  – &#x2F;opt目录下的程序的配置文件。\n\n\n&#x2F;usr  – 包含了主要的多用户工具及应用。\n&#x2F;usr&#x2F;share – Architecture-independent(shared) data.独立架构的共享数据。\n&#x2F;usr&#x2F;sbin – 非基础的系统库文件，例如网络管理的守护进程。\n&#x2F;usr&#x2F;src – 放内核源码及头文件的目录。\n&#x2F;usr&#x2F;bin – 非基础的命令二进制文件，不需要运行在单用户模式下的命令。\n&#x2F;usr&#x2F;local – 特指存放本地的数据，例如源码包，二进制包等等。\n&#x2F;usr&#x2F;lib – &#x2F;usr&#x2F;bin &amp; &#x2F;usr&#x2F;sbin下的命令所需要的库文件。\n\n\n&#x2F;opt – Optional application software packages.\n&#x2F;mnt – 文件系统的临时挂载位置。\n&#x2F;media – 默认的可移动设备挂载位置。\n&#x2F;dev  – 所有的设备，字符设备，块设备。\n&#x2F;lib – 为&#x2F;bin &amp; &#x2F;sbin目录下的文件及程序提供的库文件。\n&#x2F;lib64 – 可替代格式的库文件，这个目录不是必须存在的，例如64位程序会需要这个目录。\n&#x2F;tmp – 临时文件存放目录。定期清除。\n&#x2F;sys  – 设备，驱动，一些内核功能的相关信息。\n&#x2F;proc – 虚拟文件系统展示进程及内核信息文件。系统启动目录创建，系统关闭目录消失。\n&#x2F;var – 变量文件。在系统运行的过程中反复变化的文件。比如日志，邮件，信息输出等。\n&#x2F;var&#x2F;log – 系统应用程序产生的日志文件默认存放路径。\n&#x2F;var&#x2F;spool&#x2F;mail – 每个用户的邮件。\n\n\n&#x2F;home – 用户的家目录，saved files, personal settings,etc.\n\nBash文件色彩显示的定义在： &#x2F;etc&#x2F;DIR_COLORS , 文件中定义了所有文件类型在Bash中的色彩。新技巧： 有一个$OLDPWD，可通过cd - 切换到上一次离开的目录\n","categories":["Linux"],"tags":["Linux"]},{"title":"Kubernetes集群的学习笔记(2)","url":"/2019/07/23/Linux/Linux_k8s-basic-2/","content":"Kubernetes的基本使用命令。\n\n\n对控制命令进行分类整理：\n查看k8s整体状态的命令\nkubectl describe node node1.docker 查看node的详细信息\nkubectl version 查看kube的版本信息，同时显示客户端的版本及服务端的版本\nkubectl cluster-info 查看kube的集群信息，master节点的所在地址及kubeDNS的所在地址\n\n手动运行Pods的命令\nkubectl run nginx –image&#x3D;nginx –replicas&#x3D;5 启动5个nginx的pods\nkubectl run nginx –image&#x3D;nginx –port:80 –replicas&#x3D;5 –dry-run&#x3D;true 启动5个nginx的pod的测试，但不执行改变，并expose端口80\n\n查看Pods的命令\nkubectl get pods 列出所有节点正在运行的pod的状态信息\nkubectl get deployment 列出所有的deployment控制器的信息，所有的pod属于cni0桥，不属于docker桥\n\n删除单独一个Pods的命令\nkubectl delete pods hayden-nginx-6dd3ffd4c5-ww7mk删除一个指定的pod\n\n手动建立Pods并对外发布的命令\nkubectl expose deployment hayden-nginx –type&#x3D;[{ClusterIP},NodePort,LoadBalancer,ExternalName]  –name&#x3D;nginx-service –target-port&#x3D;80 –protocol&#x3D;TCP创建一个新的service，将内部的通信以service的形式对外交付\n\n查看svc状态的命令\nkubectl get svc查看service的状态\n\n查看属于某个名称空间的命令\nkubectl get nodes -n kube-system -o wide查看属于kube-system的节点的信息\nkubectl get svc -n kube-system -o wide查看属于kube-system的服务的信息\n\n从不重启Pods的设置\nkubectl run t-centos –image&#x3D;centos -it –restart&#x3D;Never –replicas&#x3D;1启动1个普通的容器作为客户端进行服务的访问\nkubectl describe svc nginx-service查看指定服务的详细信息\nkubectl get nodes –show-labels查看节点的标签\n\n对svc进行编辑和修改\nkubectl edit svc nginx-service编辑一个service的属性，命令打开vim的编辑界面进行配置文件的更改\nkubectl delete svc nginx-service删除一个service\nkubectl get deployment -w长时间监控一个控制器的状态改变\n\n对Pods的升级和回滚\nkubectl set image deployment hayden-nginx hayden-nginx&#x3D;nginx:latest对容器指定其他的镜像进行动态的升级及更新\nkubectl rollout status deployment hayden-nginx对容器版本的更新进行进度的查看\n\n指定版本的回滚\nkubectl rollout undo deployment hayden-nginx对升级的版本进行回滚，可指定回滚的版本，默认是上一个版本\n\nService发布到NodePort外部访问到service的方法是定义type为NodePort，可在创建service的时候定义类型，或者使用kubectl edit svc nginx-service将type变更为NodePort 。\n创建资源的逻辑思路刚从docker的管理思路过来的时候还是有些茫然， 不太知道该怎么用。其实从k8s的管理逻辑上，我们将之前的container 变更为 pod，将pod的管理交给一个deployment，将deployment已经启动的所有容器通过对外提供service的方式expose到外部。例如：我需要一个httpd服务，不需要直接去docker启动了，我直接在k8s上定义一个deployment-httpd，通过k8s的控制器去调度容器。如果需要控制容器对外提供服务，那就直接创建一个service，通过service去直接定义和管理对外的服务即可。\n","categories":["Linux"],"tags":["Kubernetes"]},{"title":"Kubernetes集群的学习笔记(1)","url":"/2019/07/21/Linux/Linux_k8s-basic-1/","content":"Kubernetes基础知识及笔记。\n\n\n概念定义容器的出现以及容器编排引擎出现的原因。\n容器编排工具容器最早的模型时LXC+Linux Namespace。容器的出现导致了我们需要对容器进行管理，单机的管理不能满足业务的需要，于是快速衍生出了多种不同的容器编排工具。Docker提供的工具：\n\ndocker compose\ndocker swarm \ndocker machine\n\nIDC的操作系统：\n\nmesos(资源分配工具), marathron(面向容器编排的框架)\n\nGoogle的工具：\n\nKubernetes\n\n当一个产品可以占据35%以上的份额就已属于自然垄断。k8s现在已经处于垄断地位。透过容器所产生的衍生概念有: DevOps, MicroServices, BlockChain.开发模式的开发：瀑布模型，进化到了敏捷开发，精益开发，到现在的DevOps.发布线上的做法：蓝绿部署，灰度部署，金丝雀(canary)DevOps几个简单的名词解释：\n\nCI： 持续集成 - 持续集成通俗一些就是快速提交代码，快速变更需求，快速合并代码。\n\nCD： 持续部署 - 持续部署是指在代码提交变更之后，快速进行部署及测试；传统的代码在提交后需要运维人员手动部署，持续部署其实就是缩短的部署所需要的步骤和周期，尽可能将部署操作交由自动化完成。\n\nCD： 持续交付 - 让软件产品的产出过程在一个短周期内完成，以保证软件可以稳定、持续的保持在随时可以发布的状态。\n有时候，持续交付也与持续部署混淆。持续部署意味着所有的变更都会被自动部署到生产环境中。持续交付意味着所有的变更都可以被部署到生产环境中，但是出于业务考虑，可以选择不部署。如果要实施持续部署，必须先实施持续交付。\n\n\n云原生的概念Native Cloud Application To Wikipedia.\n\n A native cloud application (NCA) is a type of computer software that natively utilizes services and infrastructure from cloud computing providers.\n\n例如：当容器中的nginx需要变更项目文件的时候，容器的环境已经决定了不太容易对项目的变更,早期的程序设计使用配置文件进行定义。因此，最好的办法是可以通过传递宿主机环境变量的方式对镜像进行设置，当项目有变更的时候直接通过读取环境变量的方式对容器内项目的数据进行配置和更改。基于这种思路设计出来的容器或者软件，叫做云原生应用(Native Cloud Application)。\nK8s的前世k8s本来开始是Google内部的Brog系统，Google在Docker被发现了之后，快速的使用Go语言重写了Brog系统，借鉴了Brog的逻辑和设计，所有的代码进行了全面的重构。\nKubernetes项目的Github页面\nK8s的特点\n自动装箱\n自我修复\n水平扩展\n服务发现与负载均衡\n自动发布和回滚\n密钥的配置和管理 – 将配置定义在k8s的对象中，通过k8s在容器启动的时候自动传递环境变量到容器中。\n存储编排 – 可自动创建存储卷供容器需要的自动管理及使用\n\n\n批量处理执行\n\nK8s的工作流程及逻辑\nKubernetes组合多台主机的资源，整合成资源池，并统一对外提供计算、存储等能力的集群。k8s的集群是Master\\Node模型，具有角色分类，需要部署master节点及node节点。master节点一般有三个，node节点理论上可以无限增加。\nk8s的简要工作流程： 收到请求后，Master节点上的Scheduler用来收集所有Node上的可用资源信息，并通过API Server告知资源充足的Node启动相关的容器；Node收到请求会查找需要的镜像，如果本地不存在从Docker Registery上面拉取相应的镜像进行所请求容器的启动。\nk8s可将 Master节点 + Node节点 + Registery节点 同时托管在k8s自身的环境中，也就是直接托管在docker虚拟化层上，因此我们可以通过kubeadm等相关的工具将所有的组件使用容器的方式进行管理和调度。\n\nMaster上的主要组件及服务\nAPI Server # 负责提供对外的及对内的服务接口\n\nScheduler # 选出适合部署容器的node节点，两级筛选：选出所有可以使用的node，在所有合规的node上进行最优选择，选择取决于调度算法。\n\nController-Manager # 检测容器的状态，如果出现异常或不符合定义的状态，就向API申请重新部署相关容器。\n\n\nNode上的主要服务及组件\nKube-Proxy\n容器引擎，docker\nKubelet\nFlannel\n\n逻辑组件：PodNode上可以运行及调度逻辑单元pod，pod是k8s的一个逻辑概念，真实运行起来的还是Container，只是k8s为了方便管理，将一个或多个Container封装了成了逻辑上的Pod，打上了相应的标签，使得可以通过Seletor对不同的Pod进行分类管理和选择。Pod是模拟了传统的虚拟机模式(相同主机上的Pod可以使用宿主机的lo进行网络通信),使得可以构建精细的模型。\n一个pod中可以有多个容器，共享底层名称空间net,UTS,IPC,另外三个互相隔离user,mnt,pid. \nSidecar的概念一般来讲,一个Pod只放入一个容器，如果我们需要放置多个容器，一般为一个主要的一个辅助的，辅助的叫做 sidecar.例如主程序运行在一个容器中，收集日志的程序放在sidecar中。 \nPod的标签选择Pod已经被附加了一些元数据，创建完成的Pod可以通过标签的数值直接识别Pod的类别。k8s使用标签选择器selector来过滤符合条件的资源。所有的对象都可以通过标签选择器进行选择。\npod的不完全分类\n自主式pod # 自行控制自己的生命周期\n控制器管理的pod # 通过管理器可管理生命周期的pod\n\nk8s使用的pod的控制器\nReplicationController - 最早提供的组件，支持滚动更新和回滚，最早只有一个。现在已经不再使用。\nReplicaSet - 新的工具替代了原始的控制器，但是不直接使用，直接使用的是Deployment来管理无状态的pod。\nStatefulSet - 对有状态的Pod进行管理。\nDaemonSet - 控制容器的进程为守护进程的控制器，每个Node上都会运行一个，无法指定副本的数量。一般用于信息和日志的收集。\nJob, Cronjob - 作业管理， 周期性作业管理。\nHPA - HorizontalPodAutoscaler\n\n服务发现的概念为了能快速管理新启动的pod，在pod和用户之间添加了逻辑的中间层，pod启动后需要在service中注册自己的信息,客户端需要相关的服务的时候从service取得相关的信息，完成服务的管理。service是逻辑组件，通过iptables的DNAT来实现相关的service的功能。\n其实服务发现相当于消息中间件，有Pods(生产者)来注册，有用户(消费者)来消费，只是不会被消费掉。\nKube-proxy的作用一旦发现了service的改变，反映到Iptables上，同时向API Server报告。 \nKubernetes的安装部署通过kubeadm工具进行安装及部署，部署的过程其实不复杂，只是镜像的拉取麻烦了一些，记录了一些简单的步骤和问题。安装的时候启动了三个虚拟机，一台虚拟机作为Master节点，其他的两台作为Node节点，所有的机器都需要关闭firewalld，k8s会默认托管iptables的相关规则。安装采用k8s自托管的方式，结构如下，列出了在每个节点上必须运行的服务和容器：  \n\n\n\nmaster\nNode1\nNode2\n\n\n\nKubelet\nKubelet\nKubelet\n\n\nPause\nPause\nPause\n\n\nFlannel\nFlannel\nFlannel\n\n\nKube-Proxy\nKube-Proxy\nKube-Proxy\n\n\nAPI Server\n\n\n\n\nSchedular\n\n\n\n\nController-manager\n\n\n\n\nCoredns\n\n\n\n\netcd\n\n\n\n\n机器的信息如下：  \n\n\n\nIP-ADDRESS\nHOSTNAME\nROLES\n\n\n\n192.168.229.144\nmaster.docker\nmaster\n\n\n192.168.229.145\nnode1.docker\nnode\n\n\n192.168.229.146\nnode2.docker\nnode\n\n\n安装基础环境这里记录一下所有机器都需要安装的环境及软件。\n\n编辑hostname vim /etc/hostname\n关闭firewalld systemctl mask firewalld\n关闭selinux sudo sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config\n关闭Swap分区, 注释掉swap的相关行，或者在安装的时候默认不分区即可。 vim /etc/fstab \n配置docker-ce，kubernetes相关软件源,从阿里云直接配置到本地的机器上。 # kubernetes部分\ncat &lt;&lt;EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n# docker -ce部分\ncurl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun\n同步镜像仓库，从docker-ce源安装docker-ce&#x2F;docker-cli&#x2F;containerd；其实可以关闭gpgcheck，并将其他同步失败的源关闭即可，我自己只是为了做学习用途，关闭了fedora的update源，总是同步失败耽误我的时间。 dnf makecache -y &amp;&amp; \\\ndnf install -y docker-ce docker-cli containerd &amp;&amp; \\\nsystemctl start docker &amp;&amp; \\\nsystemctl enable docker \n从kubernetes源安装kubeadm\\kubectl\\kubernetes-cni\\kubelet四个组件。 dnf install - y kubeadm kubectl kubelet kubernetes-cni &amp;&amp; \\\nsystemctl start kubelet &amp;&amp; \\\nsystemctl enable kubelet\n 四个组件的用途分别是：\nkubeadm 用来进行所有节点的部署及初始化。\nkubectl 集群的cli接口。\nkubelet 每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理Pod和其中的容器。kubelet会在API Server上注册节点信息，定期向Master汇报节点资源使用情况。\nkubernetes-cni cni网络规范，通过cni的框架可以通过插件的方式对k8s集群间的网络进行管理，常见的是flannal。\n\n\n查看所有节点的时间是否一致for i in master, node1, node2;do echo $i &amp;&amp; date;done &amp;&amp; hostname &amp;&amp; date\n接下来就是不同节点了，部署的方式也不同，分开说明。\n\nMaster的初始化\n准备镜像,google镜像访问不到，所以我找到了一个使用阿里云的解决方案。可以先将镜像拉到本地，然后再对master节点进行初始化。镜像的版本是可以手动更改和指定的。\n # 1. 列出需要的镜像列表，记录下来。\nkubeadm config images list\n\n# 2. 建立一个bash脚本\nimages=(  \n  kube-apiserver:v1.12.1\n  kube-controller-manager:v1.12.1\n  kube-scheduler:v1.12.1\n  kube-proxy:v1.12.1\n  pause:3.1\n  \tetcd:3.2.24\ncoredns:1.2.2\n )\n\nfor imageName in $&#123;images[@]&#125; ; do\n  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\n  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName\n  docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName\ndone\n# 3. 运行脚本，等待镜像导入本地的docker存储中。\n\n查看节点的所有image已经正确的拉取到了本地即可\n [root@master ~]$ docker image ls\nREPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE\nk8s.gcr.io/kube-proxy                v1.15.0             d235b23c3570        4 weeks ago         82.4MB\nk8s.gcr.io/kube-apiserver            v1.15.0             201c7a840312        4 weeks ago         207MB\nk8s.gcr.io/kube-controller-manager   v1.15.0             8328bb49b652        4 weeks ago         159MB\nk8s.gcr.io/kube-scheduler            v1.15.0             2d3813851e87        4 weeks ago         81.1MB\nquay.io/coreos/flannel               v0.11.0-amd64       ff281650a721        5 months ago        52.6MB\nk8s.gcr.io/coredns                   1.3.1               eb516548c180        6 months ago        40.3MB\nk8s.gcr.io/etcd                      3.3.10              2c4adeb21b4f        7 months ago        258MB\nk8s.gcr.io/pause                     3.1                 da86e6ba6ca1        19 months ago       742kB\n\nkubeadm init命令进行集群初始化\nkubeadm init --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 \n# --pod-network-cidr 是指明了flannel的ip地址范围,这个是集群的Node网络，负责Node之间通信的网络。\n# --service-cidr=指明了容器建立时直接分配的ip地址，这个是集群的Pod网络，负责Pod之间通信的网络。\n\n使用命令查看集群节点的状态\n 这个信息不是刚部署完成的信息，只是举个例子展示一下效果。\n kubectl get pods -n kube-system 命令可以获取集群名称空间kube-system内的所有pod的信息； kubectl get nodes和kubectl get nodes -o wide 可以显示所有节点的信息；\n   [root@master ~]# kubectl get pods -n kube-system\n  NAME                                    READY   STATUS    RESTARTS   AGE\n  coredns-5c98db65d4-m5gj8                1/1     Running   12         3d3h\n  coredns-5c98db65d4-nmdzx                1/1     Running   12         3d3h\n  etcd-master.docker                      1/1     Running   2          3d3h\n  kube-apiserver-master.docker            1/1     Running   2          3d3h\n  kube-controller-manager-master.docker   1/1     Running   2          3d3h\n  kube-flannel-ds-amd64-fdcr9             1/1     Running   3          3d3h\n  kube-flannel-ds-amd64-hqwtr             1/1     Running   2          2d1h\n  kube-flannel-ds-amd64-lqtjc             1/1     Running   4          3d3h\n  kube-proxy-k8vcz                        1/1     Running   2          3d3h\n  kube-proxy-m6j4m                        1/1     Running   2          2d1h\n  kube-proxy-ww2z9                        1/1     Running   2          3d3h\n  kube-scheduler-master.docker            1/1     Running   2          3d3h\n  \n  [root@master ~]# kubectl get nodes\nNAME            STATUS   ROLES    AGE    VERSION\n  master.docker   Ready    master   3d3h   v1.15.0\n  node1.docker    Ready    &lt;none>   3d3h   v1.15.0\n  node2.docker    Ready    &lt;none>   2d1h   v1.15.1\n  \n  [root@master ~]# kubectl get nodes -o wide\n  NAME            STATUS   ROLES    AGE    VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE                     KERNEL-VERSION          CONTAINER-RUNTIME\n  master.docker   Ready    master   3d4h   v1.15.0   192.168.229.144   &lt;none>        Fedora 30 (Server Edition)   5.0.9-301.fc30.x86_64   docker://19.3.0\n  node1.docker    Ready    &lt;none>   3d3h   v1.15.0   192.168.229.145   &lt;none>        Fedora 30 (Server Edition)   5.0.9-301.fc30.x86_64   docker://19.3.0\n  node2.docker    Ready    &lt;none>   2d1h   v1.15.1   192.168.229.146   &lt;none>        Fedora 30 (Server Edition)   5.0.9-301.fc30.x86_64   docker://19.3.0\n\nNode节点加入集群在Node节点上，将节点加入集群，将需要的镜像导入。\n\nkubeadm join将节点加入到集群中，同时在master上注册节点的信息。 使用kubeadm join将节点注册到master上,给出master上生成的token，指定API Server的IP地址，指出ca证书的hash就可以将节点加入集群并初始化\n kubeadm join --token jj10e9.7robjkn1202au8a4 192.168.229.144:6443 --discovery-token-ca-cert-hash sha256:452775d08cca2523f36b1d41101856b3d90f469a39bf7ccdd5bf45150fbff94d\n使用docker save 在master节点上保存下来kube-proxy\\kube-pause\\flannel镜像\ndocker save &gt; .&#x2F;kube-proxy.tar \ndocker save &gt; .&#x2F;kube-pause.tar\ndocker save &gt; .&#x2F;flannel.tar\n使用docker load 在所有的node节点上导入kube-proxy\\kube-pause\\flannel镜像\ndocker load &lt; .&#x2F;kube-proxy.tar\ndocker load &lt; .&#x2F;kube-pause.tar\ndocker load &lt; .&#x2F;flannel.tar\n重启每个节点上的kubelet服务，重启之后会自动加载并启动相关的imagesystemctl restart kubelet.service\n\n生成新的节点token\n   # 生成新的token\t\n   # kubeadm token create token的有效期只有24h，超过时间就需要重新生成。\n[root@master ~]# kubeadm token create\njj10e9.7robjkn1202au8a4\n\n# 查看所有可用的token\n# kubeadm token list命令用来查看现在节点的链接需要的认证信息\n[root@master ~]# kubeadm token list\nTOKEN                     TTL       EXPIRES                     USAGES                   DESCRIPTION   EXTRA GROUPS\njj10e9.7robjkn1202au8a4   23h       2019-07-24T17:14:56+08:00   authentication,signing   &lt;none>        system:bootstrappers:kubeadm:default-node-token\n重新查看节点的ID\n # 获取ca证书sha256编码hash值\n[root@master ~]$ openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'\n452775d08cca2523f36b1d41101856b3d90f469a39bf7ccdd5bf45150fbff94d\n\n部署完成","categories":["Linux"],"tags":["Kubernetes"]},{"title":"面试记录","url":"/2019/07/18/Interview_2019/","content":"这个月还是发生了不少事情了，面试，换房子，真快。\n\n\n还是决定了换工作，工作这两年我知道自己做什么的时候速度快，我知道我做什么速度慢，我知道我喜欢什么不喜欢什么，两年的售前也让我意识到有些东西是性格和兴趣综合决定的结果，性格太难改变了，兴趣太难找了，现在我很高兴我知道自己的性格，不适合做售前，不适合去做销售，我骗不了别人也说服不了别人，我觉得挺好，这算是认识了自己，可能这个认识来的有些太晚了。兴趣这东西吧，我其实自己应该知道自己喜欢什么了，哈哈哈。我觉得我这么多年做了好多事情，我那天惊喜的发现自己还搭过热血江湖的服务器，现在还有客户端和服务器的package。两年对自己了解的差不多了，我应该可以确定我的目标了，剩下就是时间的问题了。最近发生的事情还是让我惊讶的，新来的同事是从医院出来的医生，产品基本上很快就玩起来了，最重要的是走到哪儿都有医院，有合作商想挖走。对比一下我自己就不是这样了，我还真的是没什么发展。其实说起来毕竟人家是五年的医科大学毕业，应该的。但是心里其实是不平衡，我也想凭我自己的本事，自己的知识变成这样，我可以不去，但是我一定有机会去。决定了换工作也是突然的决定，觉得自己还是应该给自己一个答案，我还是应该去试试，我在这个公司自己闭门造车也是造车，出去大公司造车也是造车，可能比我自己造车强吧，毕竟我遇到的事情就不一样了，问题也多了。我的专业是技能，我应该在这个那个领域继续走下去，我的价值大概应该在这里，所以我决定来了，我觉得我找到了自己的价值所在，就像同事的价值很明确，医学知识和人脉。我的价值其实在公司也很明确，去尝试不同的开源架构和解决方案，应付公司的目前产品的问题。面试对数据中心还是挺感兴趣的，但是是个应用运维岗位，我和面试官聊了，很直接，我想从运维进架构师，面试官和我说的是，走到架构去需要可以写的了代码，要有能力，我目前不行，差得好多。算是我自己的一个总结吧，两年还行。可能其他公司的岗位和行业更好一些吧。去个IT公司吧。\n\n面试题\n有一个字符串s&#x3D;“kjdfdsfevsdf”,使用python单独输出每个字符并在后面加上”th”.\n s = \"asdfaurhgauh\"\nfor i in s:\n  print(i+'th')\t\n数据库问题，增删改查。基本上都没答上来。select语句对数据库内容Where做一个筛选。\n\n提取b.txt中的所有域名，awk我写的grep。grep -E -o “www.[[:alpha:]]*.com” .&#x2F;b.txt | sort | uniq -c | sort -nrawk -F &#x2F; ‘{print $3}’ b.txt | sort | uniq -c | sort -nr我写的这个唯一个不好的地方就是不能匹配数字的部分，如果域名有数字就提取不出了。其实中间如果全部用正则也可以，但是正则会特别的长。\n\nnginx的反向代理配置文件是不是能看懂，考了一个upstream模块，考了一个weight的分配，考了一个 proxy_pass模块的调用。\n\n描述Raid0 ， 1 ，5的区别，但是没让说raid10。\n\n描述如果你有多的资源如何搭建高可用和高并发，这种题目其实还挺无聊的，高并发靠负载均衡分流，四层转发七层代理。高可用靠的冗余和故障的快速切换。没什么特别的架构，web服务也就是keepalived + lvs。硬件的话F5直接搞，虽然我没见过，但是也听说过。\n\n容器问了一个私有的register,不记得叫什么名字，但是我知道Docker官方有文档。软件名称是Harbor\n\nk8s没什么可说了，昨天晚上听了一堆的k8s的理论，一个集群简单而要三个网络，要不是设计思路清晰，网络就已经是一锅粥了。软件的架构真的是越来越复杂了，现在的网络里面不是桥桥桥，就是NAT转换。\n\n查看Linux系统CPU，MEM，NET-IO，DISK-IO的命令。top | htop | free -m | iostat -a\n\next4如果分区超过2T如何操作。这题估计本来是问Ext3文件系统的，改的。Ext4随便分了已经。\n\n如何理解top命令中的load average的含义 这个问题本来是聊到了服务器性能的观察，我说到了top命令查看系统的当前状况，所以后续有了这个问题，其实在系统中，这三个数值的计算方式是通过CPU进程队列的长度来进行计算的，具体的数值及计算公式网上有很多,等我不记得了我在去找资料吧，这种东西已经是死知识了，到处都是。 我说说我的理解，这三个数值其实是system load average,是系统的负载状态，其实是描述了系统的压力变化趋势，计算的CPU参数主要是Running Process &amp; uninterreptable Process , 其实就是正在运行和不可中断运行的进程，这些是CPU的工作量。还有一个衡量的参数实际上是IO，IO的指标也会体现在三个不同时间点的计算中。 如何查看或者分析是我一直理解的不透彻的点，记录一下。单项指标的观察不足以解决系统的问题，需要搭配其他的工具进行分析才可以。 看三个参数的变化趋势，1，5，15三个时间点：  \n\n如果1分钟高，但是后面两个都低，说明系统当前的状态是压力高的，但是是暂时的。\n如果是三个数值都高，可以说明可能是系统的性能不足或者有问题需要解决。\n如果1分钟的数值低，但是15分钟的数值高，说明系统的压力会慢慢平稳下来，不会持续太久。\n\n\nps命令的使用\n\nps aux \nTo see every process on the system using BSD syntax USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND\nroot         1  0.1  0.0 186736 10160 ?        Ss   09:32   0:03 &#x2F;sbin&#x2F;init\nroot         2  0.0  0.0      0     0 ?        S    09:32   0:00 [kthreadd]\nroot         3  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [rcu_gp]\nroot         4  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [rcu_par_gp]\nroot         6  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [kworker&#x2F;0:0H-kblockd]\nroot         8  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [mm_percpu_wq]\nroot         9  0.0  0.0      0     0 ?        S    09:32   0:00 [ksoftirqd&#x2F;0]\nroot        10  0.0  0.0      0     0 ?        I    09:32   0:00 [rcu_preempt]\nroot        11  0.0  0.0      0     0 ?        I    09:32   0:00 [rcu_sched]\nroot        12  0.0  0.0      0     0 ?        I    09:32   0:00 [rcu_bh]\n\n\nps axjf &#x2F; ps -ejH\nTo print a process tree  PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND\n1   555   555   555 ?           -1 Ssl      0   0:00 &#x2F;usr&#x2F;bin&#x2F;gdm\n555   585   555   555 ?           -1 Sl       0   0:00  \\_ gdm-session-worker [pam&#x2F;gdm-launch-environment]\n585   789   789   789 tty1       789 Ssl+   120   0:00  |   \\_ &#x2F;usr&#x2F;lib&#x2F;gdm-x-session gnome-session --autostart &#x2F;usr&#x2F;share&#x2F;gdm&#x2F;g\n789   791   789   789 tty1       789 Sl+    120   0:01  |       \\_ &#x2F;usr&#x2F;lib&#x2F;Xorg vt1 -displayfd 3 -auth &#x2F;run&#x2F;user&#x2F;120&#x2F;gdm&#x2F;Xautho\n791   848   789   789 tty1       789 S+       0   0:00  |       |   \\_ xf86-video-intel-backlight-helper intel_backlight\n789   939   789   789 tty1       789 Sl+    120   0:00  |       \\_ &#x2F;usr&#x2F;lib&#x2F;gnome-session-binary --autostart &#x2F;usr&#x2F;share&#x2F;gdm&#x2F;gree\n939   970   789   789 tty1       789 Sl+    120   0:04  |           \\_ &#x2F;usr&#x2F;bin&#x2F;gnome-shell\n970  1032  1032   789 tty1       789 Sl     120   0:00  |           |   \\_ ibus-daemon --xim --panel disable\n1032  1035  1032   789 tty1       789 Sl     120   0:00  |           |       \\_ &#x2F;usr&#x2F;lib&#x2F;ibus&#x2F;ibus-dconf\n1032  1200  1032   789 tty1       789 Sl     120   0:00  |           |       \\_ &#x2F;usr&#x2F;lib&#x2F;ibus&#x2F;ibus-engine-simple\n939  1086   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-rfkill\n939  1088   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-smartcard\n939  1089   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-clipboard\n939  1090   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-xsettings\n939  1091   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-housekeeping\n939  1092   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-mouse\n939  1093   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-power\n939  1094   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-screensaver-proxy\n939  1095   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-sound\n939  1099   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-color\n939  1101   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-keyboard\n939  1103   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-print-notifications\n939  1104   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-sharing\n939  1105   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-a11y-settings\n939  1106   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-wacom\n939  1107   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-datetime\n939  1108   789   789 tty1       789 Sl+    120   0:00  |           \\_ &#x2F;usr&#x2F;lib&#x2F;gsd-media-keys\n\n\n\n\nBuffer 和 Cache的区别 \n\nBuffer：A buffer is somthing that has yet to be “written” to disk.\nCache：A Cache is something that has been “read” from the disk and stored for later use.\n\nfree 或者 top 命令中的buffer 和 cache：  \n\nBuffer： 作为Buffer-cache的一部分内存，是块设备的读写缓冲区。\nCache： 作为Page-cache的一部分内存，是缓冲文件系统的cache。\nBuffer（缓冲区）是系统两端处理速度平衡（从长时间尺度上看）时使用的。它的引入是为了减小短期内突发I&#x2F;O的影响，起到流量整形的作用。比如生产者——消费者问题，他们产生和消耗资源的速度大体接近，加一个buffer可以抵消掉资源刚产生&#x2F;消耗时的突然变化。\nCache（缓存）则是系统两端处理速度不匹配时的一种折衷策略。因为CPU和memory之间的速度差异越来越大，所以人们充分利用数据的局部性（locality）特征，通过使用存储系统分级（memory hierarchy）的策略来减小这种差异带来的影响。答案来源链接:知乎\n\n\niptables的链表结构： 表：   \n\nFilter   \t\t\t一般的过滤功能  \nNAT  \t\t\tNAT功能的相关设置  \nMangle  \t\t\t用于对特定数据包的修改  \nRaw  \t\t\t一般设置不需要iptables过滤及监控的流量，提高性能使用。  \nRaw表的使用:iptables -t raw -A PREROUTING -d 1.2.3.4 -p tcp --dport 80 -j NOTRACK\niptables -t raw -A PREROUTING -s 1.2.3.4 -p tcp --sport 80 -j NOTRACK\niptables -A FORWARD -m state --state UNTRACKED -j ACCEPT\n\n\n\n\n\n 链：  \n\nPREROUTING 数据包进入路由表之前的Hook函数  \nINPUT 数据包经过路由表后进入本机的数据  \nFORWARD  \t数据包经过路由表后不是本机的数据  \n前面三个经过路由表的选择，后面两个不会再次经过路由表\n\n\nOUTPUT 数据包是本机产生的，向外发送\nPOSTROUTING 发送到网卡接口之前\n\n\n如何观察网卡的流量及tcpdump的使用\n\n\n","categories":[],"tags":["Personal"]},{"title":"Nginx反向代理笔记","url":"/2019/07/13/Linux/Linux_Nginx-Proxy/","content":"Nginx的反向代理笔记。\n\n\nNginx程序的主要功能\nload configuration\nlaunch workers\nnon-stop upgrade\n\n可以使用epoll单进程响应多个用户请求，如果是BSD可以使用kevent时间驱动模式响应。磁盘一侧使用的是高级IO中的sendfile机制，AIO异步IO，以及内存映射机制来完成硬盘IO的高级特性。\n\nNginx官方文档及参数说明在这里： nginx documentation\n\nSNAT &amp;&amp; DNAT主要是工作在三层&#x2F;四层的协议SNAT主要的功能是隐藏客户端，DNAT对服务器接受并转发请求。NAT功能无法触及7层协议的上面三层，所以无权控制上面的数据包内容，所以NAT不能对应用层的内容作出更改及缓存。只能对网络的内容及数据包进行直接的转发以及控制。\n正向代理 &amp;&amp; 反向代理正向代理是通过代理服务器对客户端发出的请求进行全部修改及转发；反向代理是通过代理服务器对发送到服务器的请求进行全部修改及转发；由于代理的服务器可以控制判定URL的资源内容，因此可以对站点进行动静分离处理。如果这个功能可以工作在应用层叫做代理，如果工作类似SNAT上的叫做正向代理，工作类似DNAT就叫做反向代理。\nNginx反向代理服务器\n面向客户端：该方向支持两种协议： http&#x2F;https\n面向服务端：该方向支持HTTP&#x2F;FastCGI&#x2F;memcache协议Nginx需要支持相关的协议需要有相关协议的对应模块原理以及流程：  从远程服务器取得数据进行nginx服务器本地的缓存，然后响应给客户端.可修改或具有修改意义的报文有两种：第一种是发到后面处理服务器的报文；第二种是发给客户端的响应报文。\n\nNginx的代理模块\nngx_http_proxy_module – Nginx的反向代理模块官方说明：Module ngx_http_proxy_module 添加新的反向代理服务配置文件： vim &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;proxy.conf\n server &#123;\n    listen 80；\n    server_name [SERVER_FQDN]\n    location &#x2F; &#123;\n        proxy_pass http:&#x2F;&#x2F;[IP_ADDR:PORT]\n        # 字段后加上&#x2F;表示将location后的目录映射为后端服务器的目录；\n        # 例如：如果是location &#x2F;test，那么在反向代理不加&#x2F;的时候，访问的是后端真实的&#x2F;test目录；\n        # 如果加上&#x2F;，访问到的是后端服务器的根目录；\n        # 当使用了正则表达式进行了匹配的时候后面不能添加&#x2F;符号；\n    &#125;\n\n    location ~* \\.&#123;jpg|jpeg|png&#125;$ &#123;\n        proxy_pass http:&#x2F;&#x2F;[IP_ADDR:PORT];\n        # 正则表达模式下部分在代理服务器的地址后面加上&#x2F;；\n    &#125;\n&#125;\nproxy_pass [RealServerAddress]；# 设置反向代理服务器的地址； proxy_set_header [field] [value]; # 设定传递到后端服务器的请求报文首部的值； proxy_set_header – Nginx的设定代理头文件参数模块的官方说明：proxy_set_header    \n proxy_set_header X-Real-IP $remote_addr;\n在设定字段后，更改后端真实服务器的日志记录的值，即可记录所有的真正的客户端IP；  \nproxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  \n\nproxy_http_headers_module # 设定发给客户端的响应报文的地址的值； Module ngx_http_headers_module – Nginx的客户端响应报文模块官方说明：  Module ngx_http_headers_module  \n add_header X_Via $server_addr;&#96; # 显示后端服务器的真实地址；\n\n","categories":["Linux"],"tags":["Nginx"]},{"title":"Linux启动流程简述","url":"/2019/07/13/Linux/Linux_Boot-sequence/","content":"记录Linux启动流程。\n\n\nLinux启动的简要流程Linux-MBR启动流程POST – GRUB(Bootloader-MBR) – Kernel – init\n详细描述及说明\n在触发开机通电之后，计算机读取BIOS中CMOS芯片的已经写好的程序进行主板设备的通电自检.\n在自检完成后将读取硬盘上的前512个字节，通过前面的446个字节载入grub的bootloader及硬盘相关驱动.\n同时引导grub进入stage1.5,stage1.5指向了定义在boot分区下的grub.cfg，及相关的grub图形文件.\ngrub的stage2进行了grub的菜单展示及内核选择的界面.\n通过grub的引导，计算机挂载内核，识别的根文件系统.\n启动init进程，通过SysV管理其他进程的启动及执行.\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"vim笔记","url":"/2019/07/08/Linux/Linux_VIM-note/","content":"vim的常用的命令速记。  \n\n可在~&#x2F;.vimrc中进行vim的的默认配置，echo ‘:set nu’ &gt; ~&#x2F;.vimrc即可设置vim默认显示行号。30分钟正则表达式入门教程\nvim一种模式化的编辑器，具有多种不同的模式。  \n\n编辑模式，命令模式\n插入模式\n末行模式 内置的命令行接口\n\nvim +12 test.sh\nvim +/PATTERN test.sh 打开自动定位到匹配模式的第一个结果的行首。\nvim + test.sh 直接出现在文件末尾  \n切换模式的说明i – 直接在当前光标的位置输入a – 在光标字符的后面输入o – 在光标下面直接新建一行，开始输入I – 在光标所在行的行首输入O – 在光标所在的上面直接新建一行，开始输入A – 在光所所在行的行尾输入\n编辑模式到末行模式 使用符号 ：:10，100d:set nu:set nonu:s&#x2F;dhcp&#x2F;static&#x2F;g\n关闭文件：    编辑模式下 连续ZZ，表示保存退出    :q 表示直接退出，类似的常用还有 :wq :wq! :q! :w! :w    :x 保存退出    :w [PATHTOFILE]\n光标调整:字符间hjkl 左 下 上 右 10l 右侧10个字符w 下一个单词的词首b 前一个单词的词首e 下一个单词的词尾\n行首行尾^ 调至行首第一个非空白字符0 调至行首$ 调至行尾\nHIJK 行间G 直接到最后一行句间段间\n","categories":["Linux"],"tags":["Linux"]},{"title":"Ansible笔记-2","url":"/2019/07/07/Linux/Linux_Ansible-Note-2/","content":"这份笔记介绍的是Ansible playbook的格式及相关的内容。\n\n\nAnsible笔记Ansible PlaybookYAML\nYAML（&#x2F;ˈjæməl&#x2F;，尾音类似 camel ) 是“YAML不是一种标记语言”的外语缩写（见前方参考资料原文内容）；但为了强调这种语言以数据做为中心，而不是以置标语言为重点，而用返璞词重新命名。它是一种直观的能够被电脑识别的数据序列化格式，是一个可读性高并且容易被人类阅读，容易和脚本语言交互，用来表达资料序列的编程语言。数据结构可以用类似大纲的缩排方式呈现，结构通过缩进来表示，连续的项目通过减号“-”来表示，map结构里面的key&#x2F;value对用冒号“:”来分隔。YAML文件一般的文件名为.yaml 或 .yml,文本结构举例如下：\n\nhouse:\n  family:\n    name: Doe\n    parents:\n      - John\n      - Jane\n    children:\n      - Paul\n      - Mark\n      - Simone\n  address:\n    number: 34\n    street: Main Street\n    city: Nowheretown\n    zipcode: 12345\nAnsible Playbook的关键字内容与命令的内容基本一致，有如下的几个关键字：\n\n- hosts  用来指定控制主机的范围,注意短横线后空格接字符\nremote_user 用来指定使用的用户\ntasks 可在字段下方缩进指定需要执行的任务,注意缩进\n- name 用来定义任务的名称或描述,注意短横线后空格接字符\nyum: name&#x3D;httpd state&#x3D;latest 定义使用的模块功能：后面跟操作参数\ntags: 对任务进行标记，可通过命令调用标记执行或排除某些任务\nwhen: 判断，满足when后面的条件才执行任务\nnotify: 触发handler的标志\n\n\nhandlers: 定义触发任务的内容\n- name: 任务的名称,注意短横线后空格接字符\nservice: name&#x3D;httpd state&#x3D;restarted 定义使用的模块：后面跟操作参数\n\n\n\nAnsible Playbook示例先看一个已经写好的playbook，针对写好的来解释playbook如何书写。\n---\t\t# 默认的开头\n- hosts: all \t\t# 先定义控制的范围，all表示所有主机，分组可定义在/etc/ansible/hosts文件中；\n  remote_user: root\t# 定义执行下面操作的用户，控制权限\n  tasks:\t\t# tasks字段负责定义任务\n  # 如果是Redhat系，执行安装httpd\n    - name: install httpd CentOS\n      yum: \n        name: httpd\n        state: latest\n      tags: install_httpd\n      when: ansible_os_family == \"RedHat\"\n  # 如果是Debain系，执行安装apache\n    - name: install httpd Ubuntu\n      apt:\n        name: apache\n        state: latest\n      when: ansible_os_family == \"Ubuntu\" \n      tags: install_httpd\n  # 执行网站主页的替换，如果变更触发handler字段定义的重启服务任务\n    - name: set the homepage\n      copy: \n        src: ./index.html\n        dest: /var/www/html/index.html\n      notify: \n        - restart_the_service\n  # 执行启动服务\n    - name: start httpd\n      service: \n        name: httpd\n        service: started\n  # 执行清空防火墙\n    - name: empty firewalld\n      shell: iptables -F\n  # 移除apache软件包\n    - name: remove httpd\n      yum: \n        name: httpd\n        state: absent\n      tags: remove_httpd\n  # 删除预设的apache网站文件\n    - name: clean stuff\n      file:\n        name: /var/www/html/\n        state: absent\n      tags: clean_stuff\n  #  handler触发后需要执行的任务\n  handlers:\n  # 重启httpd服务\n    - name: restart_the_service\n      service: \n        name: httpd\n        state: restarted\nAnsible playbook执行命令使用格式：\n  ansible-playbook [options] playbook.yml [playbook2 ...]\n命令作用：\n  Runs Ansible playbooks, executing the defined tasks on the targeted hosts.\n  # 运行ansible的playbook，在目标主机上执行已经定义好的任务。\n\n命令示例：\n[root@Hayden ~]$ ansible-playbook --syntax-check install_httpd.yaml\n# 对playbook进行语法检查\n[root@Hayden ~]$ ansible-playbook -C install_httpd.yaml\n# 对playbook进行运行测试，不改变结果，仅仅进行测试\n[root@Hayden ~]$ ansible-playbook install_httpd.yaml\n# 对playbook进行运行，并生成运行的结果\n[root@Hayden ~]$ ansible-playbook -t &quot;install_httpd&quot; install-httpd.yaml\n# 只运行具有“install_httpd”标签的任务\n[root@Hayden ~]$ ansible-playbook --skip-tags &quot;install_httpd&quot; install-httpd.yaml\n# 跳过install_httpd标签的任务\n[root@Hayden ~]$ ansible-playbook --skip-tags &quot;install_httpd clean_stuff&quot; install-httpd.yaml\n# 跳过多个标签的任务示例\n\n默认文件位置sudo pacman -Ql ansible | grep hosts 查看hosts文件的范例文件所在目录，其他文件操作类似.\n如果有不明白的命令可通过ansible-doc命令直接查看内置的说明文件，针对模块的ansible-doc -l 为列出所有可用模块\n","categories":["Linux"],"tags":["Ansible"]},{"title":"Ansible笔记-1","url":"/2019/06/26/Linux/Linux_Ansible-Note-1/","content":"Ansible的学习笔记。Ansible管理方式是资源在目标主机上，定义所期望的目标状态的方式；每一个操作必须是幂等的（可重复操作但结果不变的）。ansible采用ssh链接所管理的服务器，因此具有agentless的优势。\n\n\nAnsible的安装Ansible在Redhat的仓库中就有二进制包，直接dnf或yum安装就可以了。\n[root@localhost Liarlee]$ yum install -y Ansible\n\nAnsible的配置文件Ansible的配置文件常用的有：\n\n&#x2F;etc&#x2F;ansible&#x2F;ansible.cfg  Ansible的配置文件\n&#x2F;etc&#x2F;ansible&#x2F;hosts       Ansible允许控制的主机列表，可在hosts文件中对服务器进行分组\n\nAnsible的组件\nansible\nansible-playbook\nansible-doc\n\nAnsible的配置和使用Ansible命令模式\nansible [HOST_PARTTEN] -m [MODUELS] -a “[ARGS]” -f [Forks] -C -u [USERNAME] -c [CONNTECTION]\n\n基于密钥认证ansibleansible支持使用ssh用户命名密码的认证方式，也支持使用ssh密钥认证的方式进行链接。ssh密钥的方式可以保证安全性，同时免去输入密码的麻烦。\n\n生成ansible的密钥 [root@localhost Liarlee]$ ssh-keygen -t rsa -P \"\"\n复制ansible的公钥到需要连接控制的host上   [root@localhost Liarlee]$ ssh-copy-id -i ~/.ssh/id-rsa.pub root@[host1-IP]\n[root@localhost Liarlee]$ ssh-copy-id -i ~/.ssh/id-rsa.pub root@[host2-IP]\n在ansible的hosts文件中记录需要控制的主机名或者IP [root@localhost Liarlee]$ echo \"[host1-IP]\" >> /etc/ansible/hosts\n[root@localhost Liarlee]$ echo \"[host2-IP]\" >> /etc/ansible/hosts\n使用ansible进行控制测试 [root@localhost Liarlee]$ ansible all -m ping       # 进行连通测试\n[root@localhost Liarlee]$ ansible all -m ping --list-hosts      # 列举所有受影响的主机，但是不执行操作\n[root@localhost Liarlee]$ ansible all -m ping -C        # 进行测试，但是不对控制的主机作更改\n\nansible使用示例Ansible默认将所有的操作通过模块的方式定义，这里列举了一些常用的模块：\nansible管理查询命令使用ansible-doc命令来进行模块的文档查看, &#x2F;var&#x2F;log&#x2F;messages 中会记录操作日志。\n[root@localhost Liarlee]$ ansible-doc -l      # 列举所有当前可用的模块和简单说明\n[root@localhost Liarlee]$ ansible-doc -s [MODULES_NAME]     # 查看指定模块的使用方法和说明\nuser模块设定主机的用户状态，对用户进行创建删除，更改信息以及参数。\n# 设置所有主机创建用户user,设置内容有uid,groups,shell\n    [root@localhost Liarlee]$ ansible all -m user -a \"name=user1 uid=3000 state=present groups=testgrp shell=/bin/zsh\"\ngroup模块设置主机的组状态，对组状态进行编辑。\n# 控制所有主机创建组testgrp,设置内容有gid,非系统组\n    [root@localhost Liarlee]$ ansible all -m group -a \"gid=3000 name=testgrp state=present system=no\"\n# 控制所有主机删除testgrp\n    [root@localhost Liarlee]$ ansible all -m group -a \"gid=3000 name=testgrp state=absent\"\ncopy模块从本地复制内容到控制的所有主机,指定源地址和目的地址。\n# 复制本地/etc/fstab到所有主机的/tmp/fstab.ansible,同时设置权限为755\n    [root@localhost Liarlee]$ ansible all -m copy -a \"src=/etc/fstab dest=/tmp/fstab.ansible mode=755\"\n# 通过键盘输入的文本内容传输到目的文件中，文件可不存在，可同时设置文件的属主属组\n    [root@localhost Liarlee]$ ansible all -m copy -a \"content='hello,world\\n' dest=/tmp/test.txt owner=liarlee group=liarlee\"\n# 递归复制目录及其子文件到所有主机的/tmp/下\n    [root@localhost Liarlee]$ ansible all -m copy -a \"src=/etc/httpd dest=/tmp/\"\n# 复制目录下的所有文件到所有主机的/tmp/下，不创建对应的目录\n    [root@localhost Liarlee]$ ansible all -m copy -a \"src=/etc/httpd/ dest=/tmp/\"\n# 在所有主机的目录下新建一个空文件\n    [root@localhost Liarlee]$ ansible all -m copy -a \"content='' dest=/tmp/testfile\"\nfetch模块可从远程主机复制文件到本地。\n# 从某个主机复制文件到本地目录，文件不存在退出\n    [root@localhost Liarlee]$ ansible [HOST1] -m fetch -a \"src=/etc/fstab dest=/tmp/fstab.host1 fail-on-missing=yes\"\ncommand模块command模块不调用shell去解析命令，仅仅读取第一个命令进行简单执行,因此不支持管道传递参数。\n# 在所有主机上执行ifconfig\n    [root@localhost Liarlee]$ ansible all -m command -a \"ifconfig\"\nshell模块使用shell执行传递的命令，支持管道传递参数\n# 执行shell命令修改用户的密码\n    [root@localhost Liarlee]$ ansible all -m shell -a \"echo PASSWORD | passwd --stdin user1\"\nfile模块用于设定文件的状态以及属性\n# 在所有主机上建立目录\n    [root@localhost Liarlee]$ ansible all -m file -a \"path=/tmp/testdir state=directory\"\n# 在所有主机上对文件建立符号链接\n    [root@localhost Liarlee]$ ansible all -m file -a \"src=/tmp/testfile path=/tmp/testfile.link state=link\"\n# 在所有主机上设置文件或目录的权限\n    [root@localhost Liarlee]$ ansible all -m file -a \"path=/tmp/testfile mode=0755\"\ncron模块用于设置计划任务\n# 设置每三分钟运行一次同步时间的脚本\n    [root@localhost Liarlee]$ ansible all -m cron -a \"miniute=*/3 name=synctime job='usr/sbin/update 172.16.0.1 &amp;> /dev/null'state=present\"\n# 删除设置的计划任务\n    [root@localhost Liarlee]$ ansible all -m cron -a \"miniute=*/3 name=synctime job='usr/sbin/update 172.16.0.1 &amp;> /dev/null'state=absent\"\nyum模块用于调用yum进行软件包的安装卸载等，定义主机安装软件包的状态\n# 在所有主机安装nginx\n    [root@localhost Liarlee]$ ansible all -m yum -a \"name=nginx state=install\"\nserivce模块用于定义管理目标主机的服务状态\n# 在所有的主机上启动nginx服务\n    [root@localhost Liarlee]$ ansible all -m service -a \"name=nginx state=startd\"\n# 在所有的主机上设置nginx开机启动\n    [root@localhost Liarlee]$ ansible all -m service -a \"name=nginx enabled\"\n# 在所有主机上停止nginx服务\n    [root@localhost Liarlee]$ ansible all -m service -a \"name=nginx state=stoppd\"\n# 在所有的主机上重载nginx的配置文件\n    [root@localhost Liarlee]$ ansible all -m service -a \"name=nginx state=reloaded\"\n# 在所有主机上重启nginx服务\n    [root@localhost Liarlee]$ ansible all -m service -a \"name=nginx state=restarted\"\nscripts模块用于在所有主机上执行设置好的脚本\n# 在所有的主机上执行test.sh\n    [root@localhost Liarlee]$ ansible all -m scripts -a \"/tmp/test.sh\"","categories":["Linux"],"tags":["Ansible"]},{"title":"KVM中windows7虚拟机时间问题","url":"/2019/06/25/Linux/Linux_VM-timer/","content":"在KVM虚拟机中，安装Windows7的虚拟机，安装完成启动的时候发现虚拟机的时间与外部时间的速度不一致。记录问题的原因及解决方法。\n\n\n解决方式查看虚拟机的配置文件观察运行的效果类似于GBA模拟器的加速设定，动画速度变快了，windows7的系统时间也被加速了。  首先怀疑的是虚拟机的运行速度是不是被加速了，没有相关的设置。其次是查看配置文件中时间的相关定义，发现我的配置文件中，时间的定义是这样的：\n&lt;clock offset='localtime'>\n  &lt;timer name='rtc' tickpolicy='catchup'/>\n  &lt;timer name='pit' tickpolicy='delay'/>\n  &lt;timer name='hpet' present='no'/>\n  &lt;timer name='hypervclock' present='yes'/>\n&lt;/clock>\n怀疑这几个timer是有问题的，一般来说配置文件中只有简单的 clock offset&#x3D;’localtime’ 的字段其实就够用了。在删除了第一个timer之后，系统的时间和运行速度就正常了。\n在fedora-wiki找到的说明页面Libvirt_Managed_Timers页面链接在字段中‘rtc’并不是主要的问题，主要问题是后面的tickpolicy&#x3D;’catchcup’。fedora wiki给出的答案是catchup–Deliver at a higher rate to catch up.所以这就是为什么我们删除了这个timer之后系统正常的原因。\n","categories":["Linux"],"tags":["KVM"]},{"title":"IO重定向笔记","url":"/2019/06/25/Linux/Linux_IO-Redirect/","content":"输出重定向部分的复习笔记  \n  \n\n标准输入输出文件描述符的概念可以通过命令查看以及绑定文件描述符FD。  \nliarlee@hayden-pc ~\n&gt; $ ll &#x2F;proc&#x2F;$$&#x2F;fd\n总用量 0\ndr-x------ 2 liarlee liarlee  0  6月 24 20:22 .\ndr-xr-xr-x 9 liarlee liarlee  0  6月 24 20:22 ..\nlrwx------ 1 liarlee liarlee 64  6月 24 20:22 0 -&gt; &#x2F;dev&#x2F;pts&#x2F;0\nlrwx------ 1 liarlee liarlee 64  6月 24 20:22 1 -&gt; &#x2F;dev&#x2F;pts&#x2F;0\nlrwx------ 1 liarlee liarlee 64  6月 24 20:22 10 -&gt; &#x2F;dev&#x2F;pts&#x2F;0\nlrwx------ 1 liarlee liarlee 64  6月 24 20:22 2 -&gt; &#x2F;dev&#x2F;pts&#x2F;0\nLinux提供的I&#x2F;O设备Linux系统提供的IO设备，共有三种：\n\nSTDIN - 0 默认键盘输入\nSTDOUT - 1 默认输出信息到终端\nSTDERR - 2 默认输出错误信息到终端\n\nLinux输入输出重定向重定向说明STDOUT和STDERR可以被重定向到文件中，STDIN可通过读取文件实现输入重定向，重定向命令的基本格式如下：  \n[CMD] [lOPERATION_SYMBOL] [FILENAME]  \n命令    操作符号    文件名\n\n操作符号包括：\n&gt; 把STDOUT重定向到文件 \n2&gt; 把STDERR重定向到文件\n&amp;&gt; 把所有结果输出重定向到文件\n&gt;&gt; 在原有文件内容的基础上进行追加 \n&lt; 标准输入的重定向\n\n例子那么会有如下的操作：  \n    &gt; $ ls 1&gt; &#x2F;tmp&#x2F;echo.stdout  \n        # 正确的命令结果输出到文件中\n\n    &gt; $ ls 2&gt; &#x2F;tmp&#x2F;echo.stderr  \n        # 命令执行的错误结果输出到文件中  \n\n    &gt; $ ls &#x2F;error &#x2F;usr 2&gt;&amp;1 &gt;&#x2F;tmp&#x2F;right.test   \n        # 将错误信息重定向输出到屏幕显示，正确的信息输出到文件 \n        # 通俗的讲是-- 将STDERR(2)重定向为STDOUT(1)输出到屏幕上，\n        # 命令的其他结果输出到文件中  \n\n    &gt; $ ls &#x2F;error &#x2F;usr &gt;true 2&gt;false\n        # 将正确的信息输出到true中，把错误的信息输出到false中  \n\n    &gt; $ ls &#x2F;error &#x2F;usr &gt;all 2&gt;&amp;1 \n        # 先将标准输出重定向到文件all中，在将错误信息追加到标准输出中  \n        # 命令结果等于ls &#x2F;error &#x2F;usr &amp;&gt;all \n\n    &gt; $ ls &#x2F;error &#x2F;usr &amp;&gt; &#x2F;dev&#x2F;null\n        # 静默执行命令，不显示结果，不输出到终端\n\n    &gt; $ ls &#x2F;error &#x2F;usr 2&gt;&amp;1 &gt;&#x2F;dev&#x2F;null  \n        # 错误的信息显示在终端上，其他信息不显示\n\n#  某些命令可以使用管道将STDIN输入重定向作为命令的值\n    echo [PASSWD] | passwd --stdin [username] &amp;&gt; &#x2F;dev&#x2F;null\n        # 更改某个用户的用户名和密码\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"pyautogui自动脚本","url":"/2019/05/17/Python%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/Python_Script-ForGame/","content":"实在是肝不动了，自己写了个脚本帮我点点点。  \n\n\npyautogui说明pyautogui.position()  #  获取鼠标位置pyautogui.locateOnScreen()  #  对屏幕截图，获取图片文件所对应的屏幕坐标pyautogui.click()   #  模拟鼠标点击pyautogui.doubleclick()  #  模拟鼠标双击pyautogui.moveTo()  #  移动到屏幕坐标位置pyautogui.moveRel()     #  移动固定的坐标距离pyautogui.dragRel()     #  按住鼠标拖拽  \n代码如下：#!/usr/bin/python\n# -*- coding:UTF-8 -*-\n\nimport pyautogui\nimport time\nimport os\n\ndef MOTIONMOUSE(lines):\n    if lines &lt; 9:\n        # 检测是否有书本或食物\n        results_food = pyautogui.locateOnScreen('./food.png', grayscale=True)   # 检测食物图片是否存在\n        print('- 食物检测结果：', results_food)\n        results_book = pyautogui.locateOnScreen('./book.png', grayscale=True)   # 检测书籍图片是否存在\n        print('- 书本检测结果：', results_book)\n        pyautogui.click(1900, 60, duration=0.1) # 点击换线\n        if results_book is not None:    \n            pyautogui.click(1700, 325) # 点击学习\n            pyautogui.click(1600, 645) # 点击确定\n        elif results_food is not None:\n            pyautogui.click(1700, 325)  # 点击食用\n        else:\n            pass\n\n        pyautogui.moveTo(1700, 60, duration=0.1)    # 移动到对应一线的位置\n        pyautogui.moveRel(0, 65 * lines, duration=0.1)  # 移动到对应的线路位置\n        time.sleep(1)\n        pyautogui.click()   # 触发一次点击\n    else:\n        pyautogui.click(1900, 60, duration=0.3) #  \n        # 检测是否有书本或食物\n        results_food = pyautogui.locateOnScreen('./food.png', grayscale=True)\n        print('- 食物检测结果：', results_food)\n        results_book = pyautogui.locateOnScreen('./book.png', grayscale=True)\n        print('- 书本检测结果：', results_book)\n        if results_book is not None:\n            pyautogui.click(1700, 325)\n            pyautogui.click(1600, 645)\n        elif results_food is not None:\n            pyautogui.click(1700, 325)\n        else:\n            pass\n\n        for t in range(0, lines-8):\n            pyautogui.moveTo(1700, 90, duration=0.1)\n            pyautogui.dragRel(0, -65, duration=0.3)\n        if t &lt; 14:\n            time.sleep(3)\n        else:\n            pass\n\n        pyautogui.click(1700, 480, duration=0.1)\n\n# starting .....\n\nCount = 0\nreplace_times = 0\nEnergy = input('please input value of energy: ')        # 输入体力\nAll_lines = input('please input the number of lines: ') # 输入所有线路的数字\n\nfor turns in range(1, 999):\n    print('** 这是第 ' + str(turns) + ' 轮采集。')\n    for line in range (1, int(All_lines) + 1):\n        print('* 这是第 ' + str(line) + ' 条线路。')\n        clock = time.strftime('%H:%M:%S', time.localtime(time.time()))\n        print('* 开始时间 = ' + clock + ' ')\n        MOTIONMOUSE(int(line))\n        time.sleep(2)\n        pyautogui.click(1600, 770, duration=1.5)\n        pyautogui.moveTo(1310, 800)\n\n        #检测是否有树木\n        results_click = pyautogui.locateOnScreen('./level4_usable.png', grayscale=True)  # 计算斧子在不在\n        print('- 斧子检测结果：', results_click)\n        results_replace = pyautogui.locateOnScreen('./level4_replace.png', grayscale=True)  # 计算可替换的斧子在不在\n        print('- 更换检测结果：', results_replace)\n\n        # 判断是否有物品\n        if results_click is not None:  # 判断是否有按键，有等待，没有按键换线；\n            print('* 第 ' + str(Count) + ' 次采集。')\n            Count = Count +1\n            energy = int(Energy) - 15\n        elif results_replace is not None:   # 判断是否有可更换的斧子 \n            pyautogui.click(1510, 720, duration=0.2)\n            pyautogui.click(1310, 880, duration=0.2)\n            Count = Count + 1\n            replace_times = replace_times + 1\n            print('这是第 ' + str(Count) + ' 次采集。') \n            print('这是第 ' + str(replace_times) + ' 次换斧子。')\n            energy = int(Energy) - 15\n            time.sleep(4)\n        else:\n            print('-- 采集物或斧子未存在，跳过。') \n            pass            # 啥也没有，跳过\n    print('Starting Waiting for Refresh:')\n    time.sleep(200)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","categories":["Python"],"tags":["Python"]},{"title":"Archlinux安装过程记录","url":"/2019/05/17/Linux/Linux_Archlinux-Installation/","content":"自己尝试安装了archlinux在虚拟机里，记录安装过程，不过现在archlinux的WIKI是描述清晰的，直接查看和参考即可。  \n\n\npacman常用命令pacman命令的常用说明：  \n[root@LiarLee ~]# pacman -Sg gnome  查看gnome软件包组下面的所有软件包  \n[root@LiarLee ~]# pacman -Qe openssh  查询已经安装的软件包  \n[root@LiarLee ~]# pacman -Qs openssh  正则查询软件包\n[root@LiarLee ~]# pacman -Rs $(pacman -Qtdq) 递归删除孤立软件包\n[root@LiarLee ~]# pacman -Ss 关键字：在仓库中搜索含关键字的包\n[root@LiarLee ~]# pacman -Qs 关键字： 搜索已安装的包\n[root@LiarLee ~]# pacman -Qi 包名：查看有关包的详尽信息\n[root@LiarLee ~]# pacman -Ql 包名：列出该包的文件\n[root@LiarLee ~]# pacman -Syyu 下载已经更新本地的所有软件包\n\n安装Archlinux1. 获取Archlinux镜像从这个页面获取镜像： LINK HERE\n2. 启动到Archlinux Live环境在VMware中使用镜像文件直接引导Archlinux Live环境。  \n3. 设置Archlinux的键盘布局默认为US键盘布局，此处未作修改。    \n4. 连接到Internet使用ip link命令查看目前网卡的状态  \nroot@archiso ~ # ip link show\n  1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n  2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; dmtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link&#x2F;ether 00:0c:29:5e:ea:d5 brd ff:ff:ff:ff:ff:ff\n  \nroot@archiso ~ # ifconfig\n  ens33: flags&#x3D;4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt;  mtu 1500\n      inet 192.168.229.129  netmask 255.255.255.0  broadcast 192.168.229.255\n      inet6 fe80::9888:f57a:5c25:9cf9  prefixlen 64  scopeid 0x20&lt;link&gt;\n      inet6 fd15:4ba5:5a2b:1008:6e02:ff6:feeb:c250  prefixlen 64  scopeid 0x0&lt;global&gt;\n      ether 00:0c:29:5e:ea:d5  txqueuelen 1000  (Ethernet)\n      RX packets 199  bytes 18652 (18.2 KiB)\n      RX errors 0  dropped 0  overruns 0  frame 0\n      TX packets 186  bytes 23144 (22.6 KiB)\n      TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n\n  lo: flags&#x3D;73&lt;UP,LOOPBACK,RUNNING&gt;  mtu 65536\n      inet 127.0.0.1  netmask 255.0.0.0\n      inet6 ::1  prefixlen 128  scopeid 0x10&lt;host&gt;\n      loop  txqueuelen 1000  (Local Loopback)\n      RX packets 0  bytes 0 (0.0 B)\n      RX errors 0  dropped 0  overruns 0  frame 0\n      TX packets 0  bytes 0 (0.0 B)\n      TX errors 0  dropped 0 overruns 0  carrier 0  collisions 0\n    \nroot@archiso ~ # ping baidu.com   \n  PING baidu.com (123.125.114.144) 56(84) bytes of data.\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;1 ttl&#x3D;128 time&#x3D;14.8 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;2 ttl&#x3D;128 time&#x3D;13.6 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;3 ttl&#x3D;128 time&#x3D;17.1 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;4 ttl&#x3D;128 time&#x3D;14.1 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;5 ttl&#x3D;128 time&#x3D;13.9 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;6 ttl&#x3D;128 time&#x3D;14.7 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;7 ttl&#x3D;128 time&#x3D;13.5 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;8 ttl&#x3D;128 time&#x3D;16.2 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;9 ttl&#x3D;128 time&#x3D;16.9 ms\n  64 bytes from 123.125.114.144 (123.125.114.144): icmp_seq&#x3D;10 ttl&#x3D;128 time&#x3D;14.1 ms\n  ^C\n  --- baidu.com ping statistics ---\n  10 packets transmitted, 10 received, 0% packet loss, time 26ms\n  rtt min&#x2F;avg&#x2F;max&#x2F;mdev &#x3D; 13.543&#x2F;14.895&#x2F;17.105&#x2F;1.278 ms\n网络连接到此为止无需配置，可以正常访问网络。\n5. 更新系统时间，硬件时间同步使用命令查看系统时间以及硬件时间，将同步的当前时间写入硬件：  \nroot@archiso ~ # timedatectl status\n      Local time: Wed 2019-04-24 08:55:39 UTC\n    Universal time: Wed 2019-04-24 08:55:39 UTC\n        RTC time: Wed 2019-04-24 08:55:40\n        Time zone: UTC (UTC, +0000)\nSystem clock synchronized: yes\n      NTP service: active\n    RTC in local TZ: no\n\nroot@archiso ~ # timedatectl set-ntp true\n\n6. 建立系统的硬盘分区建立硬盘分区，我建立了两个分区，一个根分区和一个交换分区。  \nroot@archiso ~ # fdisk -l \n  Disk &#x2F;dev&#x2F;sda: 50 GiB, 53687091200 bytes, 104857600 sectors\n  Disk model: VMware Virtual S\n  Units: sectors of 1 * 512 &#x3D; 512 bytes\n  Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes\n  I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes\n  Disklabel type: dos\n  Disk identifier: 0xba00941d\n\n  Device     Boot   Start       End   Sectors  Size Id Type\n  &#x2F;dev&#x2F;sda1          2048   1050623   1048576  512M 82 Linux swap &#x2F; Solaris\n  &#x2F;dev&#x2F;sda2       1050624 104857599 103806976 49.5G 83 Linux\n\n\n  Disk &#x2F;dev&#x2F;loop0: 491.2 MiB, 515084288 bytes, 1006024 sectors\n  Units: sectors of 1 * 512 &#x3D; 512 bytes\n  Sector size (logical&#x2F;physical): 512 bytes &#x2F; 512 bytes\n  I&#x2F;O size (minimum&#x2F;optimal): 512 bytes &#x2F; 512 bytes\n7. 已经建立分区的格式化\n格式化ext4分区root@archiso ~ # mkfs.ext4 &#x2F;dev&#x2F;sda2\n格式化swap分区root@archiso ~ # mkswap &#x2F;dev&#x2F;sda1  # 将&#x2F;dev&#x2F;sda1格式化为swap分区\nroot@archiso ~ # swapon &#x2F;dev&#x2F;sda1  # 将&#x2F;dev&#x2F;sda1启用为swap分区\n\n8. 挂载分区使用系统挂载点&#x2F;mnt，将&#x2F;dev&#x2F;sda2作为系统的根目录挂载到&#x2F;mnt上。 \nroot@archiso ~ # mount &#x2F;dev&#x2F;sda2 &#x2F;mnt\n9. 定义安装所需的Mirrorlist10. 安装基础的Archlinux系统组件部署安装linux的文件系统，安装系统基础组建的软件包。  \n[root@LiarLee &#x2F;]# pacstrap &#x2F;mnt base\n11. 新系统的相关配置1. fstab根据自己建立的分区自动生成系统的分区表。    [root@LiarLee &#x2F;]# genfstab -U &#x2F;mnt &gt;&gt; &#x2F;mnt&#x2F;etc&#x2F;fstab\t\t# 生成新的fstab，写入安装硬盘的&#x2F;etc&#x2F;fstab  \n[root@LiarLee ~]# cat &#x2F;etc&#x2F;fstab\t\t#  查看新的分区表\n  # Static information about the filesystems.\n  # See fstab(5) for details.\n\n  # &lt;file system&gt; &lt;dir&gt; &lt;type&gt; &lt;options&gt; &lt;dump&gt; &lt;pass&gt;\n  # &#x2F;dev&#x2F;sda2\n  UUID&#x3D;3b61b364-b9eb-4e31-b65a-55fdcf09c614\t&#x2F;         \text4      \trw,relatime\t0 1\n\n  # &#x2F;dev&#x2F;sda1\n  UUID&#x3D;91987ac9-fa6b-4ac5-a9c5-014169dcf058\tnone      \tswap      \tdefaults  \t0 0\n2. chroot使用chroot命令切换根文件系统。  \n[root@LiarLee &#x2F;]# arch-chroot &#x2F;mnt\n3. timezonechroot命令相当于直接将当前的shell切换到了新安装的系统中。设置新的系统的时间及同步硬件时间。  \n[root@LiarLee &#x2F;]# ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime\n[root@LiarLee &#x2F;]# hwclock --systohc\n4. 本地设置5. 网络设置配置hostname以及本地解析hosts文件的解析条目。  \n[root@LiarLee &#x2F;]# vim &#x2F;etc&#x2F;hostname\n  YOUR_HOSTNAME.YOUR_DOMAIN\n\n[root@LiarLee &#x2F;]# vim &#x2F;etc&#x2F;hosts\n  127.0.0.1\tlocalhost\n  ::1\t\tlocalhost\n  IP_ADDRESS \tYOUR_HOSTNAME.YOUR_DOMAIN YOUR_HOSTNAME\n\n6. 生成initramfs为新的系统生成ramfs。ramfs的作用是，利用ramfs的文件系统，快速驱动周边的系统设备。在ramfs的使命结束后，自动chroot到新系统的文件系统中。  \n[root@LiarLee &#x2F;]# mkinitcpio -p linux\n7. 设置root密码修改root密码。  \n[root@LiarLee &#x2F;]# passwd \n8. 安装grub2这里pacman命令已经可以使用了，因此直接在这个安装grub2，这样才可以在后面正确的重启到新安装好的系统。  \n[root@LiarLee &#x2F;]# pacman -S grub2 \n[root@LiarLee &#x2F;]# grub-mkconfig \n12. 重启引导新系统将已经安装完成的那个根分区卸载，之后就可以重启了。  \n[root@LiarLee &#x2F;]# umount -R &#x2F;mnt  # umount 接触挂载新的根文件系统  \n[root@LiarLee &#x2F;]# fuser &#x2F;mnt\t\t# fuser 查看占用文件系统的进程，如果确认的话可以直接-k结束所有占用的进程\n[root@LiarLee &#x2F;]# reboot \t\t# 重启\n13. 安装图形化界面1. 安装Xorg:安装如下的所有组件，重启就可以进入图形化环境了。    1. 运行错误 xterm:command not found        [root@LiarLee &#x2F;]# pacman -S xterm zlib    1. 运行报错 twm:command not found        [root@LiarLee &#x2F;]# pacman -S xorg-twm xorg-xclock    1. 安装鼠标驱动        [root@LiarLee &#x2F;]# pacman -S xf86-input-mouse    1. 安装VMware图形驱动        [root@LiarLee &#x2F;]# pacman -S xf86-video-vmware    1. 安装Xorg服务本体        [root@LiarLee &#x2F;]# pacman -S xorg-server xorg-xinit    1. 安装Gnome桌面环境        [root@LiarLee &#x2F;]# pacman -S gnome    1. 安装GDM - Gnome桌面管理工具，也可用其他替换        [root@LiarLee &#x2F;]# pacman -S gdm\n2. 安装图形化之后测试启动速度systemd提供了命令检测启动的速度，包括内核启动速度以及用户空间的启动速度。\n\nsystemd-analyze  \nsystemd-analyze blame\n\n[root@LiarLee ~]# systemd-analyze \nStartup finished in 1.892s (kernel) + 1.182s (userspace) &#x3D; 3.075s \ngraphical.target reached after 1.182s in userspace\n3. 刷新国内的镜像源[root@LiarLee &#x2F;]# cd &#x2F;etc&#x2F;pacman.d&#x2F;\n[root@LiarLee pacman.d]#  mv mirrorlist mirrorlist.bak\n[root@LiarLee pacman.d]#  wget -O &#x2F;etc&#x2F;pacman.d&#x2F;mirrorlist https:&#x2F;&#x2F;www.archlinux.org&#x2F;mirrorlist&#x2F;?country&#x3D;CN\n[root@LiarLee pacman.d]#  mv mirrorlist mirrorlist.rank \n[root@LiarLee pacman.d]#  rankmirrors mirrorlist.rank &gt;&gt; mirrorlist\n[root@LiarLee pacman.d]# sed &#39;s&#x2F;^Server&#x2F;#Server&#x2F;g&#39; .&#x2F;mirrorlist &gt;&gt; .&#x2F;mirrorlist\n[root@LiarLee pacman.d]# s[root@LiarLee pacman.d]# spacman -Syyu\n14. 自行编译内核操作：[root@LiarLee &#x2F;]# pacman -S base-devel bc xmlto kmod inetutils\n[root@LiarLee &#x2F;]# useradd -d &#x2F;home&#x2F;admin -p admin admin  \n[root@LiarLee &#x2F;]# chown -R admin:admin &#x2F;home&#x2F;admin\n[root@LiarLee &#x2F;]# mkdir &#x2F;home&#x2F;admin&#x2F;mk-new-kernel&#x2F;\n[root@LiarLee &#x2F;]# su - admin \n[root@LiarLee &#x2F;]# cd &#x2F;home&#x2F;admin&#x2F;mk-new-kernel&#x2F;\n[root@LiarLee &#x2F;]# wget https:&#x2F;&#x2F;cdn.kernel.org&#x2F;pub&#x2F;linux&#x2F;kernel&#x2F;v5.x&#x2F;linux-5.0.8.tar.xz\n[root@LiarLee &#x2F;]# tar xvf linux-5.0.8.tar.xz \n[root@LiarLee &#x2F;]# cd linux-5.0.8&#x2F;\n[root@LiarLee &#x2F;]# make clean &amp;&amp; make mrproper\n[root@LiarLee &#x2F;]# make localmodconfig\n[root@LiarLee &#x2F;]# make nconfig\n[root@LiarLee &#x2F;]# make \n15. 一些问题\n重启之后没有DHCP获取IP地址的解决方法：\n\n    [root@LiarLee /]# systemctl enable dhcpcd\n    [root@LiarLee /]# systemctl start dhcpcd \n    [root@LiarLee /]# pacman -S net-tools \n```\t \n    \n\n","categories":["Linux"],"tags":["ArchLinux"]},{"title":"DockerFile笔记","url":"/2019/04/24/Linux/Linux_DockerFile-Note/","content":"Dockerfile的书写规则及Dockerfile的指令说明。\n\n\nDocker的镜像存储到Overlay2  \nDocker images ls\n    # 查看所有的Docker Images  \nDocker exec -it Container_Name &#x2F;bin&#x2F;sh  \n    # 将容器启动并执行shell命令行  \n\nDocker Images\nDocker Images中有启动容器所需要的文件系统及内容，用于启动并创建Docker容器,采用分层机制，最底层为bootfs，之上是rootfs  \nrootfs:Docker的根文件系统，由Kernel挂载为“ReadOnly”模式，而后通过联合挂载技术额外挂在一个可写层 \nbootfs:用于系统引导的文件系统，包括bootloader及kernel，容器启动之后自动卸载\n\n\nDocker Images Layer下层的镜像称为父镜像，最底层的叫做Base Images  \nAufs - Advanced multi-layered unification filesystem  \nOverlayfs - 3.18版本被合并到Linux内核   \nDocker的分层镜像，除了Aufs，还支持btrfs，devicemapper和vfs  \nDocker Registry - Docker daemon自动视图从DockerHub拉取镜像文件  \nDocker Registry的分类 \nSponsor Registry：第三方，提供给客户或Docker社区  \nMirror Registry：第三方，只给客户使用 \nVendor Registry：由发布Docker镜像的供应商提供  \nPrivate Regisry：通过设有防火墙及额外的安全层的私有实体提供\n\n\n云原生 - 面向云环境的运行了云系统本身的调用的程序。通过环境变量进行配置  \nWebhooks - 自动创建镜像  \nQuay.io 除了DockerHub其他的镜像仓库  \ndocker pull quay.io&#x2F;coreos&#x2F;flannel:latest\n\n\n\nDocker镜像的保存与恢复docker save -o myimages.gz IMAGE_NAME1 IMAGE_NAME2 \n    # 将多个镜像保存到本地压缩文件  \nscp myimages.gz \n    # 传输镜像到其他节点  \ndocker load -i myimages.gz  \n    # 在新的节点加载镜像  \nDocker FileDockerFile存在的意义docker exec CONTAINER –&gt; vi –&gt; RELOADDocker的容器不利于我们对需要反复调试的服务进行更改，通过Dockerfile的修改可快速调整容器的配置。\n自定义镜像的方法基于Docker容器制作镜像\n创建你需要的容器，Docker commit命令进行镜像的制作 \ndocker run –name b1 -it busybox  \nmkdir -p &#x2F;data&#x2F;html  \nvi &#x2F;data&#x2F;html&#x2F;index.html  \ndocker commit -p b1 \ndocker tag IMAGE_ID REPOSITORY:TAG  \ndocker image ls  \ndocker image rm IMAGE_TAG\ndocker imspect – cmd字段自动标志启动自动运行的命令  \n更改docker的默认运行命令  \ndocker commit -p -a ‘Liarlee’ -c ‘CMD [“&#x2F;bin&#x2F;httpd”,”-f”,”-h”,”&#x2F;data&#x2F;html”]’ b1 hayden&#x2F;httpd:v0.2 \ndocker login -u USERNAME 登录到服务器  \ndocker push Liarlee&#x2F;httpd\n\n基于DockerFile制作DockerImageDockerfile Format  \n\n# 开头的为注释文字   \nINSTRUCTION arguments，采用指令+参数的格式   \nDockerfile执行的时候是从上至下执行的  \n第一个非注释行必须是FROM指令\n\n.dockerignore文件路径记录，可以通配，打包时忽略list中的文件可以使用环境变量替换BASH SHELL:echo ${NAME:-tom} 给一个变量设置一个默认值echo ${NAME:+tom} 如果变量有数值，强行改为默认值   \n- FROMFROM指定的镜像将自动拉取作为底层的镜像进行制作;  \nFROM &lt;repository&gt;[:tag]  \nFROM &lt;repository&gt;@&lt;HASH number&gt;  \n\nEXAMPLE:   \nFROM centos:latest\n    # 使用Centos的最新发行镜像作为底层镜像\n\n- MAINTAINER提供制作人的信息，废弃不用了，现在使用LABEL  \nMAINTAINER &quot;LiarLee&lt;Test@LiarLee.com&gt;&quot;\n\n- LABELLABEL是给镜像指定元数据的命令  \nLABEL maintainer&#x3D;&quot;LiarLee&lt;Test@LiarLee.com&gt;&quot;\n\n\n- COPY复制本地文件或目录到镜像文件系统中。  \nCOPY \\&lt;src&gt; ... \\&lt;dest&gt;  \nCOPY [&quot;\\&lt;src&gt;&quot; ... &quot;\\&lt;dest&gt;&quot;]  \n\\&lt;src&gt; -- 相对路径    \n\\&lt;dest&gt; -- 绝对路径   \n\n- 指定的src目录，会将目录下的所有文件复制到目的地，但是不会将src复制。\n- 如果使用了多个src，或者src使用了通配，目的必须是个目录  \n- 如果dest不存在会被自动创建  \n\nCOPY &#x2F;etc&#x2F;passwd &#x2F;etc&#x2F;passwd\n    # 与cp命令相似，复制本地目标文件到容器文件系统中\nCOPY &#x2F;usr&#x2F;local&#x2F;src&#x2F;nginx&#x2F;* &#x2F;usr&#x2F;local&#x2F;src&#x2F;nginx&#x2F;\n    # 如果目标目录不存在需要先行创建\n\ndocker build命令[root@Hayden test]# docker build -h\nFlag shorthand -h has been deprecated, please use --help\nUsage:\tdocker build [OPTIONS] PATH | URL | -\nBuild an image from a Dockerfile\nOptions:\n      --add-host list           Add a custom host-to-IP mapping (host:ip)\n      --build-arg list          Set build-time variables\n      --cache-from strings      Images to consider as cache sources\n      --cgroup-parent string    Optional parent cgroup for the container\n      --compress                Compress the build context using gzip\n      --cpu-period int          Limit the CPU CFS (Completely Fair Scheduler) period\n      --cpu-quota int           Limit the CPU CFS (Completely Fair Scheduler) quota\n  -c, --cpu-shares int          CPU shares (relative weight)\n      --cpuset-cpus string      CPUs in which to allow execution (0-3, 0,1)\n      --cpuset-mems string      MEMs in which to allow execution (0-3, 0,1)\n      --disable-content-trust   Skip image verification (default true)\n  -f, --file string             Name of the Dockerfile (Default is 'PATH/Dockerfile')\n      --force-rm                Always remove intermediate containers\n      --iidfile string          Write the image ID to the file\n      --isolation string        Container isolation technology\n      --label list              Set metadata for an image\n  -m, --memory bytes            Memory limit\n      --memory-swap bytes       Swap limit equal to memory plus swap: '-1' to enable\n                                unlimited swap\n      --network string          Set the networking mode for the RUN instructions\n                                during build (default \"default\")\n      --no-cache                Do not use cache when building the image\n      --pull                    Always attempt to pull a newer version of the image\n  -q, --quiet                   Suppress the build output and print image ID on success\n      --rm                      Remove intermediate containers after a successful\n                                build (default true)\n      --security-opt strings    Security options\n      --shm-size bytes          Size of /dev/shm\n  -t, --tag list                Name and optionally a tag in the 'name:tag' format\n      --target string           Set the target build stage to build.\n      --ulimit ulimit           Ulimit options (default [])\n制作一个容器，COPY本地的HTML文件到容器文件系统中  \n[root@Hayden test]# docker build -t busybox-httpd:v0.1-1 .&#x2F;  \nSending build context to Docker daemon  3.072kB  \nStep 1&#x2F;3 : FROM busybox:latest  \n ---&gt; d8233ab899d4  \nStep 2&#x2F;3 : MAINTAINER &quot;HAYDEN&lt;HAYDEN@lee.com&gt;&quot;  \n ---&gt; Running in d581bd9c9aba  \nRemoving intermediate container d581bd9c9aba  \n ---&gt; 8137f8096ce4  \nStep 3&#x2F;3 : COPY index.html &#x2F;data&#x2F;web&#x2F;html&#x2F;  \n ---&gt; bced33a9e4a4\nSuccessfully built bced33a9e4a4\nSuccessfully tagged busybox-httpd:v0.1-1\n查看已经制作完成的镜像\n[root@Hayden test]# docker image ls\nREPOSITORY          TAG                 IMAGE ID            CREATED             SIZE\nbusybox-httpd       v0.1-1              bced33a9e4a4        17 seconds ago      1.2MB\nnginx               latest              881bd08c0b08        3 weeks ago         109MB\nbusybox             latest              d8233ab899d4        6 weeks ago         1.2MB\n运行容器查看结果\n[root@Hayden test]# docker run --name t1 --rm busybox-httpd:v0.1-1 cat &#x2F;data&#x2F;web&#x2F;html&#x2F;index.html\n&lt;h1&gt;Busybox httpd server.&lt;&#x2F;h1&gt;\n\n- ADD用于添加URL链接或本地文件到镜像中，支持tar包的自动解压。  \nADD &lt;src&gt; ... &lt;dest&gt;  \nADD [&quot;&lt;src&gt;&quot; ... &quot;&lt;dest&gt;&quot;]  \nADD命令可以支持URL，在打包镜像的时候下载并打包进去\nADD命令支持对本地的tar文件打包进镜像的时候进行解压  \n\nEXAMPLE:  \nADD nginx.org&#x2F;download&#x2F;nginx-1.15.12.tar.gz  \n    # 指令自动打包进入镜像，并同时解压tar包\n\n- WORKDIRWORKDIR &lt;dirpath&gt;  \n指定当前的工作目录\n\nEXAMPLE:\nWORKDIR &#x2F;usr&#x2F;local&#x2F;src&#x2F;nginx\n    # 制定后续操作的工作目录，以调整指令中的相对路径\n\n- VOLUMEVOLUME \\&lt;mountpoint&gt;  \nVOLUME [&quot;\\&lt;mountpoint&gt;&quot;]  \nVOLUME指定挂载的卷  \n只能设置容器中的卷目录，不能制定宿主机的目录，只能使用Docker自动管理的卷 \n\n- EXPOSEEXPOSE &lt;port&gt;[&#x2F;&lt;protocol&gt;]   \n指定的协议为tcp or udp , defaults option is TCP  \n可以一次指定多个端口  \n\n- ENVENV &lt;key&gt; &lt;value1&gt; &lt;value2&gt; &lt;value3&gt; &lt;value4&gt;   \nENV &lt;key&gt;&#x3D;&lt;value&gt; &lt;key&gt;&#x3D;&lt;value&gt;...  \n在Dockerfile中指定环境变量，可将指定的变量在Docker run的时候进行手动的指定，\n影响运行容器时候的命令执行结果，但是不影响docker build的运行结果  ","categories":["Linux"],"tags":["Docker"]},{"title":"Fedora开机启动速度的优化","url":"/2019/01/13/Linux/Linux_Fedora-bootspeedup/","content":"一直认为我的虚拟机性能不够所以导致自己的机器开桌面环境，开机慢慢慢慢慢慢…….今天终于发现了原因……是自己的傻(╯‵□′)╯︵┻━┻ ……\n\n\n过程使用systemd-analyze 命令使用systemd-analyze blame 命令罗列所有的启动的服务和耗时\n我的机器耗时间最长的是 \n\ndnf-makecache.service 占用了 1min 8.124s；  \nplymouth-quit-wait.service 占用了 1min 744ms\n\n重点来了所以关闭它，阻止今后开机的时候启动\nsystemctl disable dnf-makecache.service\nsystemctl disable dnf-makecache.timer\nOR\nsystemctl mask dnf-makecache.service\nsystemctl mask dnf-makecache.timer\nsystemctl mask plymouth-quit-wait.service\nsystemctl mask firewalld.service\n\n尽量不使用DHCP使用固定的IP可以提高启动速度，其他的不需要服务可以自行关闭即可  \n优化后的结果[root@localhost ~]# systemd-analyze \nStartup finished in 3.091s (kernel) + 1.669s (initrd) + 5.211s (userspace) &#x3D; 9.971s\n\n[root@localhost ~]# systemd-analyze blame\n          2.868s vmware-tools.service\n          1.317s lvm2-monitor.service\n          1.115s dev-mapper-fedora\\x2droot.device\n          1.044s fwupd.service\n           865ms NetworkManager-wait-online.service\n           834ms systemd-udev-settle.service\n           794ms dracut-initqueue.service\n           715ms udisks2.service\n           566ms sssd.service\n           533ms initrd-switch-root.service\n           443ms abrtd.service\n           404ms systemd-udev-trigger.service\n           326ms systemd-journal-flush.service\n           280ms ModemManager.service\n           259ms libvirtd.service\n           251ms polkit.service\n           209ms chronyd.service\n           199ms NetworkManager.service\n           192ms systemd-vconsole-setup.service\n           164ms accounts-daemon.service\n           149ms user@42.service\n           146ms systemd-udevd.service\n           143ms dracut-cmdline.service\n           137ms dracut-pre-pivot.service\n           137ms systemd-tmpfiles-setup-dev.service\n           134ms systemd-sysctl.service\n           131ms packagekit.service\n           124ms gssproxy.service\n           119ms fedora-readonly.service\n           116ms lvm2-pvscan@8:2.service\n           111ms user@0.service\n           110ms avahi-daemon.service\n           105ms auditd.service\n           105ms dmraid-activation.service\n           101ms gdm.service\n            86ms systemd-user-sessions.service\n            71ms fedora-import-state.service\n            65ms initrd-parse-etc.service\n            58ms upower.service\n            51ms systemd-logind.service\n            47ms var-lib-nfs-rpc_pipefs.mount\n            47ms systemd-fsck@dev-disk-by\\x2duuid-e37f7ce7\\x2d367b\\x2d4e28\\x2d8c1e\\x2d3b98d1e4d441.service\n            46ms systemd-journald.service\n            45ms systemd-tmpfiles-setup.service\n            44ms boot.mount\n            41ms home.mount\n            41ms wpa_supplicant.service\n            40ms dev-hugepages.mount\n            37ms nfs-config.service\n            35ms plymouth-read-write.service\n            33ms rpc-statd-notify.service\n            30ms systemd-remount-fs.service\n            30ms systemd-fsck@dev-mapper-fedora\\x2dhome.service\n            27ms geoclue.service\n            27ms cups.service\n            26ms colord.service\n            24ms livesys.service\n            23ms rtkit-daemon.service\n            22ms dracut-pre-udev.service\n            21ms dev-mqueue.mount\n            21ms dracut-shutdown.service\n            20ms sysroot.mount\n            20ms switcheroo-control.service\n            20ms plymouth-start.service\n            19ms plymouth-switch-root.service\n            19ms kmod-static-nodes.service\n            16ms dev-mapper-fedora\\x2dswap.swap\n            16ms initrd-cleanup.service\n            15ms systemd-fsck-root.service\n            13ms livesys-late.service\n            10ms sys-kernel-debug.mount\n             6ms systemd-update-utmp-runlevel.service\n             6ms initrd-udevadm-cleanup-db.service\n             6ms systemd-update-utmp.service\n             6ms systemd-random-seed.service\n             5ms tmp.mount\n             2ms sys-kernel-config.mount\n\nMark Tips\naxel  dnf可用的多线程更新\nyum-fastestmirror  自动挑选最快的服务器更新\n附加一个systemd的使用教程：- ClickThisLink    \n在附加一个plymouth的教程：- ClickThisLink\n\n","categories":["Linux"],"tags":["Fedora"]},{"title":"VMwareTools共享文件夹方案","url":"/2019/01/08/Linux/Linux_Fedora-InstallVMwareTools/","content":"升级了VMware Workstation 15, 迁移过来Fedora 27的虚拟机里面不能共享剪贴板，不能拖拽复制，共享文件夹设置之后不会挂载在&#x2F;mnt&#x2F;hgfs目录下，找到的解决方案如下：  \n\n\n解决共享剪贴板\nfedora 27下，直接安装  dnf update -y   \ndnf install -y open-vm-tools-desktop   \nOR  \ndnf install -y open-vm-*\n重启虚拟机即可\n\n解决挂载VMware共享文件不显示\n这个问题我的解决办法是： 卸载这个机器上的open-vm-tools所有包，安装VMware提供的Tools，才可以正常使用  \n那么步骤如下： dnf remove -y opem-vm-*\nreboot  \n------------\n在VMware中，选择安装VMwareTools，然后提示推荐使用Open-VM-Toools工具，输入YES，强制安装  \n一路回车，直到出现Enjoy.\nreboot\n重启之后可以在&#x2F;mnt&#x2F;hgfs目录下看见共享的目录了，完美  \n如果需要可以添加一条指令在&#x2F;etc&#x2F;fstab条目， &#96;&#96;&#96; vim &#x2F;etc&#x2F;fstab\n.host:&#x2F;SHAREDFOLDERNAME     &#x2F;mnt&#x2F;hgfs   vmhgfs   defaults    0 0\n\n\n收工\n\n","categories":["Linux"],"tags":["Fedora"]},{"title":"荣耀8-刷LineageOS14.1简述","url":"/2018/07/08/Android_Honor8InstallLineageOS/","content":"荣耀8刷LineageOS 14.1 , 感谢XDA的大神们。用到的刷机包我转存了一份到微云，链接在末尾。  \n  \n\n\n一 基本情况\n设备： 华为荣耀8 FRD-AL10 国行  \n初始系统版本： B396   EMUI 5.0.1   \n解锁状态： Phone Unlocked\n\n\n二 需要准备的原料\n手机  \nSD卡  \n电脑一台，USB线一根  \n荣耀8海外版安装包 Name:update.zip &amp; update_data_full_hw_usa.zip\nOpenKirin TWRP 3.1.1.1 Name:twrp-3.1.1-1-frd.img \nLineageOS ROM From XDA Name:lineage-14.1-20170812-Unofficial.zip\nOpenGAPPS Name:open_gapps-arm64-7.1-stock-20180705.zip \nSuperSU Name:SuperSU-v2.82-201705271822.zip\nDolby ATOM安装包\n\n\n三 流程1 解锁手机\n首先需要进行华为手机的解锁，百度一下有很多，所以不再过多的介绍。\n需要知道是我大华为要关闭后续的解锁服务了，所以今后即便是刷如果你没有解锁码也只能作罢。\n\n\n2 复制安装包到SD卡\n在SD卡上新建一个文件夹，名字叫做packages , 或者其他也可以。\n复制4-9项文件到SD卡\n将SD卡放入手机。\n\n\n3 写入TWRP\n解锁之后可以使用命令来写入TWRP Recovery。 Recovery使用的openkirin项目组的版本。请自行确保驱动及手机的连接处于正常状态即可。\n打开adb工具目录，在工具目录中使用Shift+鼠标右键，将文件夹在命令行或Powershell中打开。\nadb.exe devices   ——  ## 查看是否识别手机成功。\nadb.exe reboot fastboot ——— ## 将手机重启至fastboot模式. NOTE – 进入fastboot的方式，除了adb reboot fastboot之外还可以，关机，手机连接电脑，开机键加*音量-*，直到出现fastboot界面为止。\nfastboot.exe flash recovery .&#x2F;twrp-3.1.1.1-frd.img ——  ## 写入Recovery\nfastboot.exe reboot ——  ## 重启手机。\n\n\n4 安装海外版ROM\nXDA原帖 – HERE\n拔掉数据线，关机，长按 音量+ 和 开机 ， 直到出现 解锁警告 和 Your device is booting now …\n进入Recovery，一次进入Install – Select Storage – Micro SDCard – OK – Packages。\n点击update.zip – Swipt to confirm Flash. \n等待写入成功后会自动重启。\n自动进入eRecovery安装更新，这个是正常的，不用管他就好。\n安装更新之后会自动重启，这个时候TWRP会被覆盖，也就是说你的TWRP手机上已经没有了，被替换成了ROM里面华为默认的recovery，需要重新进入fastboot再写入一次TWRP。\n按照写入TWRP的步骤再来一次即可。 NOTE – 如果提示写入TWRP失败，需要再使用解锁码解锁一次手机，不要需要管fastboot界面的那个提示。我的手机当时显示的是已经解锁，但是其实是没有的，需要2次解锁手机才可以写入TWRP。\n再次进入TWRP之后，重复第2步，找到update_data_full_hw_usa.zip – Swipt to confirm Flash.\n自动写入成功后会自动重启，进入系统后查看关于手机，其中名称变为NRD90M。说明我们已经可以写入LineageOS了。\n\n\n5 卡刷LineageOS\nXDA原帖 – HERE\n我用的是US Model – FRD-L14C567 – B360 ，可以使用。\n重启手机，进入TWRP， 双清，格式化data分区。\n选择lineage-14.1-20170812-Unofficial.zip – Swipt to confirm Flash.\n等待写入成功之后重启。NOTE – 如果写入之后一直卡在开机动画，解决方法是进入TWRP – 格式化data分区即可。\n第一次开机的过程可能有些慢，但是只要第一进入了系统就可以了，后面不会再出现问题了。\n刷入LineageOS完成。\n\n\n6 安装SuperSU\nSuperSU官网 – HERE\n选择SuperSU-v2.82-201705271822.zip – Swipt to confirm Flash.\n进入系统就可以看到了，可以直接使用。\n如果不是用SuperSU，可以使用Magisk，好像这个好用一些。\n\n\n7 安装OpenGAPPS\nOpenGAPPS官网 – HERE\n进入TWRP， 选择open_gapps-arm64-7.1-stock-20180705.zip – Swipt to confirm Flash.\n重启之后就可以在系统里面看到了，我选择的这个版本是替换系统应用的版本，所以可以提供一个近乎于原生的体验。\n\n\n8 安装杜比ATOMS\nXDA原帖 – HERE\n选择dax_lemax2_v1.6.3.zip – Swipe to confirm Flash.NOTE – 帖子中给出了五个安装包，我基本上是尝试到第二个的时候成功了，所以我在这个把原帖放出来是说，可能这个需要不同的机型去尝试。\n重启可以在系统应用中直接看到。可以开启或者关闭。  \n可以选择使用Viper或者Dolby，看自己的喜好了。NOTE – 这种应用，我觉得有一个就够了，其实只是优化一下外放的效果。\n\n\n四 结尾只是不习惯如此多的ROM里面内置了很多我不使用的应用，还不能删除。其实还是那句话，不建议刷机，如果你知道自己在做什么。\n\n五 共享下载资源包括我所有文中使用到的安装包及文件，我上传到了微云，下载链接如下。NOTE – opengapps的安装包我替换成了pico，这样不需要翻墙，只有Google的框架和基础应用，也不会替换系统应用。下载地址 – HERE密码：9yn57o\n\n","categories":["Linux"],"tags":["Android"]},{"title":"Hexo备份和恢复","url":"/2018/07/03/Hexo_backup-restore/","content":"hexo项目的备份和还原方法。\n\n\n问题一个特别无奈的问题，我尝试在我写博客的虚拟机里面使用了一个破坏性的命令，dnf autoremove, 就是这个命令导致我的虚拟机彻底坏了。所以我恢复了快照，但是是两个月前的记录了。发现自己的blog无法恢复，找到了这样一个解决方案。\n思路\n在github上设置一个新的分支hexo。\n在这个分支上放置自己工作目录下的原始文件。\n每次推送Blog到github的时候同时推送自己的工作目录到github。\n当需要恢复自己的本地环境的时候，直接从github上面Clone下来就可以了。\n\nNote: 之前的思路是去hexo的工作目录下面找blog的项目，编译之后推送静态页面到gtihub。现在是直接把工作目录传上去，用的时候下载下来。  \n1. 准备一个新的工作目录在目录下git clone 自己的Blog项目。···git clone https://github.com/xxxx/xxxx.github.io.git···\n2. 目录的构建\n到xxxx.github.io目录下面，保留下面的.git目录，删除所有的其余目录。  \n将之前的hexo工作目录的所有文件复制到xxxx.github.io下。  \n在lxxxx.github.io目录下放置.gitignore文件，内容如下：  .DS_Store  \nThumbs.db  \ndb.json  \n*.log  \nnode_modules&#x2F;  \npublic&#x2F;  \n.deploy*&#x2F;   \ncd 到xxxx.liarlee.io目录下，使用命令新建分支：  git checkout -b hexo\n将hexo工作目提交到缓存  git add --all\n提交到github的hexo分支  git commit -m &quot;Some statement...&quot;\n推送到自己博客项目的hexo分支下    git push --set-upstream origin hexo\n结束\n\n3. 更新文章\ngit add –all\ngit commit -m “SaySomethingHERE”\ngit push origin hexo \nhexo clean &amp;&amp; hexo g -d\n\n4. 恢复hexo的工作目录\nsudo cnpm install -g hexo-cli\ndnf install -y npm\nsudo npm install -g cnpm –registry&#x3D;https://registry.npm.taobao.org\ncd Liarlee.github.io&#x2F;Liarlee\nsudo cnpm install -no-bin-links\nsudo cnpm install hexo-deployer-git\n去Github上面添加机器的ssh public key \n尝试使用hexo d , 查看是否可以成功。\n\n","categories":["Linux"],"tags":["Hexo"]},{"title":"RPM制作的笔记","url":"/2018/07/03/Linux/Linux_BuildRPMs/","content":"关于制作RPM包的笔记～\n\n\n一.制作RPM包教程源码包的制作教程基于RHEL 5 &amp; 6,当我写这个的时候还没有7版本,我会在后续更新新版本的路数.\n1. rpm包的制作流程简述\n放置源码进入SOURCES文件夹\n写好SPEC文件\n运行rpmbuild命令,自动执行安装和清理\n自动将所有的源码解压到BUILD目录\n自动安装所有的源码程序到BUILDROOT目录\n自动根据SPEC文件里面的file程序段打包到rpm包中\n自动进行后续的清理\n生成完整的RPM包\n手动进行安装测试\n\n2. 如何做准备需要明确的几个问题：\n\n我们需要做一个什么样的RPM包，这个RPM包使用来做什么的，RPMs不一定包含的是二进制的内容，不一定需要编译。\n至少我们需要源码，注意原材料的收集。\n官方建议使用干净的源码，如果有补丁需要在制作的过程中打上补丁。\n同一个软件，不同版本的RPM，新版本是否可以支持升级旧版本。需不需要清理旧版本的文件。升级是否会造成冲突。\n依赖关系。\n每一个PRM包都提供一种功能，Capability，可以被其他的PRM包依赖。RPM包的名字和所安装的文件都可以给其他的软件提供依赖。\n自身名字的意义，提供的每一文件也可以被依赖，\n他的安装和运行需要依赖于其他的RPM包本身或者所具有的文件，叫做依赖关系。  \n两类依赖关系，编译依赖和安装依赖。\n规划依赖关系，写SPEC文件。\n制作RPMs。\n简单测试RPMs。\n\n3. RPMs的规划\n是否是应用程序，是否需要补丁，是否需要新的功能。\n是一个程序的库文件\n是一个系统配置文件集\n是一个文档文件包\n是否拆分完整的软件，例如：MySQL-5.5.22.tar.gz，在制作RPM包的时候被拆分为MySQL，mysql-server，mysql-devel，等等。\n是一个二进制还是源码，当时都有。例如:src.rpm,里面包括了 source.tar.gz 和 spec,需要使用者安装完成之后编译再安装。\n\n4. 制作过程1. 设计目录结构(set up the directory structure)制作RPM过程中千万不能用root用户每个版本对于目录的要求不同，五个不同的目录：  \n\nBUILD：不需要放任何的内容，这个目录是真正工作的目录。用于解压编译源码。  \nRPMS：制作完成的RPM包放在这个里面，里面的目录的名字和结构与特定平台架构有关，可以交叉平台编译。  \nSOURCES:所有收集的源码都在这个目录里面。  \nSPECS:放置SPEC文件，作为制作过程的指导说明。以软件包的名字命名，以.spec结尾。  \nSRPMS:放置了SRC(source)格式的RPM包。红帽提供了默认的制作目录，在&#x2F;usr&#x2F;src&#x2F;redhat.[LiarLee@localhost ~] tree &#x2F;usr&#x2F;src&#x2F;redhat\n\n2. 放置文件到正确的指定的目录(Place the sources in the right dirctory)。  我们首先需要自己制定自己的制作源码目录，在不使用root用户的前提下进行制作,需要修改系统的宏，来制定新的工作目录。修改工作目录的过程如下：\n\n使用命令查看默认的宏：  [LiarLee@localhost ~] rpmbuild --showrc   \\\\ 显示所有的相关宏定义\n_build——表示目录; __rm——表示命令\n使用命令查看默认的配置文件：    [LiarLee@localhost ~] rpmbuild --showrc | grep macrofiles    \\\\ 显示配置文件的全局路径和文件名  \n\\\\ 权限由小到大，后一个文件的参数覆盖前面所有的定义  \n\\\\ 所以我们在家目录下创建隐藏文件.rpmmacros可以直接配置自定义的设置。\n使用命令查看默认的工作目录定义：    [LiarLee@localhost ~] rpmbuild --showrc | grep _topdir   \\\\ 显示默认的工作目录宏定义  \n\\\\ 以相同的模式在.rpmmacros中直接覆盖配置，可以更改工作目录  \n更改topdir的宏，使用rpmbuilder的用户，创建.rpmmacros,添加内容配置宏:    [LiarLee@localhost ~] vim .rpmmacros         \\\\ Create .rpmmacros file  \n    %_topdir    &#x2F;home&#x2F;rpmbuilder&#x2F;rpmbild&#x2F;  \n[LiarLee@localhost ~] mkdir -pv rpmbuild&#x2F;&#123;BUILD,RPMS,SOURCES,SPEC,SRPMS&#125;  \n[LiarLee@localhost ~] rpmbuild --showrc | grep _topdir      \\\\ Review the Result  \n\n3. 创建一个spec文件(Create a spec file that tells the rpmbuild command what to do)。\nspec文件使用软件的名字版本作为文件名；.spec作为扩展名。  \nrpm -qi mysql &amp; rpm -qpi mysql,命令查看rpm信息，信息从spec文件中定义，软件包信息说明段落定义。\nspec文件有如下几个段落：1. The introduction section设置软件包的基本信息Summary:  \\\\ 简单描述\nName:     \\\\ 软件名字\nVersion:  \\\\ 版本号\nRelease:  \\\\ 发行号\nLicense:  \\\\ 协议\nGroup:    \\\\ GROUP范围在这个文件中描述 &#x2F;usr&#x2F;share&#x2F;doc&#x2F;rpm-*&#x2F;GROUPS\nURL:      \\\\ 从何处获取的站点链接，下载路径\nPackager:  \\\\ 制作者&lt;制作者邮箱&gt;\nVendor：   \\\\ 制作者的公司或者本人名字      \nSource:      \\\\ 源文件地址，一个链接地址\nBuildRoot:   \\\\ 制作RPM包的时候的虚拟Root目录\nBuildRequires:   \\\\ 制作过程依赖于哪些软件包的名字1. The prep section解压源码包到BUILD目录的段，cd到需要的目录，设置环境变量。%prep\n%setup1. The build section这是源码包安装的make过程.%build\n.&#x2F;configure  OR   %configure\n.&#x2F;make       OR   %&#123;__make&#125;\nmake % &#123;?_smp_mflags&#125;    \\\\ 多对称处理器加速编译1. The install section这里是安装make install过程。系统中有install命令,install方式类似于COPY模式.%install\n%&#123;__rm&#125;\n%&#123;__make&#125; install DESTDIR&#x3D;&quot;%&#123;buildroot&#125;&quot;\n%find_lang %&#123;name&#125;1. The script section这里是定义执行需要的脚本，用来配置环境。例如:添加Apache用户. %pre        Note:安装前执行 %post       Note:安装后执行 %preun      Note:卸载前执行 %postun     Note:卸载后执行1. The clean section清理之前所用到的BuildRoot目录的。%clean\n%&#123;__rm&#125;1. The files section对安装的软件的程序进行规划,哪些文件安装到那个文件夹,BUILDROOT下的所有文件必须在这个段中存在%files\n%config(noreplace)  \\\\ 不替换旧的配置1. The changelog section记录版本迭代* Wed Apr 11 2012 Liarlee.site &lt;Liarlee@site.com&gt; - ReleaseNumber 更改时间\n- Comments\n- Comments\n\n4. 开始编译(Build the source and binary RPMs)\nrpmbuild命令说明:    rpmbuild -bp        \\\\ 执行到prep section\nrpmbuild -bc        \\\\ 执行到build section\nrpmbuild -bi        \\\\ 执行到install section\n\nrpmbuild -bs        \\\\ 制作源码格式的制作\nrpmbuild -bb        \\\\ 制作二进制格式的rpm包\nrpmbuild -ba        \\\\ 执行全部格式,BOTH二进制和源码\n\nrpmbuild -bl        \\\\ BUILDROOT存在但是没有在FILES段中为包含进去的文件的CHECK命令\n关于安装错误的说明:在执行过程中如果有报错我们只需要去按照提示修正错误即可,在执行结束之后会在RPMS目录下生成需要的RPM包和RPM-DEBUG包.我们只需要RPM包即可,使用rpm -ivh进行安装测试.[LiarLee@localhost ~] rpmbuild -ba SoftwareName.spec   \\\\开始制作的命令\n\nrpm2cpio命令的说明  src-rpm包只是将源码打包成RPM格式,当我们安装src.rpm格式的安装包的时候会把包含的文件,解压到用户默认的工作目录下,所以这种格式的RPM包我们不用安装,直接制作RPM包即可.进行rebuild OR recompile.  [LiarLee@localhost ~] rpm2cpio mysql.src.rpm &gt; mysql.cpio  \n[LiarLee@localhost ~] rpm2cpio mysql.src.rpm | cpio -t    两个网站的推荐(搜索SRC-RPM包的站点):    rpmfind.net    rpm.pbone.net  \n二. 从头开始写新的SPEC files制作RPM包的核心是写SPEC files，难以掌握的地方介绍SPEC文件的基本语法和简单用法\n1. Spec files overviewSPEC file里面都是指令，告诉RPMBuild命令如何一步一步解压，编译，做成不同的RPM包，依赖关系。  Macro是指的变量大多数的字段由tag+value组成,tag是标签–Directives,不区分大小写;value是区分大小写的.    \n1.1 宏的自定义用户自定义宏 : %define macro_name value引用方式 : %{macro_name} OR %macro_name  \n1.2 注释的方式使用#来进行注释%–不能在注释中使用,如果必须使用需要双写%%  \n%prep  \n\\#this is a comment for %%prep  \n\n2. Defining package infomation如何定义SPEC文件内的字段  \n2.1 软件包的信息\nName - 软件包名称 - 不能有短横线  \nVersion - 版本号 - 不能有短横线  \nRelease - 发行版本号  \nGroup - &#x2F;usr&#x2F;src&#x2F;doc&#x2F;rpm-version&#x2F;GROUPS文件中有详细的描述说明有哪些组可以使用\n\n2.2 制作方信息\nVendor - 公司或者组织制作的RPM  \nURL - 一个主页链接  \nPackager - 名字&lt;邮箱地址&gt;  \nLicense - 许可,GPLv2,….etc.\n\n2.3 描述信息\nSummary - 不能超过50个字符,短描述  \n%description section - 全面描述,如果字符过多可以提前换行.\n\n2.4 定义依赖关系\nRequires : Capability - 定义软件包的能力,如果未定义显示包名.  \nProvides : Capability - 定义对外提供的能力  \nBuildRequires : Capability - 可以出现多次,直接写出需要的软件包名\n\n2.5 设定Build目录\nbuild - 用于解压安装源码  \nbuildroot : ${_tmppath}&#x2F;%{name}-%{version}-root使用$RPM_BUILD_ROOT 或者 %{buildroot}\n\n2.6 命名Source文件使用Sources字段 和 patch字段,指定源文件和补丁  \n\nSource0: https:&#x2F;&#x2F; OR 相对路径, https不会下载, 自动本地寻址  \nSource1: sourcefiles_name  \nSource2: sourcefiles_name  \nPatch1: patchfiles_name  \nPatch2: patchfiles_name  \nPatch3: patchfiles_name补丁定义后可以直接使用patch命令进行补丁的安装,所以使用patch字段\n\n3. Controlling the build如何控制编译\n%prep                 \\\\ 把Source内的解压源码包到BUILD目录,cd到源码目录,配置环境  \n\n%setup -q             \\\\ 控制解压的流程  \n%setup -n name        \\\\ 目录名字  \n%setup -q             \\\\ 静默模式  \n%setup -a number      \\\\ AFTER,cd到目录之后解压缩     \n%setup -b number      \\\\ BEFORE,先解压之后cd到目录里  \n%setup -c             \\\\ 解压前创建目录  \n%setup -T             \\\\ 不展开 直接复制  \n\n%patch1               \\\\ 打补丁    \n%patch2               \\\\ 打第二个补丁        \n\n%build                \\\\ C类程序的configure &amp; make过程   \n.&#x2F;configure --prefix&#x3D;&#x2F;usr \\  \n  --sysconfdir&#x3D;&#x2F;etc&#x2F;nginx  \nmake %&#123;?_smp_mflags&#125;    \n\n%build                \\\\Perl的不同\nperl Mailfile PL  \nmake  \n\n%install                            \\\\ make install 过程  \nrm -rf %&#123;buildroot&#125;                 \\\\ Clean Stuff  \nmake install DESTDIR&#x3D;%&#123;buildroot&#125;             \\\\ make install 命令编译安装  \n%&#123;__install&#125; -p -D -m 0644 %&#123;SOURCE5&#125; \\     \n  %&#123;buildroot&#125;%&#123;_sysconfdir&#125;&#x2F;sysconfig&#x2F;%&#123;name&#125;            \\\\ install命令直接安装文件到目录  \n%&#123;__install&#125; -p -d -m 0755 %&#123;buildroot&#125;&#x2F;var&#x2F;log&#x2F;nginx     \\\\ install命令直接安装文件到指定的文件路径  \n\n%clean              \\\\ 清理BUILD目录,清空为下次做准备; rpmbuild --clean mysql.spec  \nrm -rf %&#123;buildroot&#125;     \\\\清理BUILD目录  \n\n%prep  \n%post  \n%prepun  \n%postun  \n使用IF可以对如下参数进行判断  \n$1            \\\\ 第一次安装  \n$2 OR $2+     \\\\ 升级  \n$0            \\\\ 卸载  \n4. Filling the list of files填充文件列表\n5. adding change log entries添加更新日志\nCentOS7打包Nginx过程记录\nuseradd rpmbuilder -p rpmbuilder\nyum install -y rpmdevtools rpmbuild\ncd &#x2F;home&#x2F;rpmbuilder&#x2F;\nrpmdev-setuptree\ncd .&#x2F;rpmbuilder\ntree .&#x2F;\nmv nginx-1.16.1.tar.gz .&#x2F;rpmbuild&#x2F;SOURCES&#x2F;\nvim .&#x2F;rpmbuilder&#x2F;SPECS&#x2F;nginx.spec\nvim nginx.service\nmv nginx.service &#x2F;rpmbuild&#x2F;SOURCES&#x2F;nginx.service  %define nginx_user nginx\n%define nginx_group nginx\n\nName:\t\tnginx\t\nVersion:\t1.16.1\nRelease:\t1%&#123;?dist&#125;\nSummary:\tmake rpm for nginx, version 1.16.1\n\nGroup:\t\tSystem Environment&#x2F;Daemons\nLicense:\tGPLv2\t\nURL:\t\thttp:&#x2F;&#x2F;liarlee.site\nSource0:\t%&#123;name&#125;-%&#123;version&#125;.tar.gz\nSource1:\tnginx.service\t\nBuildRoot: \t$_topdir&#x2F;BUILDROOT\n\nBuildRequires:\tgcc\nBuildRequires:\tgcc-c++\nBuildRequires:\topenssl\nBuildRequires:  openssl-devel\nBuildRequires:\tpcre\nBuildRequires:\tpcre-devel\nBuildRequires:\tsystemd\nRequires:\topenssl\nRequires:\topenssl-devel\nRequires:\tpcre\nRequires:\tpcre-devel\nRequires:\tsystemd\n\n%description\nFor online Ean portal, make by Hayden Lee, and take some personal option.\n\n%prep\n%setup -q\n\n\n%build\n.&#x2F;configure \\\n  --prefix&#x3D;&#x2F;data&#x2F;web-server&#x2F;nginx \\\n  --user&#x3D;%&#123;nginx_user&#125; \\\n  --group&#x3D;%&#123;nginx_group&#125; \\\n  --with-threads \\\n  --with-http_ssl_module \\\n  --with-http_stub_status_module \\\n  --with-http_realip_module \\\n  --with-http_gzip_static_module\nmake %&#123;?_smp_mflags&#125;\n\n\n%install\n%&#123;__rm&#125; -rf %&#123;buildroot&#125;\nmake install DESTDIR&#x3D;%&#123;buildroot&#125;\n# install systemd-specific files\n%&#123;__mkdir&#125; -p $RPM_BUILD_ROOT%&#123;_unitdir&#125;\n%&#123;__install&#125; -m644 %SOURCE1 \\\n    $RPM_BUILD_ROOT%&#123;_unitdir&#125;&#x2F;nginx.service\n\n%files\n%defattr(-,root,root)\n&#x2F;data&#x2F;web-server&#x2F;nginx&#x2F;\n%config(noreplace) &#x2F;data&#x2F;web-server&#x2F;nginx&#x2F;conf&#x2F;nginx.conf\n%config(noreplace) &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;nginx.service\n%attr(0644,root,root) &#x2F;data&#x2F;web-server&#x2F;nginx&#x2F;conf&#x2F;nginx.conf\n%pre\n    getent group %&#123;nginx_group&#125; &gt;&#x2F;dev&#x2F;null || groupadd -r %&#123;nginx_group&#125;\n    getent passwd %&#123;nginx_user&#125; &gt;&#x2F;dev&#x2F;null || \\\n        useradd -r -g %&#123;nginx_group&#125; -s &#x2F;sbin&#x2F;nologin \\\n        -d %&#123;nginx_home&#125; -c &quot;nginx user&quot;  %&#123;nginx_user&#125;\n    exit 0\n%post\n    &#x2F;usr&#x2F;bin&#x2F;systemctl preset nginx.service &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 ||:\n    &#x2F;usr&#x2F;bin&#x2F;systemctl preset nginx-debug.service &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 ||:\n    # print site info\n    cat &lt;&lt;BANNER\n    ----------------------------------------------------------------------\n\n    Thanks for using nginx!\n\n    Please find the official documentation for nginx here:\n    * http:&#x2F;&#x2F;nginx.org&#x2F;en&#x2F;docs&#x2F;\n    \n    Commercial subscriptions for nginx are available on:\n    * http:&#x2F;&#x2F;nginx.com&#x2F;products&#x2F;\n    \n    ----------------------------------------------------------------------\n    BANNER\n%preun\n    &#x2F;usr&#x2F;bin&#x2F;systemctl --no-reload disable nginx.service &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 ||:\n    &#x2F;usr&#x2F;bin&#x2F;systemctl stop nginx.service &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 ||:\n\n%postun\n    &#x2F;usr&#x2F;bin&#x2F;systemctl daemon-reload &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1 ||:\n    %&#123;__rm&#125; -rf &#x2F;data&#x2F;web-server&#x2F;nginx\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"Docker中运行DCM4CHEE-arc-light","url":"/2018/07/02/DCM4CHEE_RunDocker/","content":"在Docker中安装DCM4CHEE-arc-light项目。\n\n\n因为需要进行测试所以使用了DCM4CHEE， 但是DCM4CHEE现在的版本已经很古老了， 从而我接触了两个古老的PACS程序， 一个是Windows平台上有名的ClearCanvas, 还有一个就是DCM4CHEE。在安装的过程中遇到了很多的麻烦。 看到官方有把项目放在docker上， 所以决定直接使用。直接记录了所有的组件启动的方式和命令， 方便今后的查阅。\nDCM4CHEE-arc-light是目前比较新项目了，我这里写下了最小的安装模式， 基本上足够我日常测试使用了。\n这几个Docker容器是：\n\nDocker\ndocker网桥\nDAOCloud加速器\nOpenLDAP\nPostgreSQL数据库\nDCM4CHEE-arc-light本体\n\nDCM4CHEE_Docker_Command\nInstall Docker component:   \n dnf install -y docker  \n\nCreate the dcm4chee bridge network:  \n docker network create dcm4chee_default  \nDAOCloud加速器配置docker：\n curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;daotools&#x2F;set_mirror.sh | sh -s http:&#x2F;&#x2F;f1dac9f0.m.daocloud.io\n\nStart OpenLDAP Server: \n docker run --network&#x3D;dcm4chee_default --name ldap \\\n       -p 389:389 \\\n       -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime:ro \\\n       -v &#x2F;var&#x2F;local&#x2F;dcm4chee-arc&#x2F;ldap:&#x2F;var&#x2F;lib&#x2F;ldap \\\n       -v &#x2F;var&#x2F;local&#x2F;dcm4chee-arc&#x2F;slapd.d:&#x2F;etc&#x2F;ldap&#x2F;slapd.d \\\n       -d dcm4che&#x2F;slapd-dcm4chee:2.4.44-13.2  \n\nStart PostgreSQL:   \n docker run --network&#x3D;dcm4chee_default --name db \\\n       -p 5432:5432 \\\n       -e POSTGRES_DB&#x3D;pacsdb \\\n       -e POSTGRES_USER&#x3D;pacs \\\n       -e POSTGRES_PASSWORD&#x3D;pacs \\\n       -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime:ro \\\n       -v &#x2F;var&#x2F;local&#x2F;dcm4chee-arc&#x2F;db:&#x2F;var&#x2F;lib&#x2F;postgresql&#x2F;data \\\n       -d dcm4che&#x2F;postgres-dcm4chee:10.0-13  \n\nStart Wildfly With DCM4CHEE Archive 5:  \n docker run --network&#x3D;dcm4chee_default --name arc \\\n       -p 8080:8080 \\\n       -p 8443:8443 \\\n       -p 9990:9990 \\\n       -p 11112:11112 \\\n       -p 2575:2575 \\\n       -e POSTGRES_DB&#x3D;pacsdb \\\n       -e POSTGRES_USER&#x3D;pacs \\\n       -e POSTGRES_PASSWORD&#x3D;pacs \\\n       -e WILDFLY_WAIT_FOR&#x3D;&quot;ldap:389 db:5432&quot; \\\n       -v &#x2F;etc&#x2F;localtime:&#x2F;etc&#x2F;localtime:ro \\\n       -v &#x2F;var&#x2F;local&#x2F;dcm4chee-arc&#x2F;wildfly:&#x2F;opt&#x2F;wildfly&#x2F;standalone \\\n       -d dcm4che&#x2F;dcm4chee-arc-psql:5.13.2  \n\nStart the three applications use on command:  \n docker start ldap db arc  \n\nStop the three application:  \n docker stop ldap db arc  \n\n附加说明：Host 说明：dcm4chee_docker: 11.11.11.209/dcm4chee-arc/ui2\n\ngithub project DCM4CHEE-arc-light Note\nHL7 Relative Features\nWeasis Integrationhttps://github.com/dcm4che/dcm4chee-arc-light/wiki/Weasis-Viewer-Integration\n\n","categories":["Healthcare-IT"],"tags":["Docker"]},{"title":"安装Maven教程","url":"/2018/06/25/Linux/Linux_Install-Maven-Guide/","content":"安装Maven过程，备忘。\n\n\n下载安装包\nMaven Download Links  \n\n解压安装包：tar xzvf apache-maven-3.5.4-bin.tar.gz \n\n设置环境变量： vim &#x2F;etc&#x2F;profile export M2_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;apache-maven&#x2F;apache-maven-3.2.5 export M2&#x3D;$M2_HOME&#x2F;bin export MAVEN_OPTS&#x3D;-Xms256m -Xmx512m\n\n添加环境变量到PATH： export PATH&#x3D;M2:PATH\n\n\n","categories":["Linux"],"tags":["Linux"]},{"title":"配置Ip-forward参数","url":"/2018/06/23/Linux/Linux_IP-forward-active/","content":"配置Ip-forward参数备忘。\n\n\n1 Config the ip_forward option1.1 Config ip_forwarding in CentOS6vim /etc/sysctl.conf\n    net.ipv4.ipforward = 1\n\nsysctl -p /etc/sysctl.conf \n\n1.2 Config ip_forwarding In CentOS7vim /usr/lib/sysctl.d/00-system.conf \n    net.ipv4.ip_forward = 1 \n\nsysctl -p OR systemctl reboot\n\n2 Check the ip_forward status2.1 Check the ip_forward settingcat /proc/sys/net/ipv4/ip_forward  \n    net.ipv4.ip_forward = 1\nstatus 1   ---   forward is active \n\n","categories":["Linux"],"tags":["Fedora"]},{"title":"Docker基本命令说明","url":"/2018/05/28/Linux/Linux_DockerCommands/","content":"docker基础命令的说明。\n\n\nDocker常用命令的说明docker pull\\ 下载一个Image  \n[root@localhost ~]# docker pull nginx:lastest  \nUsing default tag: latest  \nTrying to pull repository docker.io&#x2F;library&#x2F;nginx ...   \nsha256:0fb320e2a1b1620b4905facb3447e3d84ad36da0b2c8aa8fe3a5a81d1187b884: Pulling from docker.io&#x2F;library&#x2F;nginx\nDigest: sha256:0fb320e2a1b1620b4905facb3447e3d84ad36da0b2c8aa8fe3a5a81d1187b884\nStatus: Image is up to date for docker.io&#x2F;nginx:latest\n\ndocker push\\ 上传一个Image  \ndocker run\\ 启动一个Container  \n[root@localhost ~]# docker run nginx\n\ndocker kill\\ 结束一个Container  \n[root@localhost ~]# docker kill flamboyant_thompson  \n\ndocker system prune\\ 清理Docker的无用文件,包括未使用的容器和不具有Dangling的镜像(不具有启动能力的Image)  \ndocker images\\ 列出所有的docker images\n[root@localhost ~]# docker images\nONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                    NAMES\n1cca834eb80c        nginx                      &quot;nginx -g &#39;daemon ...&quot;   57 seconds ago      Up 56 seconds       80&#x2F;tcp                   flamboyant_thompson\n769aefe85e29        achabill&#x2F;lh-toolkit        &quot;dockerize -wait t...&quot;   3 hours ago         Up 3 hours          0.0.0.0:8080-&gt;8080&#x2F;tcp   sad_payne\n93431e5f2f66        achabill&#x2F;lh-mysql:latest   &quot;docker-entrypoint...&quot;   3 hours ago         Up 3 hours          0.0.0.0:3308-&gt;3306&#x2F;tcp   brave_mcclintock\n\ndocker search\\ 搜索特定名称的image  \n[root@localhost ~]# docker search nginx\nINDEX        NAME                                                             DESCRIPTION                                     STARS     OFFICIAL   AUTOMATED\ndocker.io    docker.io&#x2F;nginx                                                  Official build of Nginx.                        8564      [OK]       \ndocker.io    docker.io&#x2F;nginx                                                  Official build of Nginx.                        8564      [OK]       \ndocker.io    docker.io&#x2F;jwilder&#x2F;nginx-proxy                                    Automated Nginx reverse proxy for docker c...   1334                 [OK]\ndocker.io    docker.io&#x2F;jwilder&#x2F;nginx-proxy                                    Automated Nginx reverse proxy for docker c...   1334                 [OK]\ndocker.io    docker.io&#x2F;richarvey&#x2F;nginx-php-fpm                                Container running Nginx + PHP-FPM capable ...   547                  [OK]\ndocker.io    docker.io&#x2F;richarvey&#x2F;nginx-php-fpm                                Container running Nginx + PHP-FPM capable ...   547                  [OK]\ndocker.io    docker.io&#x2F;jrcs&#x2F;letsencrypt-nginx-proxy-companion                 LetsEncrypt container to use with nginx as...   367                  [OK]\ndocker.io    docker.io&#x2F;jrcs&#x2F;letsencrypt-nginx-proxy-companion                 LetsEncrypt container to use with nginx as...   367                  [OK]\ndocker.io    docker.io&#x2F;kong                                                   Open-source Microservice &amp; API Management ...   187       [OK]       \ndocker.io    docker.io&#x2F;kong                                                   Open-source Microservice &amp; API Management ...   187       [OK]       \ndocker.io    docker.io&#x2F;webdevops&#x2F;php-nginx                                    Nginx with PHP-FPM                              103                  [OK]\ndocker.io    docker.io&#x2F;webdevops&#x2F;php-nginx                                    Nginx with PHP-FPM                              103                  [OK]\ndocker.io    docker.io&#x2F;kitematic&#x2F;hello-world-nginx                            A light-weight nginx container that demons...   98                   \ndocker.io    docker.io&#x2F;kitematic&#x2F;hello-world-nginx                            A light-weight nginx container that demons...   98                   \ndocker.io    docker.io&#x2F;bitnami&#x2F;nginx                                          Bitnami nginx Docker Image                      50                   [OK]\ndocker.io    docker.io&#x2F;bitnami&#x2F;nginx                                          Bitnami nginx Docker Image                      50                   [OK]\ndocker.io    docker.io&#x2F;zabbix&#x2F;zabbix-web-nginx-mysql                          Zabbix frontend based on Nginx web-server ...   50                   [OK]\ndocker.io    docker.io&#x2F;zabbix&#x2F;zabbix-web-nginx-mysql                          Zabbix frontend based on Nginx web-server ...   50                   [OK]\ndocker.io    docker.io&#x2F;1and1internet&#x2F;ubuntu-16-nginx-php-phpmyadmin-mysql-5   ubuntu-16-nginx-php-phpmyadmin-mysql-5          35                   [OK]\ndocker.io    docker.io&#x2F;1and1internet&#x2F;ubuntu-16-nginx-php-phpmyadmin-mysql-5   ubuntu-16-nginx-php-phpmyadmin-mysql-5          35                   [OK]\ndocker.io    docker.io&#x2F;linuxserver&#x2F;nginx                                      An Nginx container, brought to you by Linu...   35                   \ndocker.io    docker.io&#x2F;linuxserver&#x2F;nginx                                      An Nginx container, brought to you by Linu...   35                   \ndocker.io    docker.io&#x2F;tobi312&#x2F;rpi-nginx                                      NGINX on Raspberry Pi &#x2F; armhf                   19                   [OK]\ndocker.io    docker.io&#x2F;tobi312&#x2F;rpi-nginx                                      NGINX on Raspberry Pi &#x2F; armhf                   19                   [OK]\ndocker.io    docker.io&#x2F;nginxdemos&#x2F;nginx-ingress                               NGINX Ingress Controller for Kubernetes . ...   11                   \ndocker.io    docker.io&#x2F;nginxdemos&#x2F;nginx-ingress                               NGINX Ingress Controller for Kubernetes . ...   11                   \ndocker.io    docker.io&#x2F;blacklabelops&#x2F;nginx                                    Dockerized Nginx Reverse Proxy Server.          9                    [OK]\ndocker.io    docker.io&#x2F;blacklabelops&#x2F;nginx                                    Dockerized Nginx Reverse Proxy Server.          9                    [OK]\ndocker.io    docker.io&#x2F;wodby&#x2F;drupal-nginx                                     Nginx for Drupal container image                9                    [OK]\ndocker.io    docker.io&#x2F;wodby&#x2F;drupal-nginx                                     Nginx for Drupal container image                9                    [OK]\ndocker.io    docker.io&#x2F;webdevops&#x2F;nginx                                        Nginx container                                 8                    [OK]\ndocker.io    docker.io&#x2F;webdevops&#x2F;nginx                                        Nginx container                                 8                    [OK]\ndocker.io    docker.io&#x2F;centos&#x2F;nginx-18-centos7                                Platform for running nginx 1.8 or building...   6                    \ndocker.io    docker.io&#x2F;centos&#x2F;nginx-18-centos7                                Platform for running nginx 1.8 or building...   6                    \ndocker.io    docker.io&#x2F;nginxdemos&#x2F;hello                                       NGINX webserver that serves a simple page ...   6                    [OK]\ndocker.io    docker.io&#x2F;nginxdemos&#x2F;hello                                       NGINX webserver that serves a simple page ...   6                    [OK]\ndocker.io    docker.io&#x2F;1science&#x2F;nginx                                         Nginx Docker images that include Consul Te...   4                    [OK]\ndocker.io    docker.io&#x2F;1science&#x2F;nginx                                         Nginx Docker images that include Consul Te...   4                    [OK]\ndocker.io    docker.io&#x2F;centos&#x2F;nginx-112-centos7                               Platform for running nginx 1.12 or buildin...   3                    \ndocker.io    docker.io&#x2F;behance&#x2F;docker-nginx                                   Provides base OS, patches and stable nginx...   2                    [OK]\ndocker.io    docker.io&#x2F;behance&#x2F;docker-nginx                                   Provides base OS, patches and stable nginx...   2                    [OK]\ndocker.io    docker.io&#x2F;pebbletech&#x2F;nginx-proxy                                 nginx-proxy sets up a container running ng...   2                    [OK]\ndocker.io    docker.io&#x2F;pebbletech&#x2F;nginx-proxy                                 nginx-proxy sets up a container running ng...   2                    [OK]\ndocker.io    docker.io&#x2F;toccoag&#x2F;openshift-nginx                                Nginx reverse proxy for Nice running on sa...   1                    [OK]\ndocker.io    docker.io&#x2F;toccoag&#x2F;openshift-nginx                                Nginx reverse proxy for Nice running on sa...   1                    [OK]\ndocker.io    docker.io&#x2F;travix&#x2F;nginx                                           NGinx reverse proxy                             1                    [OK]\ndocker.io    docker.io&#x2F;travix&#x2F;nginx                                           NGinx reverse proxy                             1                    [OK]\ndocker.io    docker.io&#x2F;ansibleplaybookbundle&#x2F;nginx-apb                        An APB to deploy NGINX                          0                    [OK]\ndocker.io    docker.io&#x2F;mailu&#x2F;nginx                                            Mailu nginx frontend                            0                    [OK]\ndocker.io    docker.io&#x2F;mailu&#x2F;nginx                                            Mailu nginx frontend                            0                    [OK]\nredhat.com   registry.access.redhat.com&#x2F;3scale-amp20-beta&#x2F;apicast-gateway     3scale&#39;s API gateway (APIcast) is an OpenR...   0                    \nredhat.com   registry.access.redhat.com&#x2F;3scale-amp20&#x2F;apicast-gateway          3scale&#39;s API gateway (APIcast) is an OpenR...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhamp10&#x2F;apicast-gateway               3scale&#39;s API gateway (APIcast) is an OpenR...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhmap43&#x2F;wildcard-proxy                RHMAP Docker image that provides mapping a...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhmap44&#x2F;wildcard-proxy                RHMAP Docker image that provides mapping a...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhmap45&#x2F;wildcard-proxy                RHMAP image that provides mapping and prox...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhmap46&#x2F;wildcard-proxy                RHMAP image that provides mapping and prox...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhscl&#x2F;nginx-110-rhel7                 Nginx container image that delivers an ngi...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhscl&#x2F;nginx-112-rhel7                 Nginx is a web server and a reverse proxy ...   0                    \nredhat.com   registry.access.redhat.com&#x2F;rhscl&#x2F;nginx-16-rhel7                  Nginx 1.6 server and a reverse proxy server     0                    \nredhat.com   registry.access.redhat.com&#x2F;rhscl&#x2F;nginx-18-rhel7                  Nginx 1.8 server and a reverse proxy server     0                    \n\ndocker ps\\ 列出docker正在运行的Container  \n[root@localhost ~]# docker ps \nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                    NAMES\n1cca834eb80c        nginx                      &quot;nginx -g &#39;daemon ...&quot;   57 seconds ago      Up 56 seconds       80&#x2F;tcp                   flamboyant_thompson\n769aefe85e29        achabill&#x2F;lh-toolkit        &quot;dockerize -wait t...&quot;   3 hours ago         Up 3 hours          0.0.0.0:8080-&gt;8080&#x2F;tcp   sad_payne\n93431e5f2f66        achabill&#x2F;lh-mysql:latest   &quot;docker-entrypoint...&quot;   3 hours ago         Up 3 hours          0.0.0.0:3308-&gt;3306&#x2F;tcp   brave_mcclintock\n\ndocker container ls\\ 列出所有存在的Container,包括为运行的和未使用的  \n[root@localhost ~]# docker container ls\nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS              PORTS                    NAMES\n1cca834eb80c        nginx                      &quot;nginx -g &#39;daemon ...&quot;   57 seconds ago      Up 56 seconds       80&#x2F;tcp                   flamboyant_thompson\n769aefe85e29        achabill&#x2F;lh-toolkit        &quot;dockerize -wait t...&quot;   3 hours ago         Up 3 hours          0.0.0.0:8080-&gt;8080&#x2F;tcp   sad_payne\n93431e5f2f66        achabill&#x2F;lh-mysql:latest   &quot;docker-entrypoint...&quot;   3 hours ago         Up 3 hours          0.0.0.0:3308-&gt;3306&#x2F;tcp   brave_mcclintock\n\ndocker run -dt\\ 运行一个image,给予一个Terminal,放入后台,返回一个ContainerID  \n[root@localhost ~]# docker run -dt -p 80:80 nginx\n1cca834eb80cd8467dec6d103bf9072adfb55d8cfb2fdc257af548dc25917868\n\ndocker run -it\\ 运行一个image,给予一个Terminal,直接进入Container  \n[root@localhost ~]# docker run -it -p 80:80 nginx\n\ndocker run\\ 运行一个image,如果本地没有自动到docker仓库检索  \n[root@localhost ~]# docker run nginx\n\ndocker run -p\\ 运行一个image,指定port的映射关系  \n[root@localhost ~]# docker run -p 80:80 nginx \n\ndocker attach\\ 进入到某一个Container内部,如果没有shell,无法操作, 输出一片空白  \n[root@localhost ~]# docker attach flamboyant_thompson\n[root@d4a75f165ce6 &#x2F;]#","categories":["Linux"],"tags":["Docker"]},{"title":"Fedora防火墙关闭，如何开机运行脚本","url":"/2018/05/11/Linux/Linux_%E5%85%B3%E9%97%ADFedora%E9%98%B2%E7%81%AB%E5%A2%99/","content":"这次的内容是两个部分，关闭Firewalld 和 启动rc.local的服务。  \n\n如何彻底关闭Fedora27的Firewalld，防止每次重启自动开启。我们都知道在很早之前就已经关闭了rc.local的使用，开机启动的内容完全由systemd进行管理，如果要使用rc.local需要自己配置。\n如何彻底关闭Fedora27的FirewalldFedora的Firewalld,每次重启都会自动启动，不会彻底关闭，我们需要特殊的方式来关闭。  \n\n直接移除   dnf remove firewalld\n指向不存在的设备   systemctl mask firewalld  -- 表示直接将这个服务指向了&#x2F;dev&#x2F;null,无法启动也无法被其他的程序直接调用。\nsystemctl disable firewalld -- 开机的时候不会自动启动，但是接受其他的服务调用并启动。\n\n配置rc.local服务\n新建rc.local文件。   vim &#x2F;etc&#x2F;rc.d&#x2F;rc.local\n写入内容。   #!&#x2F;bin&#x2F;bash\n&#x2F;usr&#x2F;sbin&#x2F;Orthanc\n&#x2F;etc&#x2F;orthanc&#x2F;orthanc.json\n启动服务,配置开机自启。 systemctl start rc-local.service\nsystemctl enable rc-local.service\n\n","categories":["Linux"],"tags":["Fedora"]},{"title":"Orthanc的安装","url":"/2018/04/25/Linux/Linux_Orthanc_InstallationNote/","content":"记录了Orthanc的安装过程。只有简单的安装，复杂没研究，待补全。  \n\n\nOrthanc是一个开源的DICOM Server，支持RESTful API，是轻量级的DICOM Server，默认基于数据库SQLlite，同时也支持PostgreSQL。\n准备工作  平台：Fedora OR Windows  \n安装Orthanc快速部署Fedora集成的RPM包Fedora提供的BuildVersion下载到本地之后：执行  \ndnf install -y orthanc*\nsystemctl enable orthanc\nsystemctl start orthanc\niptables -A INPUT -p tcp --dport 8042 -j ACCEPT\niptables -A INPUT -p tcp --dport 4242 -j ACCEPT\niptables-save\n安装结束。  \nWindows一键安装包Windows安装包下载地址直接下载之后运行即可。\n插件部分","categories":["Healthcare-IT"],"tags":["Orthanc"]},{"title":"DISM的备份与还原","url":"/2018/04/19/Windows_DISM%E5%91%BD%E4%BB%A4/","content":"DISM的备份与还原主要是用了Win8之后微软同步发行的系统映像管理工具，全称是：部署映像服务和管理。  \n\n\nDISM命令行选项：\n/Append-Image      \n    进行映像的附加，更新，对比原有的文件内容进行增量更新。\n    *示例：*\n    对映像的增量更新\t Dism /Append-Image /ImageFile:install.wim /CaptureDir:D:\\ /Name:Drive-D\n\n/Apply-Image       \n    将映像应用于指定的驱动器。\n    *示例：*\n    单一wim文件的恢复\tDism /apply-image /imagefile:install.wim /index:1 /ApplyDir:D:\\\n    拆分多个映像文件的恢复\tDism /apply-image /imagefile:install.swm /swmfile:install*.swm /index:1 /applydir:D:\\\n\n/Capture-Image   \n    将某个驱动器的映像捕捉到新的 .wim 文件。捕捉的目录包括所有子文件夹和数据。不能捕捉空目录。目录必须至少包含一个文件。\n    *示例：*\n    生成.wim备份文件到当前目录下\tDism /Capture-Image /ImageFile:install.wim /CaptureDir:D:\\ /Name:Drive-D\n\n/Commit-Image\n    对已经装载的映像进行确认提交。\n    *示例：*\n    对已经挂载的镜像文件进行确认\tDism /Commit-Image /MountDir:C:\\test\\offline\n\n/Delete-Image\n    从包含多个映像卷的.wim文件中删除指定的映像。\n    *示例：*\n    删除指定的映像卷\tDism /Delete-Image /ImageFile:install.wim /Index:1\n\n/List-Image\n    显示指定卷映像中的文件和文件夹列表。\n    *示例：*\n    列出镜像中文件夹列表\tDism /List-Image /ImageFile:install.wim /Index:1\n\n/Split-Image\n    将现有的 .wim 文件拆分为多个只读的拆分 .wim 文件。\n    *示例：*\n    分割并指定分卷大小\tDism /Split-Image /ImageFile:install.wim /SWMFile:split.swm /FileSize:650\n\n/Mount-Image\n    将wim映像挂载到某个目录下\n    *示例：*\n    可读写模式\tDism /Mount-Image /ImageFile:C:\\test\\images\\myimage.wim /index:1 /MountDir:C:\\test\\offline\n    只读模式\tDism /Mount-Image /ImageFile:C:\\test\\images\\myimage.vhd /index:1 /MountDir:C:\\test\\offline /ReadOnly\n    只读更改为可读写\tDism /Remount-Image /MountDir:C:\\test\\offline\n\n\n常规处理流程：\n1.捕捉影像并且保存为.wim文件\n2.列出.wim .vhd .vhdx文件中的所有文件\n3.准备一个winPE\n4.进行备份镜像的还原\n\n","categories":["Windows"],"tags":["Windows"]},{"title":"Cacti的安装教程","url":"/2018/04/01/Linux/Linux_Cacti-installation/","content":"这次的内容是Cacti的安装。\n\n\n安装环境OS : CentOS 7 Server Everything – Minimal VersionRequired Packages : LAMP, RRDTool\n部署基础组件\n安装Apache命令如下：\nyum install -y httpd httpd-devel  \ndnf install -y httpd httpd-devel  \n安装完毕。  \n\n安装MySQL命令如下：\nyum install -y mysql mysql-server   \ndnf install -y mysql mysql-server  \nOR\n\n\nyum install -y Mariadb-server  \ndnf install -y Mariadb-server\n安装完毕。  \n\n安装PHP命令如下：  \nyum install -y php-mysql php-pear php-common php-gd php-devel php php-mbstring php-cli  \ndnf install -y php-mysql php-pear php-common php-gd php-devel php php-mbstring php-cli  \n安装完毕。  \n\n安装PHP-SNMP命令如下：\nyum install -y php-snmp  \ndnf install -y php-snmp  \n安装完毕。  \n\n安装NET-SNMP命令如下：\nyum install -y net-snmp-utils net-snmp-libs net-snmp   \ndnf install -y net-snmp-utils net-snmp-libs net-snmp  \n如果需要安装Spine，需要安装net-snmp-devel  \nyum install -y net-snmp-devel\ndnf install -y net-snmp-devel\n安装完毕。\n\n安装RRDTool命令如下：\nyum install -y rrdtool  \ndnf install -y rrdtool  \n安装完毕。  \n\n开始所有的服务：命令如下：\nservice httpd start  \nservice mysqld start OR service mariadb start\nservice snmpd start\nOR\nsystemctl start httpd.service\nsystemctl start mariadb.service\nsystemctl start snmpd.service\n安装完毕。  \n\n调整服务开机启动：\nchkconfig httpd on  \nchkconfig mysqld on\nchkconfig snmpd on\nOR\nsystemctl enable httpd.service\nsystemctl enable mariadb.service\nsystemctl enable snmpd.service\n安装完毕。\n\n开启EPEL REPO &amp; 安装Cacti\nwget http:&#x2F;&#x2F;dl.fedoraproject.org&#x2F;pub&#x2F;epel&#x2F;epel-release-latest-7.noarch.rpm  \nrpm -ivh epel-release-latest-7.noarch.rpm\nyum makecache\n安装完毕。所有的安装内容已经部署完毕。\n\n\n进行配置\n配置MySQL的root密码  \nmysqladmin -u root password YOURSELF_PASSWORD\n\n创建Cacti的数据库  \nmysql -u root -p\nmysql&gt; create database cacti;\nmysql&gt; GRANT ALL ON cacti.* TO cacti@localhost IDENTITFIED BY &#39;YOURSELF_PASSWORD&#39;;\nmysql&gt; FLUSH PRIVILEGES;\nmysql&gt; quit;\n安装完毕。\n\nMySQL-数据库初始化  \nrpm -ql cacti | grep cacti.sql\nmysql -u cacti -p cacti &lt; &#x2F;usr&#x2F;share&#x2F;doc&#x2F;cacti&#x2F;cacti.sql\n命令发起后需要进行密码的输入\n\nCacti-读取数据库配置\nvim &#x2F;etc&#x2F;cacti&#x2F;db.php\n需要编辑的内容如下：\n$database_type &#x3D; &quot;mysql&quot;;\n$database_default &#x3D; &quot;cacti&quot;;\n$database_hostname &#x3D; &quot;localhost&quot;;\n$database_username &#x3D; &quot;cacti&quot;;\n$database_password &#x3D; &quot;YOURSELF_PASSWORD&quot;;\n$database_port &#x3D; &quot;3306&quot;;\n$database_ssl &#x3D; false;\n\nFirewall-配置放行防火墙\niptables -A INPUT -p udp -m state --state NEW --dport 80 -j ACCEPT\niptables -A INPUT -p tcp -m state --state NEW --dport 80 -j ACCEPT\niptables save\nOR  \nfirewall-cmd --permanent --zone&#x3D;public --add-service&#x3D;http\nfirewall-cmd --reload\n\nApache- 配置Cacti根目录\nvim &#x2F;etc&#x2F;httpd&#x2F;conf.d&#x2F;cacti.conf\n配置文件内的内容修改如下：  \n&lt;Directory &#x2F;usr&#x2F;share&#x2F;cacti&#x2F;&gt;\nOrder Deny,Allow\nDeny from all\nAllow from all\n&lt;&#x2F;Directory&gt;\n重启服务：\nservice httpd restart\nOR\nsystemctl restart httpd.service\n\nCrond-配置Poller的计划任务  \nvim &#x2F;etc&#x2F;cron.d&#x2F;cacti\n*&#x2F;5 * * * *    cacti   &#x2F;usr&#x2F;bin&#x2F;php &#x2F;usr&#x2F;share&#x2F;cacti&#x2F;poller.php &gt; &#x2F;dev&#x2F;null 2&gt;&amp;1      \\\\ 去掉注释\n\n启动Broswer，进入Cacti安装界面，http://IPADDRESS/Cacti, 之后按照说明继续即可。\n\n\nDefault Username : adminDefault Password : admin\n补充安装SPINE过程\n下载源文件，安装程序。   \n安装CentOS,Development Tool，yum group install。   \n在Spine的目录下，执行.&#x2F;configure &amp;&amp; make &amp;&amp; make install。NOTE： 这里出现了的问题，提示找不到net-snmp header file—&gt;没有安装net-snmp-devel\n进入控制台进行Spine是否安装成功。\n\n待续","categories":["Linux"],"tags":["Cacti"]},{"title":"Fw:Fedora使用Lantern","url":"/2018/02/25/Linux/Linux_Fedora-InstallLantern/","content":"春节刷自己的笔记本，安装的双系统，Fedora和Windows。Lantern没有RPM安装包，本来是需要自己折腾编译什么的，后来发现了这个，省了好多时间，记下来…….原博链接  \n\n\n\n正文\n开启copr源sudo dnf copr enable yelanxin/Lantern\ndnf installsudo dnf install -y Lantern\n\n","categories":["Linux"],"tags":["Lantern"]},{"title":"Linux-电话面试","url":"/2018/01/29/Linux/Linux_%E7%94%B5%E8%AF%9D%E9%9D%A2%E8%AF%95/","content":"就在刚刚经历了电话面试，快速回忆问我的问题，记下：  \n\n\nlinux中top命令的用途？top命令中load参数的详细概念问题问的我当时没反应过来，说的是top命令中第一行的那个load average,一共有三个值，三个值分别显示了一分钟，五分钟，十五分钟的系统负载情况，一般不会超过1，超过5认为是超负荷运转。  \n邮件服务器使用的协议？使用SMTP协议，IMAP协议\n\n发送邮件的协议和端口号：SMTP协议端口号为：25和465\n接受邮件的协议和端口号：POP3协议端口号为：110和995IMAP协议端口号为：143和993\n\nwindows的故障转移集群是否用过？keepalived使用什么协议实现检测心跳？keepalived是以VRRP协议为实现基础的，VRRP全称Virtual Router Redundancy Protocol，即虚拟路由冗余协议。\nkeepalived主从服务器的选举细节？Keepalived是一个基于VRRP协议来实现的WEB 服务高可用方案，可以利用其来避免单点故障。一个WEB服务至少会有2台服务器运行Keepalived，一台为主服务器（MASTER），一台为备份服务器（BACKUP），但是对外表现为一个虚拟IP，主服务器会发送特定的消息给备份服务器，当备份服务器收不到这个消息的时候，即主服务器宕机的时候，备份服务器就会接管虚拟IP，继续提供服务，从而保证了高可用性。\nkeepalived原理？脑裂问题使用ping网关的方式不能完美解决？设置主从ping网关，如果没有ping通认为自己的网络出问题，重启服务。面试官提示我正确的处理方式应该是，如果ping不通应该主服务器重启服务，从服务器直接关闭自己的对外服务。\nLVS的负载均衡模式，和转发请求模式？目前有三种IP负载均衡技术（VS&#x2F;NAT、VS&#x2F;TUN和VS&#x2F;DR）八种调度算法（rr,wrr,lc,wlc,lblc,lblcr,dh,sh）。\nweb服务器如何将不同站点配置在同一个IP上？虚拟主机配置多个站点的区分方式？ip和端口\nfree命令中cache和buffer的区别？\nA buffer is something that has yet to be “written” to disk. A cache is something that has been “read” from the disk and stored for later use.  \n\n对于计算机来讲cache 和 buffer的区别：\n\nCache：高速缓存，是位于CPU与主内存间的一种容量较小但速度很高的存储器。由于CPU的速度远高于主内存，CPU直接从内存中存取数据要等待一定时间周期，Cache中保存着CPU刚用过或循环使用的一部分数据，当CPU再次使用该部分数据时可从Cache中直接调用，这样就减少了CPU的等待时间，提高了系统的效率。Cache又分为一级Cache（L1 Cache）和二级Cache（L2 Cache），L1 Cache集成在CPU内部，L2 Cache早期一般是焊在主板上，现在也都集成在CPU内部，常见的容量有256KB或512KB L2 Cache。\n\nBuffer：缓冲区，一个用于存储速度不同步的设备或优先级不同的设备之间传输数据的区域。通过缓冲区，可以使进程之间的相互等待变少，从而使从速度慢的设备读入数据时，速度快的设备的操作进程不发生间断。\n\n\n*Free中的buffer和cache：（它们都是占用内存）   \n\nbuffer: 作为buffer cache的内存，是块设备的读写缓冲区  \ncache: 作为page cache的内存， 文件系统的cache如果 cache 的值很大，说明cache住的文件数很多。如果频繁访问到的文件都能被cache住，那么磁盘的读IO 必会非常小。\n\nwindows中手动计算的内存使用值，和任务管理器中显示出来的值相差巨大？windows隐藏了类似linux中的cache和buffer，windows10在任务管理器中已经开始显示cache了\n","categories":["Linux"],"tags":["Linux"]},{"title":"Linux-进程管理笔记","url":"/2018/01/29/Linux/Linux_%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86%E7%AC%94%E8%AE%B0/","content":"进程的优先级说明\n优先级越高的进程就能够获得更多的CPU运行时间\n能够更优先的获得运行的机会\n\n优先级的调整\n用户可以调整的优先级(priority)范围是100-139每一个进程都有一个nice值, nice值是从-20到19，分别对应100-139\n\n可以调整进程自己的nice值，来调整优先级  \n\nnice值越小，优先级越高  \n默认情况下每个进程的优先级都是0\n\n如果有进程的优先级是3，就说明该进程降低了自己的优先级，普通用户只能调大nice值，管理员可以随意调整\n\n\nPID（process ID）:进程号init进程每一个进程都有父进程，除了init，init是所有进程的父进程init的进程号永远为1   \n其他进程进程的相关属性信息在&#x2F;proc目录下，每一个目录对应一个进程每一个进程的进程号都是唯一的，即使进程退出了，进程的号码一般不会被占用访问到的都是内核的映射。不是文件，是内核参数进程号文件夹中的文件记载了进程运行的命令，内存，使用的cpu等等  \n进程的分类\n与终端有关的进程\n与终端无关的进程\n\n进程状态表\n\n\n标识符\n描述\n\n\n\nD\n不可中断的睡眠\n\n\nR\n运行或就绪\n\n\nS\n可中断的睡眠\n\n\nT\n停止\n\n\nZ\n僵死\n\n\n&lt;\n高优先级的进程\n\n\nN\n低优先级的进程\n\n\n+\n前台进程组中的进程\n\n\nl\n表示多线程进程\n\n\ns\n会话进程的首进程(领导者进程)\n\n\n进程查看命令ps命令 显示进程状态，多种使用风格，unix的两大阵营，BSD风格（不加-），SYSV风格（加-)\nExample： ps -aux  \n参数说明-BSD风格a：BSD风格，显示所有与终端有关的进程，与x一起用显示所有u：能够显示详细的信息x：BSD风格，显示所有与终端无关的进程  \nps命令输出信息的说明单独查看Firefox进程的例子  \n[root@localhost Liarlee]# ps -aux | grep firefox  \nUSER        PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND  \nroot       1915  5.7  6.9 9324108 570096 tty2   Sl+  11:23   2:00 &#x2F;usr&#x2F;lib64&#x2F;firefox&#x2F;firefox  \n\n说明表格  \n\n\n\nUSER\nPID\n%CPU\n%MEM\nRSS\nVSZ\nTTY\nSTART\nSTAT\nTIME\nCOMMAND\n\n\n\n进程发起者\n进程号\n使用cpu百分比\n使用内存的百分比\n常驻内存集\n虚拟内存集\n控制台关联\n启动时间\n状态\n运行时间\n哪一个命令启动的进程\n\n\n[]表示是一个内核线程，不是一个用户进程\n\n\n\n\n\n\n\n\n\n\n\n\n另一个风格-SYSV风格-e SYSV风格，显示所有的进程-F 额外的完全格式，显示更多的字段-l 长格式  \nPRI：优先级NI：nice值SZ：sizePSR：正在运行在那个CPU  \nps -o 指定显示的字段,ps 不加参数只显示前台进程，一般使用ps -axo例：ps -axo comm,pid,niman文档中有单独的说明-o可以指定那些字段  \npstree进程树pgrep  查找符合某种特性的进程    例： pgrep bash   pgrep -u root bash  以root用户为属主运行的bash进程   只显示进程号，不显示进程的名称   -u euid        有效ID   -U uid         进程发起者ID  \npidof     查看某一个进程名字对应得进程号   pidof init    |   pidof sshd    |    pidof crond\ntop关于top显示信息的说明   当前系统时间 ， up运行时长，登陆用户，平均负载（平均队列长度，队列长度越小cpu负载越低）。\n   所有的进程数量，正在运行的进程数量，多少个睡眠进程，多少个僵尸进程\n   每个cpu负载，默认是多颗cpu平均负载，数字1，表示展开查看。\n       us ： 用户空间占用的\n       sy： 运行在系统占据了多少\n       ni：nice值影响的cpu比例\n       id： 空闲的百分比\n       wa： 等待IO完成的百分比，占据的时间\n       hi ：中断占据的百分比\n       si ： 软中断的时间\n       st ： 被偷走的时间\n   内存的使用\n   交换的使用\n   默认使用cpu百分比排序\n\ntop交互式的子命令   M：使用内存百分比排序\n   P： 使用cpu百分比排序\n   T： 占用cpu时间排序\n\n   l：不显示平均负载启动时间  \n   t：不显示进程和cpu负载   \n   m：不显示关于内存和交换的信息\n\n   c：  是否显示完整的命令行\n   q： 退出top\n   k：  杀死某个进程\n\n更改top命令的选项：   -d    更改刷新是时常\n   -b   批处理模式   会刷新一次，刷新出一屏\n   -n  3 ： 批处理模式只显示多少次\n\n进程间通信：进程之间彼此不能感受到彼此的存在，因此进程间的通信有几种机制：  \nIPC进程间通信，unix中非常常用的机制，inter process Communication               共享内存               信号：signal               semaphore  \n手动控制进程间通信： kill       kill -l  显示所有的kill信号\n       1 sighup        让一个进程不重启，重读配置文件，让文件的新配置生效\n       2 sigint          中断一个进程，ctrl c 就是发送了 2 信号\n       9 sigkill            杀死一个进程  ， 强行关闭  ， 没有任何余地\n       15 sigterm        中止一个进程\n\n指定发送信号：   kill -1   kill -sigkill   kill -kill\n杀死所有某个名字的进程：killall  command\n调整运行中进程的优先级     renice    进程优先级   进程号\n       renice   3   3704\n\nnice -n nice值 命令\n其他vmstat   vmstat 1   每一秒显示一次\npkill 类似于 pgrep\nbg  把前台的作业放入后台   bg 【%】jobid  可以指定执行某一个作业  默认执行+ 作业jobs  查看后台的所有作业  作业号不同于进程号   如果进程上有+ 号 说明接下来将要继续操作的作业    如果进程上有-好  说明执行完+的作业之后再执行-号作业  \nfg 将后台的作业放入前台 ，用法类似于bg  \n如果杀死某些作业 ， kill  %2   杀死作业2  \n","categories":["Linux"],"tags":["Linux"]},{"title":"Nginx编译安装","url":"/2018/01/22/Linux/Linux_Nginx-InstalltionNote/","content":"很久之前的笔记了，发出来，做编译安装Nginx的时候记录的……包括安装命令和脚本……\n\n\n安装nginx服务器\n下载zlib，pcre，nginx  \n安装zlib[root@localhost Liarlee]# cd zlib-[version][root@localhost Liarlee]# ./configure --prefix=/usr/local/zlib\n安装pcre[root@localhost Liarlee]# cd pcre-[version][root@localhost Liarlee]# ./configure --prefix=/usr/local/pcre\n安装nginx[root@localhost Liarlee]# cd nginx-[version][root@localhost Liarlee]# ./configure --prefix=/usr/local/nginx\n安装完成\n\n\n配置Sys V 脚本：#!/bin/sh\n#\n# nginx - this script starts and stops the nginx daemon\n#\n# chkconfig:   - 85 15\n# description:  Nginx is an HTTP(S) server, HTTP(S) reverse \\\n#               proxy and IMAP/POP3 proxy server\n# processname: nginx\n# config:      /etc/nginx/nginx.conf\n# config:      /etc/sysconfig/nginx\n# pidfile:     /var/run/nginx.pid\n# Source function library.\n. /etc/rc.d/init.d/functions\n# Source networking configuration.\n. /etc/sysconfig/network\n# Check that networking is up.\n[ \"$NETWORKING\" = \"no\" ] &amp;&amp; exit 0\nnginx=\"/usr/sbin/nginx\"\nprog=$(basename $nginx)\nNGINX_CONF_FILE=\"/etc/nginx/nginx.conf\"\n[ -f /etc/sysconfig/nginx ] &amp;&amp; . /etc/sysconfig/nginx\nlockfile=/var/lock/subsys/nginx\nmake_dirs() &#123;\n   # make required directories\n   user=`nginx -V 2>&amp;1 | grep \"configure arguments:\" | sed 's/[^*]*--user=\\([^ ]*\\).*/\\1/g' -`\n   options=`$nginx -V 2>&amp;1 | grep 'configure arguments:'`\n   for opt in $options; do\n       if [ `echo $opt | grep '.*-temp-path'` ]; then\n           value=`echo $opt | cut -d \"=\" -f 2`\n           if [ ! -d \"$value\" ]; then\n               # echo \"creating\" $value\n               mkdir -p $value &amp;&amp; chown -R $user $value\n           fi\n       fi\n   done\n&#125;\nstart() &#123;\n    [ -x $nginx ] || exit 5\n    [ -f $NGINX_CONF_FILE ] || exit 6\n    make_dirs\n    echo -n $\"Starting $prog: \"\n    daemon $nginx -c $NGINX_CONF_FILE\n    retval=$?\n    echo\n    [ $retval -eq 0 ] &amp;&amp; touch $lockfile\n    return $retval\n&#125;\nstop() &#123;\n    echo -n $\"Stopping $prog: \"\n    killproc $prog -QUIT\n    retval=$?\n    echo\n    [ $retval -eq 0 ] &amp;&amp; rm -f $lockfile\n    return $retval\n&#125;\nrestart() &#123;\n    configtest || return $?\n    stop\n    sleep 1\n    start\n&#125;\nreload() &#123;\n    configtest || return $?\n    echo -n $\"Reloading $prog: \"\n    killproc $nginx -HUP\n    RETVAL=$?\n    echo\n&#125;\nforce_reload() &#123;\n    restart\n&#125;\nconfigtest() &#123;\n  $nginx -t -c $NGINX_CONF_FILE\n&#125;\nrh_status() &#123;\n    status $prog\n&#125;\nrh_status_q() &#123;\n    rh_status >/dev/null 2>&amp;1\n&#125;\ncase \"$1\" in\n    start)\n        rh_status_q &amp;&amp; exit 0\n        $1\n        ;;\n    stop)\n        rh_status_q || exit 0\n        $1\n        ;;\n    restart|configtest)\n        $1\n        ;;\n    reload)\n        rh_status_q || exit 7\n        $1\n        ;;\n    force-reload)\n        force_reload\n        ;;\n    status)\n        rh_status\n        ;;\n    condrestart|try-restart)\n        rh_status_q || exit 0\n            ;;\n    *)\n        echo $\"Usage: $0 &#123;start|stop|status|restart|condrestart|try-restart|reload|force-reload|configtest&#125;\"\n        exit 2\nesac\n补充一些\n在编译安装的nginx中，可能没有conf.d目录，只要自己的nginx的配置文件中， http字段下添加include &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;*.conf 就可以使用conf.d目录进行不同站点独立配置文件的配置了。\nNginx可以支持同样使用80端口， 但是使用不同域名进行站点的发布。之前没有确切的试过，今天确实碰到了这个问题，记录下来。 在配置文件中，使用不同的server {}字段，定义不同的server_name ， 两个站点可以同时listen在80 端口上。访问的时候不同的域名会直接访问到不同的目录。Over.\n\n","categories":["Linux"],"tags":["Nginx"]},{"title":"Linux-内核参数笔记","url":"/2018/01/22/Linux/Linux_%E5%86%85%E6%A0%B8%E5%8F%82%E6%95%B0%E7%AC%94%E8%AE%B0/","content":"记录几个内核参数的意思和调整的效果。\n\n开始1. net.ipv4.tcp_max_tw_buckets对于tcp连接，服务端和客户端通信完后状态变为timewait，假如某台服务器非常忙，连接数特别多的话，那么这个timewait数量就会越来越大。毕竟它也是会占用一定的资源，所以应该有一个最大值，当超过这个值，系统就会删除最早的连接，这样始终保持在一个数量级。这个数值就是由net.ipv4.tcp_max_tw_buckets这个参数来决定的。CentOS7系统，你可以使用sysctl -a |grep tw_buckets来查看它的值，默认为32768，你可以适当把它调低，比如调整到8000，毕竟这个状态的连接太多也是会消耗资源的。  \n\n但你不要把它调到几十、几百这样，因为这种状态的tcp连接也是有用的，如果同样的客户端再次和服务端通信，就不用再次建立新的连接了，用这个旧的通道，省时省力。2. net.ipv4.tcp_tw_recycle &#x3D; 1该参数的作用是快速回收timewait状态的连接。上面虽然提到系统会自动删除掉timewait状态的连接，但如果把这样的连接重新利用起来岂不是更好。所以该参数设置为1就可以让timewait状态的连接快速回收，它需要和下面的参数配合一起使用。3. net.ipv4.tcp_tw_reuse &#x3D; 1该参数设置为1，将timewait状态的连接重新用于新的TCP连接，要结合上面的参数一起使用。4. net.ipv4.tcp_syncookies &#x3D; 1tcp三次握手中，客户端向服务端发起syn请求，服务端收到后，也会向客户端发起syn请求同时连带ack确认，假如客户端发送请求后直接断开和服务端的连接，不接收服务端发起的这个请求，服务端会重试多次。这个重试的过程会持续一段时间，当这种状态的连接数量非常大时，服务器会消耗很大的资源，从而造成瘫痪，正常的连接进不来，这种恶意的半连接行为其实叫做syn flood攻击。设置为1，是开启SYN Cookies，开启后可以避免发生上述的syn flood攻击。开启该参数后，服务端接收客户端的ack后，再向客户端发送ack+syn之前会要求client在短时间内回应一个序号，如果客户端不能提供序号或者提供的序号不对则认为该客户端不合法，于是不会发ack+syn给客户端，更涉及不到重试。5. net.ipv4.tcp_max_syn_backlog该参数定义系统能接受的最大半连接状态的tcp连接数。客户端向服务端发送了syn包，服务端收到后，会记录一下，该参数决定最多能记录几个这样的连接。我的CentOS7系统，默认是256，当有syn flood攻击时，这个数值太小则很容易导致服务器瘫痪，实际上此时服务器并没有消耗太多资源（cpu、内存等），所以可以适当调大它，比如调整到30000。6. net.ipv4.tcp_syn_retries该参数适用于客户端，它定义发起syn的最大重试次数，默认为5，建议改为2。7. net.ipv4.tcp_synack_retries该参数适用于服务端，它定义发起syn+ack的最大重试次数，默认为5，建议改为2，可以适当预防syn flood攻击。8. net.ipv4.ip_local_port_range该参数定义端口范围，系统默认保留端口为1024及以下，以上部分为自定义端口。这个参数适用于客户端，当客户端和服务端建立连接时，比如说访问服务端的80端口，客户端随机开启了一个端口和服务端发起连接，这个参数定义随机端口的范围。默认为32768    61000，建议调整为1025 61000。9. net.ipv4.tcp_fin_timeouttcp连接的状态中，客户端上有一个是FIN-WAIT-2状态，它是状态变迁为timewait前一个状态。该参数定义不属于任何进程的该连接状态的超时时间，默认值为60，建议调整为6。10. net.ipv4.tcp_keepalive_timetcp连接状态里，有一个是keepalived状态，只有在这个状态下，客户端和服务端才能通信。正常情况下，当通信完毕，客户端或服务端会告诉对方要关闭连接，此时状态就会变为timewait，如果客户端没有告诉服务端，并且服务端也没有告诉客户端关闭的话（例如，客户端那边断网了），此时需要该参数来判定。比如客户端已经断网了，但服务端上本次连接的状态依然是keepalived，服务端为了确认客户端是否断网，就需要每隔一段时间去发一个探测包去确认一下看看对方是否在线。这个时间就由该参数决定。它的默认值为7200（单位为秒），建议设置为30。11. net.ipv4.tcp_keepalive_intvl该参数和上面的参数是一起的，服务端在规定时间内发起了探测，查看客户端是否在线，如果客户端并没有确认，此时服务端还不能认定为对方不在线，而是要尝试多次。该参数定义重新发送探测的时间，即第一次发现对方有问题后，过多久再次发起探测。默认值为75秒（单位为秒），可以改为3。12. net.ipv4.tcp_keepalive_probes第10和第11个参数规定了何时发起探测和探测失败后再过多久再发起探测，但并没有定义一共探测几次才算结束。该参数定义发起探测的包的数量。默认为9，建议设置2。\n","categories":["Linux"],"tags":["Linux"]},{"title":"本地方式安装.Net3.5笔记","url":"/2018/01/22/Windows_%E6%9C%AC%E5%9C%B0%E6%96%B9%E5%BC%8F%E5%AE%89%E8%A3%85-Net3-5%E6%95%99%E7%A8%8B/","content":"windows新版本淘汰了.net 3.5，默认不安装。但是在没有网络的时候需要安装就很尴尬，我平时也没有备份这种安装包的习惯。所以找到了如下的解决方案，使用微软的映像部署工具进行安装，步骤如下：  \n步骤：\n挂载微软的官方镜像，或者放入安装光盘。\n打开cmd或者Powershell\n输入命令:  dism.exe /online /enable-feature /all /featurename:NetFX3 Source:Z:\\sources\\sxs\n等待系统处理命令，完成。\n\nNote:命令中可以不添加&#x2F;all，最后一条参数中的Z盘符改为镜像挂载所在的盘符即可  \n","categories":["Windows"],"tags":["Windows"]},{"title":"PPT转换WORD","url":"/2018/01/15/Windows_PPTConvertTOWORD/","content":"前一段时间赶上期末考试，老师放出了一个学期的PPT，于是大家开始疯狂的复制讲义，把关键的知识进行提炼，方便考试。但是老师放出来的PPT，文字量实在是太大了，如果手动复制就会很麻烦，最后自己都快崩溃了\n\n\n解决方式PPT作为微软OFFCIE套件中的一员，在PPT和Word之间都会提供一个相互转换的方式，保证内容可以快捷的在不同的软件中使用。甚至是与Adobe的PDF之间，其实都可以快捷的进行转换。教程开始了~   \n开始战斗！PPT创建讲义\n选择左上角的文件选项卡\n选择导出选项\n点击创建讲义\n选择只使用大纲\n我们会直接得到一份PPT的Word文件，但是排版是混乱的，文字的大小都是不同的\n\n去掉文字格式和空格\n首先全选文档中的文字，快捷键：Ctrl+A  \n将文本全部复制到Notepad++中\n进入notepad++的文本就会自动变成无格式文本，这是notepad++和Word软件的不同。notepad++是纯文本编辑软件，而Word是字处理软件。  \n之后，选择编辑–&gt;行操作–&gt; 移除空行(包括空白字符)，这样文本中的空行和奇怪的空白字符就完全去掉，变成了完全的纯文本\n再把最后的文本复制回到Word中就好了~\n\n去掉不需要的内容最后就是根据每个人的不同需要进行文档中内容的删除，精简，大功告成~  \n总结经过这样的处理，省去了每个PPT的文字都手动复制的麻烦，其中最最消耗时间的部分就是最后的内容精简，但是这部分是我们必须要进行思考和手动操作的部分，只有我们自己知道自己需要什么内容。  \n","categories":["Windows"],"tags":["Office"]},{"title":"Python入门笔记 (三)","url":"/2018/01/10/Python%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/Python_%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0-3/","content":"\npython进行系统管理的模块。系统管理模块的介绍和使用。系统管理的四个模块:\n\nos\nos.path\nglob\nfnmatch\n\n\n\n1.OSOS模块中包含了普遍的操作系统功能，如果希望程序与平台没有强关联的情况下这个模块很重要，这个模块可以在不改动程序的基础上，使程序运行在Linux和Windows之间。\n可移植性说明os.name   #获取当前的操作系统信息\nos.getcwd()   #获取当前的工作目录\nos.listdir()    #返回目录下的所有文件和目录名称\nos.remove()     #删除指定的文件\nos.linesep      #通过字符串的方式给出当前平台的换行符号\n\nOS模块操作文件os.unlink       #删除路径所指向的文件\nos.remove       #删除路径所指向的文件\nos.rmdir        #删除path路径指向的空文件夹，必须是空的\nos.mkdir        #创建一个文件夹\nos.rename       #重命名一个文件或者文件夹\n\nOS模块操作权限os.access('/etc/fstab', os.R_OK)      #access是判断是否具有对某一个文件的相关权限，Linux中的 chmod/rwx---对应OS模块中的R_OK,W_OK，X_OK\n\nos.path模块os.path模块主要是用来拆分路径，构建新的路径，获取文件属性和判断文件的类型使用。\n拆分路径os.path.split()           #返回一个二元数组，包括文件路径和文件名称\nos.path.dirname()         #返回文件路径\nos.path.basename()        #返回文件的文件名\nos.path.splitext()        #一个文件名和扩展名组成的二元组\n\n构建路径os.path.expanduser('~/Liarlee')\nos.path.abspath('Liarlee.txt')\nos.path.join(os.path.expanduser('~mysql', 't', 'Liarlee.py'))\n\n获取文件属性os.path.getatime()        #获取文件的访问时间\nos.path.getmtime()        #获取文件的修改时间\nos.path.getctime()        #获取文件的创建时间\nos.path.getsize()         #获取文件的大小\n\n判断文件类型os.path.exists        #参数path指向的路径是否存在\nos.path.isfile        #参数path指向的路径存在，并且是一个文件\nos.path.isdir         #路径存在，并且是一个文件夹\nos.path.islink        #路径存在，并且是一个链接\n\nfnmatch模块glob模块","categories":["Python"],"tags":["Python"]},{"title":"Python入门笔记 (二)","url":"/2018/01/09/Python%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/Python_%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0-2/","content":"\ndef定义函数def fact(n):                    #定义求阶乘函数\n    if n &#x3D;&#x3D; 1:\n        return 1\n    return n * fact(n - 1)\n\nresult &#x3D; fact(5)                #计算结果\nprint(result)                   #输出计算结果\n\n\n\n递归函数 – 尾递归优化用递归函数需要注意防止栈溢出。解决递归调用栈溢出的方法是通过尾递归优化，\ndef fact(n):                                #尾递归优化\n    return fact_iter(n, 1)                  #函数返回值是之前定义的递归函数\n\ndef fact_iter(num, product):\n    if num &#x3D;&#x3D; 1:\n        return product\n    return fact_iter(num - 1, num * product)\n    print(num, product)\n\n\nresult &#x3D; fact(1000)\nprint(result)\n\n迷之递归 – 汉诺塔 (没懂)def move(n, a, b, c):\n    if n &#x3D;&#x3D; 1:                          #只有一个盘子，所以从a到c\n        print(a, &#39;--&gt;&#39;, c)\n    else:\n        move(n-1, a, c, b)              #有N个盘子，移动N-1个盘子，a-b\n        move(1, a, b, c)                #将最大的盘子从a--c\n        move(n-1, b, a, c)              #剩下的N-1从b--c\n\nmove(3, &#39;A&#39;, &#39;B&#39;, &#39;C&#39;)                      # 总共三个盘子，ABC三个柱子\n\n高级特性切片L &#x3D; [&#39;aaa&#39;, &#39;bbb&#39;, &#39;ccc&#39;]\n\n# 取出前两个元素\n\n# Method 1\n[L[0],L[1]]\n\n# Method 2 -- For\nr &#x3D; []\nn &#x3D; 2\nfor i in range(n):\n    r.append(L[i])\n\nprint(r)\n\n# Method 3 -- Slice\nprint(L[0:2])               #取出 0--2\n\nprint(L[:2])                #取出 0--2\n\nprint(L[-2:])               #取出倒数两个元素\n\nprint(L[-2:-1])             #取出倒数第二个\n\nL[a:b:c]                    #a----索引开始序号\n                            #b----索引结束序号\n                            #c----步长\n切片练习def trim(s):\n    if not len(s):        #非空\n        return s\n    if s[0] &#x3D;&#x3D; &#39; &#39;:       #起始位置位置为空\n        s &#x3D; s[1:]\n        return trim(s)        #返回处理后的字符串\n    elif s[-1] &#x3D;&#x3D; &#39; &#39;:        #末尾为空\n        s &#x3D; s[:-1]\n        return trim(s)\n    return s              #返回s\n\nif trim(&#39;hello  &#39;) !&#x3D; &#39;hello&#39;:\n    print(&#39;测试失败!&#39;)\nelif trim(&#39;  hello&#39;) !&#x3D; &#39;hello&#39;:\n    print(&#39;测试失败!&#39;)\nelif trim(&#39;  hello  &#39;) !&#x3D; &#39;hello&#39;:\n    print(&#39;测试失败!&#39;)\nelif trim(&#39;  hello  world  &#39;) !&#x3D; &#39;hello  world&#39;:\n    print(&#39;测试失败!&#39;)\nelif trim(&#39;&#39;) !&#x3D; &#39;&#39;:\n    print(&#39;测试失败!&#39;)\nelif trim(&#39;    &#39;) !&#x3D; &#39;&#39;:\n    print(&#39;测试失败!&#39;)\nelse:\n    print(&#39;测试成功!&#39;)\n\nFor循环迭代 练习def findMinAndMax(L):\n    if 0 &#x3D;&#x3D; len(L):\n        return None, None\n\n    min &#x3D; L[0]\n    max &#x3D; L[0]\n    i &#x3D; 0\n    for i in L:\n        if i &gt; max:\n            max &#x3D; i\n        if i &lt; min:\n            min &#x3D; i\n    return min, max\n\n# 测试\nif findMinAndMax([]) !&#x3D; (None, None):\n    print(&#39;测试失败!&#39;)\nelif findMinAndMax([7]) !&#x3D; (7, 7):\n    print(&#39;测试失败!&#39;)\nelif findMinAndMax([7, 1]) !&#x3D; (1, 7):\n    print(&#39;测试失败!&#39;)\nelif findMinAndMax([7, 1, 3, 9, 5]) !&#x3D; (1, 9):\n    print(&#39;测试失败!&#39;)\nelse:\n    print(&#39;测试成功!&#39;)\n\n\n\n\n\n\n\n列表生成式list(range(1,11))\n\nM &#x3D; [x * x for x in range(1, 11)]     #列表生成式\nprint(M)\n\nlist &#x3D; [s.lower() for s in L1]\n\n# 练习\nL1 &#x3D; [&#39;Hello&#39;,&#39;World&#39;, 18, &#39;Apple&#39;, None]\ntest &#x3D; isinstance(L1, str)\nprint(test)\n\nL2 &#x3D; [s.lower() for s in L1 if isinstance(s, str) &#x3D;&#x3D; True]\n\n\n# 测试:\nprint(L2)\nif L2 &#x3D;&#x3D; [&#39;hello&#39;, &#39;world&#39;, &#39;apple&#39;]:\n    print(&#39;测试通过!&#39;)\nelse:\n    print(&#39;测试失败!&#39;)\n","categories":["Python"],"tags":["Python"]},{"title":"grub2手动修复引导错误","url":"/2018/01/09/Linux/Linux_Grub2repair/","content":"grub2引导错误的手动解决方法\n\n\ngrub是什么\n引用百度百科    \n\nGNU GRUB（简称“GRUB”）是一个来自GNU项目的启动引导程序。GRUB是多启动规范的实现，它允许用户可以在计算机内同时拥有多个操作系统，并在计算机启动时选择希望运行的操作系统。GRUB可用于选择操作系统分区上的不同内核，也可用于向这些内核传递启动参数。GNU GRUB的前身为Grand Unified Bootloader。它主要用于类Unix系统；同大多Linux发行版一样，GNU系统也采用GNU GRUB作为它的启动器。Solaris从10 1&#x2F;06版开始在x86系统上也采用GNU GRUB作为启动器。  \n\n\n引用fedora官方wiki  \n\nGRUB 2 is the latest version of GNU GRUB, the GRand Unified Bootloader. A bootloader is the first software program that runs when a computer starts. It is responsible for loading and transferring control to the operating system kernel, (Linux, in the case of Fedora). The kernel, in turn, initializes the rest of the operating system.GRUB 2 has replaced what was formerly known as GRUB (i.e. version 0.9x), which has, in turn, become GRUB Legacy.Starting with Fedora 16, GRUB 2 is the default bootloader on x86 BIOS systems. For upgrades of BIOS systems the default is also to install GRUB 2, but you can opt to skip bootloader configuration entirely.\n\n\n\n修复grub2引导程序开机失败一般情况下系统的grub文件不会丢失，但是有的时候比如我们进行了系统设置的更改或者文件的误删除会导致系统无法正常启动，这个时候我们需要对grub进行手动的配置，才能是计算机正确的进入系统之中，进入系统之后只要重新生成grub引导文件，就可以让系统重启自动完成引导进入系统了。  \n指出&#x2F;boot位置\n指定grub的硬盘引导分区，也就是&#x2F;boot所在的硬盘分区grub&gt; root (hd0,0)\n\n指出vmlinuz内核文件\n指出kernel所在的分区，主要是vmlinuz内核文件所在的位置grub&gt; kernel /vmlinuz\n指出initrd所在的分区，指定初始化文件所在的位置grub&gt; initrd /initrd\n\n启动\n文件锁定成功之后，就可以启动系统grub&gt; boot\n\n在系统中重新安装和更改grub2安装或者重新安装grub2（非必须-如果有需要的时候执行）安装grub2到硬盘分区，一般为硬盘的第一个分区的开始位置  \n\n传统引导方式安装grub2[root@localhost /]# grub2-install /dev/sda  \nEFI引导安装grub2[root@localhost /]# dnf reinstall grub2-efi shim\n\n重新生成grub配置文件使用grub2-mkconfig生成新的grub2配置文件，同时指定引导时读取该文件不同的安装方式有不同的命令  \n\n标准的传统引导安装方式[root@localhost /]# grub2-mkconfig -o /boot/grub2/grub.cfg\nEFI引导方式[root@localhost /]# grub2-mkconfig -o /boot/efi/EFI/fedora/grub.cfg\n\nWindows&amp;Linux双系统修复windows和linux的双系统请注意安装顺序，可以减少很多麻烦  \n\n先安装windows - 然后安装linux\n\n如果先安装了linux，那么在安装windows之后，系统的MBR会被windows覆写，导致linux系统无法正常引导，这个时候需要使用livecd进行修复grub2，让grub2重新安装并且控制引导系统的分区  \n\n\n\n删除系统更新之后多余的启动项系统更新之后总是会多很多的旧版本内核的启动项，每次启动的时候看起来都很乱，简单操作就可以去掉多余的启动项，如下：  \n暴力方式\n在boot目录下面强制删除内核文件，然后重新生成grub2.cfg即可  \n[root@localhost boot]# rm -rf vmlinuz-4.13.9-300.fc27.x86_64   \n[root@localhost boot]# rm -rf vmlinuz-4.14.5-300.fc27.x86_64  \n\n重新生成配置文件：  \n[root@localhost boot]# grub2-mkconfig -o &#x2F;boot&#x2F;efi&#x2F;EFI&#x2F;fedora&#x2F;grub.cfg\nGenerating grub configuration file ...\nFound linux image: &#x2F;boot&#x2F;vmlinuz-4.14.11-300.fc27.x86_64\nFound initrd image: &#x2F;boot&#x2F;initramfs-4.14.11-300.fc27.x86_64.img\nFound linux image: &#x2F;boot&#x2F;vmlinuz-0-rescue-2fbfdacc99ad4255953ae8b2ec521e9d\nFound initrd image: &#x2F;boot&#x2F;initramfs-0-rescue-2fbfdacc99ad4255953ae8b2ec521e9d.img\ndone\n\n正确方式\n使用dnf命令卸载旧内核：  [root@localhost &#x2F;]# dnf remove kernel-4.13.9-300.fc27.x86_64  \n[root@localhost &#x2F;]# dnf remove kernel-4.14.5-300.fc27.x86_64  \n\n","categories":["Linux"],"tags":["grub2"]},{"title":"Python入门笔记 (一)","url":"/2018/01/08/Python%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/Python_%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0-1/","content":"  \nPython的输入输出print(&#39;Hello,World!&#39;)  \nname &#x3D; input()\nprint(&#39;Your name is&#39;, name, &#39;,Right?&#39;)\n\n\n\nPython字符中文编码Title#!&#x2F;usr&#x2F;bin&#x2F;env python3  \n# -*- coding: utf-8 -*-  \n\na &#x3D; input()\nage &#x3D; int(a)                       \\\\print输出都是字符串，格式化为数字之后才可以使用\n\nPython输出多行内容print(&#39;&#39;&#39;line1\n...line2\n...line3\n&#39;&#39;&#39;)\n\nPython的语句拼接print(&#39;Hello,World!&#39;,&#39;My Name is Liarlee.&#39;)  \n\nPython占位符name &#x3D; Liarlee\nage &#x3D; 24\nprint(&#39;Hello,%s&#39; % (name))\nprint(&#39;Your age is %d&#39; % (age))\n\npython中使用list和tupleList – 列表classmates &#x3D; [Liarlee,TEST,WTF,LOL]\nclassmates就是一个list，list可以嵌套list    \nlen(classmates)                    \\\\查看list的长度  \nclassmates[1]                      \\\\list第二个元素  \nclassmates[-1]                     \\\\list倒数第一个元素    \nclassmates.append(&#39;Liarlee&#39;)       \\\\末尾添加一个值\nclassmates.pop()                   \\\\删除末尾的值\nclassmates.pop(i)                  \\\\删除指定索引位置的值\nclassmates.insert(1, &#39;Liarlee&#39;)    \\\\将值插入到某个索引位置\nclassmates[1] &#x3D; Liarlee            \\\\直接更改某个索引位置的值\nclassmates.sort()                  \\\\list排序\n\nTuple – 元组另一种有序列表叫元组：tuple。tuple和list非常类似，但是tuple一旦初始化就不能修改  \nclassmates &#x3D; (Liarlee,TEST,WTF,LOL)\n\n循环和判断IF判断if\nelif\n.\n.\n.\nelif\nelse                                        \\\\完全if格式        \n\nFOR……IN……循环for classmate in classmates:\n  print(classmate)\n\nlist(range(101))                              \\\\生成0-100数字存入list\n\nWHILE循环# -*- coding: utf-8 -*-\nclassmates &#x3D; [Liarlee,TEST,WTF,LOL]\ni &#x3D; 0\nwhile i &lt; 4:\n    print(&#39;Hello,%s&#39; % (classmates[i]))\n    i &#x3D; i + 1\n\ndict和setdict – 字典SCORE &#x3D; &#123;&#39;Liarlee&#39;: 95, &#39;TEST&#39;: 75, &#39;WTF&#39;: 85, ‘LOL‘：54&#125;       \\\\定义字典\nd[&#39;Liarlee&#39;] &#x3D; 67                                                \\\\字典赋值\n&#39;Thomas&#39; in d                                                    \\\\确定Key是否在字典中\nd.get(&#39;Liarlee&#39;)                                                 \\\\确定Key是否在字典中\nd.pop(&#39;Liarlee&#39;)                                                 \\\\删除Key\n\nsetset和dict类似，也是一组key的集合，但不存储value。    \ns &#x3D; set([1, 2, 3])                      \\\\创建一个set然后将list传入\ns.add(4)                                \\\\添加一个元素进入set\n","categories":["Python"],"tags":["Python"]},{"title":"如何使用微软官方镜像重装系统","url":"/2018/01/02/Windows_%E5%A6%82%E4%BD%95%E7%BB%99%E8%87%AA%E5%B7%B1%E7%9A%84%E7%94%B5%E8%84%91%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F/","content":"注意，这是一个完整且详细的新手教程！\n\n\n一、写在最前面有几句有用的建议，在这里先写,能让你的电脑长久运行良好：- 国内的软件普遍都带有广告，这不是Windows的锅- 不要使用360系列的产品 ，有人和我说企业版特别良心，但是我没用过，欢迎尝试后给我反馈- 所有软件都从官方网站下载，不要图方便使用软件升级助手，或者电脑管家- 安装程序请仔细观察是否有捆绑软件，一定取消勾选的捆绑软件- 注意观察安装程序界面的角落位置|折叠菜单|高级选项|更多，否则你的电脑一定会“自动”多很多奇怪的东西- 如果可以尽量使用微软官方的系统，第三方系统或者Ghost系统确实方便了你的安装过程，但是你一定会被广告和捆绑的软件折磨的不要不要的～最重要的是，不能确定是否会有后门程序，谨慎使用Onekey-Ghost和YLMF- 最后一点千万注意，系统优化类软件除非你清楚这个软件都更改了那些设置，否则不要使用，请相信没有任何一个第三方公司比微软更了解自己的产品  \n\n二、环境及工具\n可以正常使用的 计算机A（也就是可用的电脑）  \n需要重装系统的 计算机B（一定要保证开机出现Logo，如果开机Logo都没有，请确认计算机是否通电；检查显示器是否正常；如其他情况请送维修。）  \n4G 空间以上的 空U盘  \nRufus (后面介绍这个工具的用途)Rufus-官方网站Rufus-下载链接  \n微软官方 的系统镜像文件MSDNitellyou-搬运官方的非官方下载点MSDNitellyou-Windows10中文64位下载链接ed2k  \n最重要的一点，最好有网，没有会麻烦些\n\n\n三、原理部分为计算机安装操作系统过程\n计算机的简要启动过程： 通电 - 硬件自检 - 寻找操作系统所在硬盘 - 启动操作系统  \n安装系统难理解的是在硬件自检之后，直接从U盘中读取系统安装程序，然后将系统这个大程序写入硬盘的相应分区内\n\n\n四、开始安装操作部分 – 计算机A接下来的操作都是在计算机A下完成的~  \nRufusrufus是用来制作可启动U盘的一款绿色应用程序  \n下载系统镜像文件上面提到的系统镜像文件以Win10-1709为例,这是当前最新版本的Windows,下载之后应该是一个后缀名为.iso的文件\n使用Rufus制作可启动U盘\n双击打开Rufus，插入U盘，确认最顶部设备已经识别你的空U盘  \n点击箭头指向的按钮，选择刚才下载的镜像文件  \n加载镜像文件，选项如图，选择开始  \n点击确认    \n开始写入 ， 等待  \nU盘制作成功\n\n操作部分 – 计算机B下面的部分转移到重新安装系统的计算机B上面来了～  \n从U盘中启动系统安装程序\n开机按下电源按键之后，请反复按下F12，直到屏幕变成这个样子～  \n选择引导设备菜单，请选择刚刚制作好的U盘，一般设备名称会是USB……或者Sandisk……或者Kingston……这类的名字，或者如上图–通用闪存设备，回车选择后安装程序自行启动  \n选择语言，没什么特殊要求就中文就可以了，当然如果需要请下载EN版本的镜像，或者多国语言版，在下拉菜单中选择需要的语言  \n点击现在安装…   \n稍等  \n这里有两种情况：\n\n\n有密钥的情况请直接输入，系统会被激活，安装完成之后就是官方正版的系统；  \n没有密钥，只是需要在系统安装完成之后再次输入密钥或者安装一个补丁就好；\n\n    \n\n这个位置就很随意了，选择你需要的版本，我安装的是一个WIN10专业版  \n只能接受，接受才能下一步  \n这里也是两种情况：\n\n\n如果你的计算机中本身有WIN7、8、8.1，可以选择直接升级，但是升级之后会有Windows Old文件夹占用大量C盘空间，清理会很麻烦；\n如果就已经打算重新安装了，请确认数据已经备份或者无用，之后这里选择自定义，进入下一步；\n\n    \n\nNote: 如果你的计算机中有重要数据请自行备份，\n\n  - 下面开始进行的操作会直接修改硬盘的内容！！！  - 下面开始进行的操作会直接修改硬盘的内容！！！  - 下面开始进行的操作会直接修改硬盘的内容！！！   \n  硬盘分区关键词：  驱动器0 是指物理硬盘编号0，有几块硬盘就顺序向下编号0&#x2F;1&#x2F;2&#x2F;3…  分区1 一般是指物理硬盘的第一个分区，通常对应了CDEF不同的字母，同样有几个分区就顺序向下编号1&#x2F;2&#x2F;3…  主分区 是我们在系统内使用的硬盘分区，主分区从上到下分别是CDEF盘，图中只有一个主分区，所以我的系统安装完成之后只有一个C盘  \n  图示数字说明:  1-删除 硬盘分区会让当前的硬盘空间释放为可用空间，用于打破之前划分的分区，比如可以将分区CDEF的EF删除，合并成新的E盘  \n  2-格式化 格式化是清空当前选择分区内的所有数据，所有分区我们都需要格式化  \n  3-新建 新建分区需要我们指定大小，比如新建的C盘我希望有100G，那么我需要在新建之后输入100*1024M&#x3D;102400，这样就新建了100G的新分区   \n  4-类型 中，我们只需要修改和关注主分区即可,其他保留就可以  \n  Note: 点击下一步之前，一定确认我们选择的是 由上到下的第一个主分区 ，因为系统会安装到高亮选择的主分区内，请 检查 之后在点击下一步\n\n点了 下一步 就开始安装咯， 之前的已经没有办法再更改了 ，除非从1开始再来一次…..  \n\n结束了，等待它自动重启看看全新的系统吧～\n\n\n\n五、配置新系统激活 淘宝 激活码 很便宜的  \n安装驱动官方网站 或者 驱动精灵最好不用鲁大师和360，真的  \n其他选项待续吧….累了…\n\n六、软件推荐微信QQ这些都是必备的，我就不说了……说一些好用的软件但是少有人知道的……我平时不安装电脑管家，杀毒软件就一个小红伞，如果真的不放心会在加一个火绒，基本上够了，小红伞对硬盘要求还挺高的，如果觉得安装小红伞卡，就安装小A吧，Avast……  \n软件清单如下：\n\nNotepad++ 替代Windows记事本的最佳方案，支持代码高亮哦～  \nPotplayer Windows下最简单好用的播放器～  \nEverything Windows搜索神器～  \nFirefox 浏览器……我不知道该怎么说，又爱又恨吧～  \nCCleaner Windows下的老牌清理软件….还不错～\nOffice 2016 - 全家桶 ed2k Office2016 ed2k链接，请直接迅雷下载～\nBandizip 美观友好的解压缩软件，省心免费～\nMactype Windows字体渲染优化，解救windows默认的字体渲染，配置省事，效果明显～\n\n","categories":["Windows"],"tags":["Windows"]},{"title":"Firefox-Openh264的问题","url":"/2017/12/28/Linux/Linux_Firefox-Openh264/","content":"fedora27中Firefox插件OpenH264不启用  \n\n\n环境OS：Fedora 27 WorkstationSoftware: Firefox 57  \n问题Firefox的插件中提示OpenH264 未启用，不能正常使用，打开视频站点不能播放视频。在插件中调整插件状态为 Always Activate，插件状态改变为将被安装，但是无论的等待多久这个插件的安装状态不会改变，依旧不能正常工作。  \n解决办法查找到官方WIKI，给出了如下的解决办法：  \n\n在Fedora默认给出的官方源中，有一个名称是：fedora-cisco-openh264.repo\n这个源默认关闭，开启它sudo dnf config-manager --set-enabled fedora-cisco-openh264  \n安装如下两个插件sudo dnf install gstreamer1-plugin-openh264 mozilla-openh264  \n重启Firefox查看插件状态已经恢复正常，启动openh264插件即可。\n\nOpenh264测试页Simple mozRTCPeerConnection Video Test\n","categories":["Linux"],"tags":["Firefox"]},{"title":"Hexo+Github建立个人博客记录","url":"/2017/12/27/Hexo_Install/","content":"建立Blog，记录过程所有的配置都是在Fedora 27 Workstation版本下进行，Windows配置环境恶心了我很久所以不做介绍。\n\n\nHexo部署环境Hexo运行在Linux环境中的配置及其简单，只需要确认系统中安装了git和Nodejs就好，在Fedora27中已经默认有Git软件包，如果需要安装git使用dnf install -y git就好。目前我们只需要添加nodejs就可以了，准备工作开始～使用git version 进行git是否存在于系统中的检测。\n安装Node.jsNode.js的安装只需要两条命令，按照顺序执行就好：  \n\n用于系统的更新，基本上等待命令结束就可以，只是保证运行环境的所有软件包都是最新的状态。\n\n[root@localhost test]# dnf update -y  \n\n\n用于安装Nodejs，dnf管理器会自动配置需要的依赖软件，也是等待就好，没有特别的操作。\n\n[root@localhost test]# dnf install -y nodejs\n\n\n\n安装Hexo\n接下来需要选择一个你想安装的目录，例如：&#x2F;root&#x2F;Document&#x2F;Hexo&#x2F;test  \n\n那么需要确定工作目录并且切换过去  \n\n[root@localhost test]# cd /root/Document/Hexo/test  \n\n\n之后所有的操作都会在这个目录或者它的子目录，请留意。  \n\n[root@localhost test]# npm install hexo -g  \n\n\n等待安装结果…….  \n\n安装结束之后，查看是否安装成功,使用：  \n\n[root@localhost test]# hexo -v  \n\n\n如输出如下信息则说明安装成功，可以执行下一步。  \n\n[root@localhost test]# hexo -vhexo: 3.4.4hexo-cli: 1.0.4os: Linux 4.13.13-300.fc27.x86_64 linux x64http_parser: 2.7.1node: 8.9.3v8: 6.1.534.48uv: 1.16.0zlib: 1.2.11ares: 1.10.1-DEVmodules: 57nghttp2: 1.25.0openssl: 1.0.2m-fipsicu: 57.1unicode: 8.0cldr: 29.0tz: 2016b\n\n\n\n安装成功，没有写安装失败的解决方法，因为我觉得难以失败，成功率很高的。  \n失败解决方案\n失败请更换cnpm 或者 更换淘宝源重试。  \n如果安装，确认自己在正确的目录下，执行npm install重试\n\n安装Hexo-server安装Hexo-server主要是作为本地测试使用，通过本地localhost:4000访问来预览，方便调整。  \n\n使用如下命令执行安装：  \n\n[root@localhost test]npm install hexo-server -g\n\n\n执行启动hexo-server，确认是否可以正常使用：  \n\n[root@localhost test]hexo sOR[root@localhost test]hexo server  \n\n\n输入如下结果则正常启动：  \n\n[root@localhost test]# hexo s INFO  Start processing INFO  Hexo is running at http://localhost:4000/. Press Ctrl+C to stop.  \n\n\n这里服务器已经启动了，使用Ctrl+C停止服务，恢复正常的Shell窗口。\n\n\n初始化Hexo\n这里需要创建新的Blog目录，例如：&#x2F;root&#x2F;Document&#x2F;Hexo&#x2F;test\n\n在test目录下执行：  \n\n[root@localhost test] hexo init  \n\n\n初始化会将Blog所需的文件放入test文件夹中，这个目录就是我们需要推到github上面的东西。也就是整个博客的根目录。\n\n\n配置Git这部分的内容已经和我们本地的计算机没有什么太大的关系了，我们需要去Github上面注册一个账户，在这个账户里面添加一个新的repo，例如： test.github.io我们在本地建立的site使用git工具，将整个blog发布到对应的软件仓库中，接下来我们需要配置git的信息使用命令配置git的用户名和邮箱：  \n\n[root@localhost test] git config --global user.name &quot;yourname&quot;[root@localhost test] git config --global user.email &quot;youremailaddress&quot;\n\n添加SSH认证使用ssh进行与github的通信可以免去我们每次都输入用户名密码的繁琐，因此我们需要为本地的计算机生成一个SSH的KEY。  \n\n命令如下：  \n\n[root@localhost test] ssh-keygen -t rsa -C &quot;your_email@example.com&quot;  \n\n\n期间会让我们输入密码进行验证，如果这个位置输入密码，今后每次连接到github的时候，都需要输入这个密码才能连接，如果这里我们不输入密码直接回车，今后连接的时候就不需要密码了，推送的内容会直接被推送，之后返回结果。  \n\n生成的密钥会直接放在当前用户家目录下的隐藏文件夹里，因此我们把他找出来：  \n\n[root@localhost test] cd ~/.ssh  \n\n\n复制id_rsa.pub文件内的所有内容，粘贴到github的添加SSH密钥位置。成功。\n\n\n连接Github测试连接到Github是否能够成功，使用如下命令：  \n\n[root@localhost test] ssh -T git@github.com  \n\n如果上一部分生成SSH密钥的时候输入了密码，请在这个命令运行之后按照提示输入密码，返回结果如下：  \n\n[root@localhost .ssh]# ssh -T &#x67;&#105;&#116;&#64;&#x67;&#x69;&#116;&#104;&#117;&#98;&#x2e;&#x63;&#x6f;&#109;Hi test! You’ve successfully authenticated, but GitHub does not provide shell access.  \n\n我们已经成功的连接上了Github上面的repo，github的配置结束了～  \n配置Hexo-deployergit可以将我们的blog推送到repo上面去，但是本身我们也可以使用hexo提供的工具来进行blog的更新。  \n\n直接编辑hexo的配置文件： _config.yml  \n\n[root@localhost test]# vim _config.yml\n\n\n修改配置文件中的这个部分，如下是最终的修改结果：  \n\n 77 # Deployment 78 ## Docs: https://hexo.io/docs/deployment.html 79 deploy: 80   type:  git 81   repo:  &#103;&#105;&#x74;&#x40;&#x67;&#x69;&#x74;&#104;&#x75;&#98;&#x2e;&#99;&#111;&#x6d;:test&#x2F;test.github.io.git 82   branch:  master  \n\n\n现在将我们本地的默认站点推送到Github上面去试试吧！   \n\n[root@localhost test]# hexo dINFO  Deploying: gitINFO  Clearing .deploy_git folder…INFO  Copying files from public folder…INFO  Copying files from extend dirs…[master caa83a9] Site updated: 2017-12-27 18:26:01 1 file changed, 63 insertions(+), 11 deletions(-)To github.com:test&#x2F;test.github.io.git62dcd56..caa83a9  HEAD -&gt; masterBranch master set up to track remote branch master from   &#x67;&#105;&#x74;&#64;&#x67;&#105;&#116;&#x68;&#117;&#98;&#x2e;&#x63;&#x6f;&#x6d;:test&#x2F;test.github.io.git.INFO  Deploy done: git  \n\n\n输出最后Deploy done,就表示我们的原始页面已经上传成功了～，访问Repo的名字就可以直接看到初始blog的样子了。\n\n后续插件安装的记录:\n[root@localhost ~]# cnpm list -g --depth 0\n&#x2F;usr&#x2F;lib\n├── cnpm@6.0.0\n├── hexo@3.4.4\n├── hexo-cli@1.1.0\n├── hexo-generator-archive@0.1.5\n├── hexo-generator-category@0.1.3\n├── hexo-generator-index@0.2.1\n├── hexo-generator-search@2.4.0\n├── hexo-generator-tag@0.2.0\n├── hexo-render-pug@2.1.0\n├── hexo-site@0.0.0\n└── npm@6.4.1\n\nhexo常用命令说明Hexo目录下常用的命令有：\n[root@localhost test]# hexo s           &#x2F;&#x2F;启动hexo本地服务进行blog预览\n[root@localhost test]# hexo new TITLE   &#x2F;&#x2F;新建文章\n[root@localhost test]# hexo clean       &#x2F;&#x2F;清除缓存\n[root@localhost test]# hexo g           &#x2F;&#x2F;重新生成站点页面文件\n[root@localhost test]# hexo d           &#x2F;&#x2F;推送到github\n添加鼠标爆炸点击效果在&#x2F;themes&#x2F;yelee&#x2F;layout&#x2F;layout.ejs的文件中，文件开始位置加入如下字段：  \n&lt;head&gt;\n  &lt;canvas class&#x3D;&quot;fireworks&quot; style&#x3D;&quot;position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;&quot; &gt;&lt;&#x2F;canvas&gt; \n  &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;&#x2F;cdn.bootcss.com&#x2F;animejs&#x2F;2.2.0&#x2F;anime.min.js&quot;&gt;&lt;&#x2F;script&gt; \n  &lt;script type&#x3D;&quot;text&#x2F;javascript&quot; src&#x3D;&quot;&#x2F;js&#x2F;firework.js&quot;&gt;&lt;&#x2F;script&gt;\n&lt;&#x2F;head&gt;\n在themes&#x2F;yelee&#x2F;source&#x2F;js&#x2F;建立如下文件firework.js,文件内容如下：  \n&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;&quot;use strict&quot;;function updateCoords(e)&#123;pointerX&#x3D;(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY&#x3D;e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t&#x3D;anime.random(0,360)*Math.PI&#x2F;180,a&#x3D;anime.random(50,180),n&#x3D;[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;colors[anime.random(0,colors.length-1)],a.radius&#x3D;anime.random(16,32),a.endPos&#x3D;setParticuleDirection(a),a.draw&#x3D;function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle&#x3D;a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a&#x3D;&#123;&#125;;return a.x&#x3D;e,a.y&#x3D;t,a.color&#x3D;&quot;#F00&quot;,a.radius&#x3D;0.1,a.alpha&#x3D;0.5,a.lineWidth&#x3D;6,a.draw&#x3D;function()&#123;ctx.globalAlpha&#x3D;a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth&#x3D;a.lineWidth,ctx.strokeStyle&#x3D;a.color,ctx.stroke(),ctx.globalAlpha&#x3D;1&#125;,a&#125;function renderParticule(e)&#123;for(var t&#x3D;0;t&lt;e.animatables.length;t++)&#123;e.animatables[t].target.draw()&#125;&#125;function animateParticules(e,t)&#123;for(var a&#x3D;createCircle(e,t),n&#x3D;[],i&#x3D;0;i&lt;numberOfParticules;i++)&#123;n.push(createParticule(e,t))&#125;anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:0.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n&#x3D;this,i&#x3D;arguments;clearTimeout(a),a&#x3D;setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl&#x3D;document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx&#x3D;canvasEl.getContext(&quot;2d&quot;),numberOfParticules&#x3D;30,pointerX&#x3D;0,pointerY&#x3D;0,tap&#x3D;&quot;mousedown&quot;,colors&#x3D;[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize&#x3D;debounce(function()&#123;canvasEl.width&#x3D;2*window.innerWidth,canvasEl.height&#x3D;2*window.innerHeight,canvasEl.style.width&#x3D;window.innerWidth+&quot;px&quot;,canvasEl.style.height&#x3D;window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render&#x3D;anime(&#123;duration:1&#x2F;0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!&#x3D;&#x3D;e.target.id&amp;&amp;&quot;A&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;&quot;IMG&quot;!&#x3D;&#x3D;e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;;\n","categories":["Linux"],"tags":["Hexo"]},{"title":"想说点儿什么","url":"/about/index.html","content":"嗟乎！时运不齐，命途多舛。冯唐易老，李广难封。屈贾谊于长沙，非无圣主；窜梁鸿于海曲，岂乏明时？所赖君子见机，达人知命。老当益壮，宁移白首之心？穷且益坚，不坠青云之志。酌贪泉而觉爽，处涸辙以犹欢。北海虽赊，扶摇可接；东隅已逝，桑榆非晚。孟尝高洁，空余报国之情；阮籍猖狂，岂效穷途之哭！勃，三尺微命，一介书生。无路请缨，等终军之弱冠；有怀投笔，慕宗悫之长风。舍簪笏于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨陪鲤对；今兹捧袂，喜托龙门。杨意不逢，抚凌云而自惜；钟期既遇，奏流水以何惭？  \n各位，祝好。  \nSincerely,LiarLee\n","categories":[],"tags":[]}]