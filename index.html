<!DOCTYPE html><html lang="zh-CN" data-theme="dark"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Liarlee's Notebook - Writing Down ...</title><meta name="author" content="Liarlee"><meta name="copyright" content="Liarlee"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#0d0d0d"><meta name="description" content="Archlinux User, Support Engineer">
<meta property="og:type" content="website">
<meta property="og:title" content="Liarlee&#39;s Notebook">
<meta property="og:url" content="https://liarlee.site/index.html">
<meta property="og:site_name" content="Liarlee&#39;s Notebook">
<meta property="og:description" content="Archlinux User, Support Engineer">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://liarlee.site/img/avatar.jpg">
<meta property="article:author" content="Liarlee">
<meta property="article:tag" content="Linux">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://liarlee.site/img/avatar.jpg"><link rel="shortcut icon" href="/img/logo.png"><link rel="canonical" href="https://liarlee.site/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"prismjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Liarlee\'s Notebook',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2023-12-23 16:40:00'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">135</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">66</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">16</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="Liarlee's Notebook"><img class="site-icon" src="/img/logo.png"/><span class="site-name">Liarlee's Notebook</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/22/Database/Databases_Redis-note/" title="Redis 笔记">Redis 笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-22T04:54:23.000Z" title="发表于 2023-12-22 12:54:23">2023-12-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.720Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">ElastiCache主要概念
ElastiCache nodesA node is the smallest building block of an ElastiCache deployment.node is a fixed-size chunk of secure, network-attached RAM.总结来说， 就是ec2实例上面跑了相同版本的Engine for Redis。

ElastiCache for Redis shards 
A Redis shard (called a node group in the API and CLI) is a grouping of one to six related nodes. A Redis (cluster mode disabled) cluster always has one shard.Redis (cluster mode enabled) clusters can have up to 500 shards, with your data partitioned across the shards.
Shard 是多个 Nodes 的集合， 或者叫作节点组. 一个Shard包括的节点有不同的角色， 一个Primary 和 最多5个 Replica.

ElastiCache for Redis clustersA Redis cluster is a logical grouping of one or more ElastiCache for Redis shards. Data is partitioned across the shards in a Redis (cluster mode enabled) cluster.集群模式关闭， 1 Cluster – 1 Shard – 1 - 6 Nodes: 1 Primary and 5 Nodes集群模式开启， 1 Cluster – 500 Shard – 1 - 6 Nodes: 1 Primary Per Shard

ElastiCache for Redis replicationRedis (cluster mode disabled) clusters support one shard (in the API and CLI, called a node group).Redis (cluster mode enabled) clusters can have up to 500 shards, with your data partitioned across the shards. 
每个Replica都是Primary中全部数据的复制。 Replica使用异步的方式进行与Primary的数据同步。 
第一次 psync2 全量复制所有内存当前存储的数据， 后续使用redis的增量复制。 

AWS Regions and availability zones目前中国区不能使用 GlobalStore， 并且Region之间也是无法相互同步数据的。 在不同的az 之间可以做高可用来降低停机的时间。 

ElastiCache for Redis endpointsAn endpoint is the unique address your application uses to connect to an ElastiCache node or cluster.

Redis 禁用集群模式的单节点端点单节点的集群端点用于读取和写入. In one word, Single Primary, read and write at one point.用这个节点的 Endpoint 即可。

Redis 禁用集群模式的多节点端点多节点有两个不同的端点， Primary 端点总是连接到Primary 节点，故障切换会切换这个dns指向的后端。Reader Endpoint 在所有的 Replica节点之间分流Connection 用来提供读取行为。


用控制台上面的 Primary Endpoint 和 Reader Endpoint

Redis 启用集群模式的端点集群模式下， 只是配置一个endpoint， 但是连接到Endpoint之后， 客户端可以发现所有的主节点和从节点。

** 用Configuration Endpoint 可以找到集群内部的每一个 Primary 和 Replica节点 **

ElastiCache parameter groupsCache parameter groups are an easy way to manage runtime settings for supported engine software. Parameters are used to control memory usage, eviction policies, item sizes, and more.
简单来说， 创建每一个配置文件在aws的控制台上， 可以调整一部分Redis的参数， 然后可能在集群内可以实时生效， 也可能需要触发一次Failover才能生效。elasticache的参数管理 
通常来说， 改动了参数组中的几个参数可能会需要重启节点： 

activerehashing
databases


ElastiCache for Redis securityPENDING… Maybe follow offical guide.

ElastiCache security groups创建集群的时候需要指定一个安全组， 需要放行 6379 port， DONE。

ElastiCache subnet groups子网组是运行 Redis 的所有子网的集合， 创建一个子网， 让托管的节点的把ENI放在这个子网里面， 私有子网。 

ElastiCache for Redis backups备份特定时间的数据， 可以使用这个用来还原成新的集群， 或者是创建一个集群的种子， 备份中包括所有的data， 和一部分集群的metadata。bgsave 或者 forkless save, 这两个取决于是否有足够的内存空间， 当空间充足的时候使用的默认的 bgsave， 但是肯定的是， bhsave 会有性能的开销，所以最好是使用副本节点来创建备份或者快照。对于forkless本身是另一个机制，但是forkless会尝试delay 写入的延迟，从而确保不会积累过多的change 导致内存压力过大失败。 

ElastiCache eventsPENDING… Maybe follow offical guide.


Redis 缓存穿透， 击穿 和 雪崩穿透主要是指异常的查询请求， 缓存未命中的场景下直接访问后端， 后端承载了大量请求导致的崩溃。目前看到需要两个条件： 

数据在缓存层未命中。 
大量的请求转移到了后端。
请求的数据可能在缓存层 以及 后端都不存在。

避免的方法可能有：

在redis 前面添加一个 BloomFilter， 在请求进来的时候进行过滤， 对于异常的请求直接拒绝或者返回一个NULL。
提前缓存一部分数值为 NULL 的数据， 在缓存层直接接下这些请求。这个本身是有点问题的。

击穿这个概念是指的 少部分热点数据被大量的查询时候，  数据突然从缓存中消失， 可能是过期也可能删除等等， 这样有大量的相同请求发送到后端。解决方案： 

设置部分数据永远不过期。 
后端进行加分布式锁。

雪崩批量的热数据过期， 这时候大部分的请求可能还在继续尝试查询不同的数据，直接将这些请求发送到了后端的数据库， 数据库崩了。解决方案： 

创建多个实例， 和副本来提高可用性。 
后端加锁限流。
调整过期时间， 让不同的数据过期的时间错开，防止批量的key 相同的时间过期。


总结起来就是， 避免缓存在特定的场景下失效， 想各种方法保护缓存的可用性。 

More NoteCOB: Client Output Buffer,   Primary 向 Replica 同步数据使用， Max Memory 没有计算这部分的内存， 使用的是Redis引擎之外的内存（os管理的内存）。 
Policy ： LRU 不严格， 随机选择三个。maxmemory-sample: 3， 可以调大。 sample越大， 信号的时间越长。 
Reclaim策略里面， 每秒扫描200个key， 如果数据量比较大的话， Redis会进入loop阶段， 反复尝试Reclaim到25% 以下。 
可以尝试使用 scan 命令扫描全部的数据， 遍历所有的key，这样会触发lazyway主动进行回收。扫描的指令： 
scan 0 match * count 20000

LRU vs LFU

为了保持主从一致， primary 上过期或者删除的key， 会同步发送一个显式的 delete 指令，  这个会统计在replica的settypecmd。
snapshot里面的数据在恢复的时候，会检查key 的ttl ， 如果超过会直接删除。在Redis内存与物理内存mapping由操作系统的虚拟内存管理器 和 指定的内存分配器进行处理。
关于Info指令的所有参数的解释：https://redis.io/commands/info/
localhost:6379> info memory
# Memory
used_memory:938464
used_memory_human:916.47K # Redis 存储数据的使用量
used_memory_rss:10747904
used_memory_rss_human:10.25M # Redis 常住内存集的用量
used_memory_peak:1134048
used_memory_peak_human:1.08M # Redis 的内存峰值
used_memory_peak_perc:82.75% 
used_memory_overhead:888488
used_memory_startup:865904
used_memory_dataset:49976
used_memory_dataset_perc:68.88%
allocator_allocated:1479168  # 内存分配器已经分配的量
allocator_active:1601536
allocator_resident:4575232 
total_system_memory:8182751232
total_system_memory_human:7.62G # 系统内存的总量
used_memory_lua:31744
used_memory_vm_eval:31744
used_memory_lua_human:31.00K # Lua 引擎的内存使用量
used_memory_scripts_eval:0
number_of_cached_scripts:0
number_of_functions:0
number_of_libraries:0
used_memory_vm_functions:32768
used_memory_vm_total:64512
used_memory_vm_total_human:63.00K
used_memory_functions:184
used_memory_scripts:184
used_memory_scripts_human:184B
maxmemory:0  # Max Memory 当前的配置
maxmemory_human:0B
maxmemory_policy:noeviction
allocator_frag_ratio:1.08
allocator_ ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/22/Linux/Linux_CFS%20Schedler/" title="CFS 调度器资料">CFS 调度器资料</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-22T04:51:25.000Z" title="发表于 2023-12-22 12:51:25">2023-12-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.722Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">https://docs.kernel.org/scheduler/sched-design-CFS.htmlhttp://arthurchiao.art/blog/linux-cfs-design-and-implementation-zh/#11-cfs%E8%BF%9B%E7%A8%8Btask%E7%9A%84%E5%85%AC%E5%B9%B3%E8%B0%83%E5%BA%A6https://danluu.com/cgroup-throttling/https://heapdump.cn/article/4235306
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/22/Linux/Linux_Iptables-and-conntrack/" title="追踪数据包经过的iptables规则">追踪数据包经过的iptables规则</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-22T04:49:30.000Z" title="发表于 2023-12-22 12:49:30">2023-12-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.734Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">https://stackoverflow.com/questions/47645859/meaning-of-modules-instances-in-proc-modules
Lsmod命令基于manpage的说明， 这个命令的数据来源是 ： &#x2F;proc&#x2F;modules
[root@centos ~]# lsmod
Module                  Size  Used by
iptable_nat            12875  0
nf_conntrack_ipv4      19149  1
nf_defrag_ipv4         12729  1 nf_conntrack_ipv4
nf_nat_ipv4            14115  1 iptable_nat
nf_nat                 26583  1 nf_nat_ipv4
nf_conntrack          143360  3 nf_nat,nf_nat_ipv4,nf_conntrack_ipv4
iptable_filter         12810  0
nfit                   59735  0
libnvdimm             163620  1 nfit
iosf_mbi               15582  0
crc32_pclmul           13133  0
ghash_clmulni_intel    13273  0
aesni_intel           189456  0
ppdev                  17671  0
lrw                    13286  1 aesni_intel
gf128mul               15139  1 lrw
glue_helper            13990  1 aesni_intel
ablk_helper            13597  1 aesni_intel
cryptd                 21190  3 ghash_clmulni_intel,aesni_intel,ablk_helper
parport_pc             28205  0
parport                46395  2 ppdev,parport_pc
i2c_piix4              22401  0
pcspkr                 12718  0
ip_tables              27126  2 iptable_filter,iptable_nat
xfs                  1014152  1
libcrc32c              12644  3 xfs,nf_nat,nf_conntrack
crct10dif_pclmul       14307  0
crct10dif_common       12595  1 crct10dif_pclmul
crc32c_intel           22094  1
nvme                   32382  1
serio_raw              13434  0
ena                    96895  0
nvme_core              63547  3 nvme
sunrpc                366617  1

&#x2F;proc&#x2F;modules 的输出结果
[root@centos ~]# cat /proc/modules
iptable_nat 12875 0 - Live 0xffffffffc0661000
nf_conntrack_ipv4 19149 1 - Live 0xffffffffc065b000
nf_defrag_ipv4 12729 1 nf_conntrack_ipv4, Live 0xffffffffc05fd000
nf_nat_ipv4 14115 1 iptable_nat, Live 0xffffffffc05f4000
nf_nat 26583 1 nf_nat_ipv4, Live 0xffffffffc0603000
nf_conntrack 143360 3 nf_conntrack_ipv4,nf_nat_ipv4,nf_nat, Live 0xffffffffc0634000
iptable_filter 12810 0 - Live 0xffffffffc05dd000
nfit 59735 0 - Live 0xffffffffc066d000
libnvdimm 163620 1 nfit, Live 0xffffffffc060b000
iosf_mbi 15582 0 - Live 0xffffffffc05ed000
crc32_pclmul 13133 0 - Live 0xffffffffc05e5000
ghash_clmulni_intel 13273 0 - Live 0xffffffffc059e000
aesni_intel 189456 0 - Live 0xffffffffc05ad000
ppdev 17671 0 - Live 0xffffffffc05a4000
lrw 13286 1 aesni_intel, Live 0xffffffffc0575000
gf128mul 15139 1 lrw, Live 0xffffffffc0590000
glue_helper 13990 1 aesni_intel, Live 0xffffffffc0570000
ablk_helper 13597 1 aesni_intel, Live 0xffffffffc0418000
cryptd 21190 3 ghash_clmulni_intel,aesni_intel,ablk_helper, Live 0xffffffffc0569000
parport_pc 28205 0 - Live 0xffffffffc0596000
parport 46395 2 ppdev,parport_pc, Live 0xffffffffc0583000
i2c_piix4 22401 0 - Live 0xffffffffc057c000
pcspkr 12718 0 - Live 0xffffffffc0466000
ip_tables 27126 2 iptable_nat,iptable_filter, Live 0xffffffffc045e000
xfs 1014152 1 - Live 0xffffffffc0470000
libcrc32c 12644 3 nf_nat,nf_conntrack,xfs, Live 0xffffffffc0459000
crct10dif_pclmul 14307 0 - Live 0xffffffffc0438000
crct10dif_common 12595 1 crct10dif_pclmul, Live 0xffffffffc046b000
crc32c_intel 22094 1 - Live 0xffffffffc041d000
nvme 32382 1 - Live 0xffffffffc040f000
serio_raw 13434 0 - Live 0xffffffffc0407000
ena 96895 0 - Live 0xffffffffc0440000
nvme_core 63547 3 nvme, Live 0xffffffffc0427000
sunrpc 366617 1 - Live 0xffffffffc03ac000

调试 Iptables 的方法Kubernetes 里面大量的使用了iptables 规则的各种转换，可以使用iptables的trace功能进行追踪， 查看具体经过了哪些链表。
iptables -t raw -A PREROUTING -p tcp -s 172.31.47.174/32 -j TRACE
or
iptables -t raw -A PREROUTING -p tcp -d 172.31.71.167 --dport 8090 -j TRACE

modprobe ipt_LOG ip6t_LOG nfnetlink_log
modprobe nf_log_ipv4
xtables-monitor  --trace
完成之后就可以在 message 里面看到记录了。 
kern  :warn  : [Thu Sep 21 15:01:24 2023] TRACE: raw:PREROUTING:policy:2 IN=eth3 OUT= MAC=02:19:e6:40:fc:c8:02:55:96:c9:21:66:08:00 SRC=172.31.47.174 DST=172.31.55.27 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=26039 DF PROTO=TCP SPT=22896 DPT=8090 SEQ=1763415057 ACK=0 WINDOW=62727 RES=0x00 SYN URGP=0 OPT (020423010402080ADBFD1C1B0000000001030307)
kern  :warn  : [Thu Sep 21 15:01:24 2023] TRACE: mangle:PREROUTING:policy:4 IN=eth3 OUT= MAC=02:19:e6:40:fc:c8:02:55:96:c9:21:66:08:00 SRC=172.31.47.174 DST=172.31.55.27 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=26039 DF PROTO=TCP SPT=22896 DPT=8090 SEQ=1763415057 ACK=0 WINDOW=62727 RES=0x00 SYN URGP=0 OPT (020423010402080ADBFD1C1B0000000001030307)
kern  :warn  : [Thu Sep 21 15:01:24 2023] TRACE: nat:PREROUTING:rule:1 IN=eth3 OUT= MAC=02:19:e6:40:fc:c8:02:55:96:c9:21:66:08:00 SRC=172.31.47.174 DST=172.31.55.27 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=26039 DF PROTO=TCP SPT=22896 DPT=8090 SEQ=1763415057 ACK=0 WINDOW=62727 RES=0x00 SYN URGP=0 OPT (020423010402080ADBFD1C1B0000000001030307)
kern  :warn  : [Thu Sep 21 15:01:24 2023] TRACE: nat:KUBE-SERVICES:return:45 IN=eth3 OUT= MAC=02:19:e6:40:fc:c8:02:55:96:c9:21:66:08:00 SRC=172.31.47.174 DST=172.31.55.27 LEN=60 TOS=0x00 PREC=0x00 TTL=64 ID=26039 DF PROTO=TCP SPT=22896 DPT=809 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/22/Linux/Linux_iostat/" title="Iostat 参数说明">Iostat 参数说明</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-22T04:44:27.000Z" title="发表于 2023-12-22 12:44:27">2023-12-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.744Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">
从manpage 里面直接获取的命令说明， 汇报CPU指标 和 设备 或者 分区 的 I&#x2F;O指标。       iostat - Report Central Processing Unit (CPU) statistics and input&#x2F;output statistics for devices and partitions.

指标的解释说明如下：rrqm&#x2F;s - wrqm&#x2F;s  两个看的是的merge的请求数量， 表示发送给驱动程序并被驱动程序合并的请求数量，表示有没有进行合并，这也表示系统将随机IO请求合并成连续以提高性能。r&#x2F;s - w&#x2F;s  IO读写的请求数量IOPS， 发送给磁盘设备的请求数。rKB&#x2F;s - wKB&#x2F;s 吞吐量， 可以使用 -m 转换成MB&#x2F;s， 设备的传输数据量的吞吐量信息。avgrq-sz  平均请求大小， 也就是IOsize 的大小。单位是扇区（512B）。rareq-sz -  wareq-sz 新版本的iostat已经是这两个指标了， 这两个指标单位是 KB.avgqu-sz  在驱动队列和设备队列中或活跃的平均请求数量。 单位是 个。await 这个指标记录的总的延迟， 新的版本也包括 rw 两类请求的延迟。 延迟单位是 ms ，通常不应该超过10ms.
命令输出结果：
iostat -dx 3

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
nvme0n1       2147.33  34842.67     0.00   0.00    0.46    16.23    0.33      1.33     0.00   0.00    1.00     4.00    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    1.00 100.00

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
nvme0n1       2128.33  34248.00     0.00   0.00    0.46    16.09    5.33     49.83     1.33  20.00    0.94     9.34    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.99  99.87

iostat -dcpx 5
Linux 6.4.10-arch1-1 (arch.liarlee.site) 	08/22/2023 	_x86_64_	(4 CPU)

avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           5.23    0.00    2.05    0.01    0.01   92.71

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
nvme0n1          1.00     32.72     0.00   0.04    0.66    32.86    1.24     23.27     0.18  12.62    0.99    18.79    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.18
nvme0n1p1        1.00     32.72     0.00   0.04    0.66    32.86    1.24     23.27     0.18  12.62    0.99    18.79    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.18


avg-cpu:  %user   %nice %system %iowait  %steal   %idle
           1.80    0.00    0.95    0.05    0.00   97.20

Device            r/s     rkB/s   rrqm/s  %rrqm r_await rareq-sz     w/s     wkB/s   wrqm/s  %wrqm w_await wareq-sz     d/s     dkB/s   drqm/s  %drqm d_await dareq-sz     f/s f_await  aqu-sz  %util
nvme0n1          0.00      0.00     0.00   0.00    0.00     0.00    1.60     10.40     0.00   0.00    0.62     6.50    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.32
nvme0n1p1        0.00      0.00     0.00   0.00    0.00     0.00    1.60     10.40     0.00   0.00    0.62     6.50    0.00      0.00     0.00   0.00    0.00     0.00    0.00    0.00    0.00   0.34

监控数据来源： cat /proc/diskstats这些所有的指标都是By Device的， 与进程的部分无关，统计的是发送到设备或者发送到设备分区的请求。基于man page 中的描述， 这四个参数分别是：     r&#x2F;s    The number (after merges) of read requests completed per second for the device.     w&#x2F;s    The number (after merges) of write requests completed per second for the device.     d&#x2F;s    The number (after merges) of discard requests completed per second for the device.     f&#x2F;s    The number (after merges) of flush requests completed per second for the device.  This counts flush requests executed by disks. Flush requests are not tracked for partitions.  Before being merged, flush operations are counted as writes.
命令行参数的一部分解释-d 展示磁盘设备使用率的信息。-x 扩展显示更多的指标数据， 不添加这个参数不区分设备 读&#x2F;写&#x2F;磁盘&#x2F;文件系统 这四个维度。-c 查看CPU统计的使用率指标。-p 磁盘分区的指标。-z 只是输出一些有IO的设备的统计信息。 性能之巅里面有写出这个参数。 
使用这个命令的思路： 

看设备， 具体是那个设备上面有IO请求。
看await，等待的时间长不长， 这里面的等待时间是从请求进入设备队列到请求返回成功的时间 ， 通常情况下应该是在10ms以内， 尤其是 ssd。
看 q-sz， 按照EBS的规则，说法， 应该是每 1000 IOPS 队列数为 1。 
综合看 IOPS， 块大小， 吞吐量，  使用率， 确认设备是不是达到了瓶颈。 
最后切换工具定位具体是那个进程在大量的发送IO请求 ，大概可能的工具是 pidstat， htop ， atop ， sar 等等， 感觉工具还可能有 pm 的这部分工具。

通常可以直接使用命令 ： iostat -xz 1 
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/22/Linux/Linux_vmstat/" title="vmstat 参数说明">vmstat 参数说明</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-22T04:44:27.000Z" title="发表于 2023-12-22 12:44:27">2023-12-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.747Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">命令说明vmstat 提供的信息是从硬件和物理参数的角度。vmstat 第一次的数据显示的是从上一次重启到现在的平均值，所以只能用来参考。标题内容就不在解释了，分别是 进程 ， 内存， 交换 ， IO， 系统 ， 和 CPU。
详细参数如下：  r - 等待运行的进程数量,  多少个进程在R状态， 并一直取得并占用CPU时间片。  b - Uninterreptable Sleep的进程数量， 有多少个处于D状态显示的进程， 通常代表有多少个进程正常等待IO资源。  swpd： VirtualMemory的值 （KB）, 交换的使用量，如果未开启交换分区就会显示是 0。 这个指标和后面的 si so 是整体的， swpd 有用量， 那么si&#x2F;so 就有数据， swpd 是 0， 那么永远不会有 si&#x2F;so。  free： IdleMemory的值（KB), 完全可用的内存，  这里free的内存是 完全没有被分配的。total - buffer - cache - used 计算完了之后剩下的, 与 free 命令中的 free 字段是同样的含义。  buff： 被buffer使用的内存量（KB），被 buff 使用的内存空间。  cache： 被cache使用的内存量（KB）， 被 cache 占用的内存空间， 这部分和buffer 一样， 是临时数据，在落盘后buffer 会被释放， cache可以直接被释放。  si： Swapin 内存从硬盘换入的数据量 （KB&#x2F;s)， 读取交换分区（disk）到 内存 的数据量 。  so：Swapout 内存交换到硬盘的数据量 （KB&#x2F;s）， 写入磁盘交换分区的数据量。  bi： blockin， 发送到块设备的块数量（blocks&#x2F;s),   系统收到disk发送来的数据量， 也就是通常说的读磁盘数据。  bo： blockout ， 从块设备接收的块数量（blocks&#x2F;s）， 系统写入disk的数据量， 也就是通常说的写磁盘数据。  in： 包括时钟在内的那每秒中断数。  cs： 进程上下文每秒切换的次数。  us：非内核代码使用的CPU百分比， 包括用户时间和Nice时间。  sy：内核代码使用的CPU时间百分比。  id： CPU的空闲时间百分比。  wa：等待IO操作花费的时间百分比。 这个容易误解的是， wa 其实cpu 是空闲的， 统计的方式是， 在收集数据的时候有多少个进程处于需要处理io或者等待处理io的状态， 当时那个时刻的百分比， 而实际上等待io的时间其实CPU没有做事， 所以在CPU的度量上， 这个时间的CPU是可以做其他事情的， 可以被其他需要进行计算的进程使用掉这部分时间。  st：虚拟化层操作花费的时间百分比。这个通常是指虚拟化层的限制。 这个指标如果有， 那么表示虚拟化层对cpu的使用进行了限制（EC2 T系列实例） 或者是 底层的资源并不完全满足需求而导致的争抢。  gu： Time spent running KVM guest code (guest time, including guest nice). ， 这个是新添加的指标， 之前没有见过。 如果是KVM host os 上面这个指标还是比较有用的。  
输出结果如下：   ~ vmstat -w 1
--procs-- -----------------------memory---------------------- ---swap-- -----io---- -system-- ----------cpu----------
   r    b         swpd         free         buff        cache   si   so    bi    bo   in   cs  us  sy  id  wa  st  gu
   2    0      1005492        42600         1328      1973356    4    7    77   312 1767    6   2   2  95   1   0   0
   1    0      1005492        42652         1328      1973360    0    0     0     0 2519 3373   1   2  97   0   0   0
   1    0      1005492        42652         1328      1973360    0    0     0     8 1001 1844   0   1  99   0   0   0
   1    0      1005492        42652         1328      1973360    0    0     0    12 1056 1822   1   0  99   0   0   0
   1    0      1005492        42652         1328      1973368    0    0     0     0 1308 2150   1   1  98   0   0   0
   1    0      1005492        43156         1328      1973376    0    0     0     0 1394 2245   3   1  97   0   0   0
   1    0      1005492        43156         1328      1973376    0    0     0     0 1125 1898   1   1  99   0   0   0
   1    0      1005492        43408         1328      1973376    0    0     0    55 1229 2218   1   1  99   0   0   0
   1    0      1005492        44948         1328      1973392    0    0     0     0 3544 5490   7   4  89   0   0   0
   1    0      1005492        45228         1328      1973428    0    0     0     0 1410 2082   3   4  93   0   0   0分析指标的逻辑输出之后 先看前几次输出的 r 和 b。  r 有没有超过核心数；  b 这里是不是有数据， 数据在运行命令的周期是不是一直存在， 数量有没有超过CPU核心数。变化的频率的幅度是什么样的。    例子：    总共观察10s ， 每秒输出一次数据， 每秒都有 超过cpu 核心数的 不可中断进程， 那么这代表当前的操作系统可能存在大量io进程，被阻塞的IO进程比较多 。 
然后看最后一列的CPU情况， id的数据是多少，如果这个时候id 比较小， 看 us&#x2F;sy&#x2F;wa ，cpu 把时间花费在了什么地方。通常的场景下， cpu 应该把时间尽可能的使用在 us 这个部分。 如果其他而部分比较多的就需要关注较多的那个部分了。 
有交换就看看交换， 没有交换直接看内存相关， cache + free 总共有多少。如果内存用量是不是在oom 的边缘 ，然后再看 in&#x2F;cs，  一个是硬中断次数， 一个是进程上下文切换次数， 这两个一个可能代表潜在的cpu被硬件事件终止， 另一个cpu忙于在进程之间反复横跳。 这些都可能指向当前系统的效率并不高， 或者潜在的问题。 
命令示例   vmstat -m – 显示内核占用内存的分配情况。   vmstat -a – 将内存的使用量分为活动内存和非活动内存。   vmstat -n 2 10 – 展示vmstat的结果10次， 每两秒一次。
更多参考https://docs.oracle.com/cd/E19455-01/805-7229/6j6q8svh5/index.html
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/22/AWS/EKS_Apply%20%E6%8F%90%E7%A4%BA%20TooLong/" title="Kubectl Apply 报错 annotation Too long">Kubectl Apply 报错 annotation Too long</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-21T16:53:54.000Z" title="发表于 2023-12-22 00:53:54">2023-12-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.714Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">重装Prometheus operator 的时候报错， 提示annotation 太长了， 不能 apply
> kubectl apply -f ./setup
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/scrapeconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
Warning: Detected changes to resource monitoring which is currently being deleted.
namespace/monitoring unchanged
Error from server (Invalid): error when creating "setup/0prometheusCustomResourceDefinition.yaml": CustomResourceDefinition.apiextensions.k8s.io "prometheuses.monitoring.coreos.com" is invalid: metadata.annotations: Too long: must have at most 262144 bytes
Error from server (Invalid): error when creating "setup/0prometheusagentCustomResourceDefinition.yaml": CustomResourceDefinition.apiextensions.k8s.io "prometheusagents.monitoring.coreos.com" is invalid: metadata.annotations: Too long: must have at most 262144 bytes

解决方案使用 Create 或者 Replace> kubectl create -f ./setup
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusagents.monitoring.coreos.com created
Error from server (AlreadyExists): error when creating "setup/0alertmanagerConfigCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "alertmanagerconfigs.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/0alertmanagerCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "alertmanagers.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/0podmonitorCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "podmonitors.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/0probeCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "probes.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/0prometheusruleCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "prometheusrules.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/0scrapeconfigCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "scrapeconfigs.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/0servicemonitorCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "servicemonitors.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/0thanosrulerCustomResourceDefinition.yaml": customresourcedefinitions.apiextensions.k8s.io "thanosrulers.monitoring.coreos.com" already exists
Error from server (AlreadyExists): error when creating "setup/namespace.yaml": object is being deleted: namespaces "monitoring" already exists

使用create命令去创建crd ，使用replace更新crd，他们都不添加 last-applied-configuration  这个字段，
> kubectl replace -f ./setup
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/prometheusagents.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/scrapeconfigs.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com replaced
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com replaced
namespace/monitoring replaced
SSA 的方式创建当然还有另一个方法就是使用server-side apply： 
> kubectl apply --server-side -f ./setup

https://kubernetes.io/docs/reference/using-api/server-side-apply/
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/21/AWS/Linux_UseNvidiaT4/" title="archlinux 配置 xorg 使用 nvidia T4">archlinux 配置 xorg 使用 nvidia T4</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-21T05:56:27.000Z" title="发表于 2023-12-21 13:56:27">2023-12-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.717Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">最近的一个想法，基于之前 hacking 到中国区域的 archlinux， 可以尝试直接改改 xorg， 用用 nvidia 的显卡。
大概折腾了一天， 记录一下步骤和过程。
之前使用的是 Xorg + DWM 的简单架构， 软件非常少。那么在这个软件的基础上启用显卡和配置xorgserver 使用显卡， 基本上就是这两部分。
安装显卡驱动
查看显卡信息> lspci
00:1e.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
安装驱动直接参考archwiki ， 一条命令搞定， 走dkms。安装的这个版本的闭源驱动。> pacman -Ss nvidia-dkms
extra/nvidia-dkms 545.29.06-1 [installed]
    NVIDIA drivers - module sources

> pacman -S nvidia-utils
等pacman自己处理完dkms的模块编译之后， 重启os就可以看到nvidia模块被装载。> lsmod | grep nvidia
nvidia_uvm           3481600  0
nvidia_drm            118784  7
nvidia_modeset       1585152  6 nvidia_drm
nvidia              62402560  141 nvidia_uvm,nvidia_modeset
video                  77824  1 nvidia_modeset
> modinfo nvidia
filename:       /lib/modules/6.6.7-zen1-1-zen/updates/dkms/nvidia.ko.zst
alias:          char-major-195-*
version:        545.29.06
supported:      external
license:        NVIDIA
firmware:       nvidia/545.29.06/gsp_tu10x.bin
firmware:       nvidia/545.29.06/gsp_ga10x.bin
srcversion:     8302209549E8FEAC029EDC0
alias:          pci:v000010DEd*sv*sd*bc06sc80i00*
alias:          pci:v000010DEd*sv*sd*bc03sc02i00*
alias:          pci:v000010DEd*sv*sd*bc03sc00i00*
depends:
retpoline:      Y
name:           nvidia
vermagic:       6.6.7-zen1-1-zen SMP preempt mod_unload

配置xrdp
这里已经可以正常的驱动显卡， 但是xorg不会去调用显卡启动图形，查看了archwiki之后，发现如果使用 xrdp ， 那么还需要重新安装 xrdp 的后端。 需要使用来自aur的后端来匹配nvidia显卡的支持， 而不是 archlinux 的主仓库里面的默认软件包， 我这边使用的是paru 进行的下面步骤。 aur 里面的 xorgxrdp，会自动拉取编译 xrdp-git， 因此这里的版本显示和 aur 仓库的不完全一致。> paru -Ss xrdp
aur/xorgxrdp-nvidia 0.2.18.r55.g3a4d465-1 [+4 ~0.00] [Installed]
    Xorg drivers for xrdp, with NVIDIA GPU support.
aur/xrdp-git 0.9.18.r565.geb1c3cd4-1 [+30 ~0.01] [Installed: 0.9.18.r599.g9fbe0ad1-1]
    An open source remote desktop protocol (RDP) server. Git version, devel branch.
安装完成之后， 需要重启一下系统。
编译安装aur的过程中，最后的步骤里面有一个红色的提示文字， 需要在配置中变更一下xrdp读取的xorg配置文件的位置。> cat -n /etc/xrdp/sesman.ini
   130  [Xorg]
   143  param=Xorg
   144  ; Leave the rest parameters as-is unless you understand what will happen.
   145  param=-config
        ; 下面的这两个， 启用nvida的配置文件，注释默认的。
        ; 这里配置的是启动xorg server 的时候读取的配置文件位置 以及 xorg server 的参数。
   146  ; param=xrdp/xorg.conf
   147  param=xrdp/xorg_nvidia.conf
   148  param=-noreset
   149  param=-nolisten
之后， 使用 nvidia 提供的 util ， 生成一份nvidia显卡的的配置文件， merge 配置文件里面的这份。> nvidia-xconfig -c /etc/X11/xrdp/xorg_nvidia.conf
# 这里面可以不手动备份， 该命令会自动备份之前的配文件， 如果有不同的配置会自动merge。
最后，我在这样做完之后， 重启 xrdp ， xorg 确实开始使用显卡了， 但是所有启动的程序不会使用显卡， 默认还是CPU在计算， 这是因为 xorg 的配置文件里面少了 FILE Section, 这个文件的位置可能会不同，我这边确实在这个位置， 文件存在的情况下直接拿来用了。Section "Files"
  ModulePath   "/usr/lib64/nvidia/xorg"
  ModulePath   "/usr/lib64/xorg/modules"
EndSection
参考了这个网页中的配置：  https://forums.developer.nvidia.com/t/glxinfo-command-returning-badwindow-invalid-window-parameter-error/36172

最终的配置文件内容如下：&#x2F;etc&#x2F;X11&#x2F;xrdp&#x2F;xorg_nvidia.conf
> cat -n /etc/X11/xrdp/xorg_nvidia.conf
     1  Section "ServerLayout"
     2  Identifier "XRDP GPU Server"
     3    Screen 0 "dGPU"
     4    InputDevice "xrdpMouse" "CorePointer"
     5    InputDevice "xrdpKeyboard" "CoreKeyboard"
     6  EndSection
     7
     8  Section "ServerFlags"
     9    # This line prevents "ServerLayout" sections in xorg.conf.d files
    10    # overriding the "XRDP GPU Server" layout (xrdp #1784)
    11    Option "DefaultServerLayout" "XRDP GPU Server"
    12    Option "DontVTSwitch" "on"
    13    Option "AutoAddDevices" "off"
    14  EndSection
    15
    16  Section "Files"
    17    ModulePath   "/usr/lib64/nvidia/xorg"
    18    ModulePath   "/usr/lib64/xorg/modules"
    19  EndSection
    20
    21  Section "Module"
    22    Load "xorgxrdp"
    23  EndSection
    24
    25  Section "InputDevice"
    26    Identifier "xrdpKeyboard"
    27    Driver "xrdpkeyb"
    28  EndSection
    29
    30  Section "InputDevice"
    31    Identifier "xrdpMouse"
    32    Driver "xrdpmouse"
    33  EndSection
    34
    35  Section "Screen"
    36    Identifier "dGPU"
    37    Device "dGPU"
    38    Option "DPI" "96 x 96"
    39  # T4 needs an entry here, this is not the desktop size
    40    SubSection "Display"
    41      Virtual 1920 1080
    42    EndSubSection
    43  EndSection
    44
    45  Section "Device"
    46    Identifier "dGPU"
    47    Driver "nvidia"
    48  # T4 may need to comment out next line
    49  # Option "UseDisplayDevice" "none"
    50    Option "ConnectToAcpid" "false"
    51    BusID "PCI:0:30:0"
    52  EndSection


排查思路 和 日志的位置。
首先查看 xrdp 的 status， 是不是正常。
排查日志 &#x2F;var&#x2F;log&#x2F;xrdp.log ， 这里面记录的是 xrdp 的启动的过程。
如果没有错误， 那么查看 &#x2F;var&#x2F;log&#x2F;xrdp-sesman.log, 这里面会记录xorg启动的命令和 xorg 的状态。[2023-12-21T16:50:17.711+0800] [INFO ] Socket 13: connection accepted from AF_UNIX
[2023-12-21T16:50:17.714+0800] [INFO ] Received system login request from xrdp for user: ec2-user IP: ::ffff:114.251.173.133
[2023-12-21T16:50:17.717+0800] [INFO ] starting xrdp-sesexec with pid 37443
[2023-12-21T16:50:17.763+0800] [INFO ] Terminal ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/08/AWS/EKS_Container-Pod-ENI/" title="查看EKS集群节点上的容器和ENI的对应关系">查看EKS集群节点上的容器和ENI的对应关系</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-08T06:39:16.000Z" title="发表于 2023-12-08 14:39:16">2023-12-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.715Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EKS/">EKS</a></span></div><div class="content">版本信息root@ip-172-31-35-61 ~ [1]# containerd -v
containerd github.com/containerd/containerd v1.6.16 31aa4358a36870b21a992d3ad2bef29e1d693bec.m

root@ip-172-31-35-61 ~# uname -a
Linux ip-172-31-35-61.cn-north-1.compute.internal 6.1.10-arch1-1 #1 SMP PREEMPT_DYNAMIC Mon, 06 Feb 2023 09:28:04 +0000 x86_64 GNU/Linux

root@ip-172-31-35-60 ~ [1]# kubelet --version
Kubernetes v1.24.9-eks-49d8fe8
容器虚拟网卡和节点网卡的关主要的思路是通过veth的id 。
找到 Pod 的 Pause 容器
root@ip-172-31-35-61 ~# nerdctl -n k8s.io ps | grep -v pause
CONTAINER ID    IMAGE                                                                                         COMMAND                   CREATED           STATUS    PORTS    NAMES
218279edbee1    918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/kube-proxy:v1.24.9-minimal-eksbuild.1    "kube-proxy --v=2 --…"    14 hours ago      Up                 k8s://kube-system/kube-proxy-kqlj2/kube-proxy
3a8aaf963501    123456123456.dkr.ecr.cn-north-1.amazonaws.com.cn/haydenarchlinux:latest                       "sleep 365d"              8 minutes ago     Up                 k8s://default/haydenarch-77c4f7cff9-dxsps/haydenarch
进入容器内部查看网卡的ID
root@ip-172-31-35-61 ~# nerdctl -n k8s.io exec -it 3a8aaf963501 fish

root@haydenarch-77c4f7cff9-dxsps /# cat /sys/class/net/eth0/iflink
54

root@haydenarch-77c4f7cff9-dxsps /# ip ad
1: lo: &lt;LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if54: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default
    link/ether 66:3a:fb:f2:00:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.31.39.84/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::643a:fbff:fef2:b2/64 scope link
       valid_lft forever preferred_lft forever
在实例上面查看 网卡的信息， 找到带有 id 54 的虚拟网卡对。
root@ip-172-31-35-61 ~# ip ad | grep 54
54: enid573ff579e6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default
查看这个虚拟网卡的全部信息， 可以看到 link-netns， 这是容器的Network NameSpace。 
&#96;&#96;&#96;shell
root@ip-172-31-35-61 ~ [1]# ip link show enid573ff579e6
54: enid573ff579e6@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc noqueue state UP mode DEFAULT group default
    link&#x2F;ether 36:06:dc:85:42:a4 brd ff:ff:ff:ff:ff:ff link-netns cni-bb451da7-00be-2d59-b5e0-1dd4e77565e8
可以使用下面的命令查看所有的ns并找到对应的id。
root@ip-172-31-35-61 ~# ip netns list
cni-2e29d022-09f4-6626-f416-29cfb9652e78 (id: 13)
cni-e5be29a1-d111-53ec-8f88-94118eb1466c (id: 10)
cni-bb451da7-00be-2d59-b5e0-1dd4e77565e8 (id: 8)  # 这个就是
cni-673e1c4b-acd1-ab7e-b374-1a4192eea86e (id: 6)
cni-1207e72f-e68e-a7e7-161e-979e9e9ada7a (id: 5)
cni-a8b7807f-a37e-dd4e-98cb-9d3e7cd42f11 (id: 4)
cni-08a7a0e2-8e33-b669-83ee-75957676ffbb (id: 3)
cni-8a26c501-2ea9-f4be-36ae-745a5c307fbc (id: 0)
cni-2598e89a-d2ba-38a8-6412-92fbff32871b (id: 9)
cni-817961be-8d28-b32c-816f-a842b9d243fe (id: 2)
cni-a84561aa-8668-8b7c-4b5e-56568e14dc83 (id: 12)
cni-5de27d6e-c311-6ee3-4ed5-85b2a05b6943 (id: 11)
cni-d102a5e6-d6ea-fb90-2ac8-a97cd8df9c85 (id: 7)
cni-88bf7480-bdd6-e063-3103-894d29b16302 (id: 1)
可以直接进入pod 的network ns 查看容器的网络信息， 这和上面在容器内直接看到的信息是一致的。
root@ip-172-31-35-61 ~ [1]# ip netns exec cni-bb451da7-00be-2d59-b5e0-1dd4e77565e8 ip ad
1: lo: &lt;LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if54: &lt;BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default
    link/ether 66:3a:fb:f2:00:b2 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 172.31.39.84/32 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::643a:fbff:fef2:b2/64 scope link
       valid_lft forever preferred_lft forever
如何确定容器的网卡绑定在了哪个ENI上面由于VPC CNI本身的实现， 在EKS的节点上面会出现多的ENI，具体pod的流量是怎么走向eth网卡的，去了eth0 还是 eth1 或者其他的网卡， 需要通过路由条目来确定。和上面的记录时间不同， 所以这个部分的记录，是另一个节点的另一个容器了， ip地址会不一致。
查看容器的IP地址
> kubectl get pods -o wide
NAME                          READY   STATUS    RESTARTS   AGE   IP              NODE                                          NOMINATED NODE   READINESS GATES
haydenarch-7d9ff55cbd-4b7pl   1/1     Running   0          18h   172.31.55.218   ip-172-31-55-30.cn-north-1.compute.internal   &lt;none>           &lt;none>
查看节点的路由表， 看看这个ip地址的路由信息。 
[root@ip-172-31-55-30 ~]# ip rule list | grep 172.31.55.218
512:	from all to 172.31.55.218 lookup main
1536:	from 172.31.55.218 lookup 2
那么上面的结果显示， 这个IP地址的数据包会查 table 2， 查看 table 2
[root@ip-172-31-55-30 ~]# ip r s table 2
default via 172.31.48.1 dev eth1
172.31.48.1 dev eth1 scope link
观察的这个容器现在是在 eth1 上面的， 查看节点的网卡信息。
[root@ip-172-31-55-30 ~]# ip ad s eth1
17 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/08/AWS/EKS_%E6%89%8B%E5%8A%A8%E8%A7%A6%E5%8F%91bootstrap%E8%84%9A%E6%9C%AC%E5%B0%86%E8%8A%82%E7%82%B9%E5%8A%A0%E5%85%A5%E9%9B%86%E7%BE%A4/" title="自管理节点加入集群">自管理节点加入集群</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-08T06:39:16.000Z" title="发表于 2023-12-08 14:39:16">2023-12-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.717Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/EKS/">EKS</a></span></div><div class="content">添加一个自管理的节点
创建集群，启动一个新的 EC2， 登录到已经启动的 EKS 优化 OS 内，准备复制一些脚本过来。

添加EC2的标签： kubernetes.io&#x2F;cluster&#x2F;clusterName  owned

配置EC2的Instance Profile

控制台获取 Kubernetes APIServer的Endpoint URL

获取 apiserver b64 CA : cat ~/.kube/config 这个文件里面可以找到 ，或者是通过EKS的控制台上面， 找到 Base64 的 CA。

编辑 userdata， 或者 ssh 登录到ec2上面创建一个bash脚本用来调用 bootstrap.sh
mkdir ~/eks; touch ~/eks/start.sh
---
#!/bin/bash
set -ex

B64_CLUSTER_CA=
API_SERVER_URL=
K8S_CLUSTER_DNS_IP=10.100.0.10

/etc/eks/bootstrap.sh $&#123;ClusterName&#125; --b64-cluster-ca $&#123;B64_CLUSTER_CA&#125; --apiserver-endpoint $&#123;API_SERVER_URL&#125;

集群里面没有节点组 ， 也不会创建aws-auth configmap 所以节点无法正常的加入集群， 需要手动创建。
apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: [CLUSTER_ROLE]
      username: system:node:&#123;&#123;EC2PrivateDNSName&#125;&#125;
  mapUsers: |
    []
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system

需要复制的文件sudo pacman -S containerd # 安装Containerd
scp /etc/eks/bootstrap.sh root@54.222.253.235:/etc/eks/bootstrap.sh # 复制bootstrap
scp /usr/bin/imds root@54.222.253.235:/usr/bin/imds # shell 脚本， 用来帮忙调用ec2 metadata 获取实例和VPC子网的信息
scp -pr /etc/eks/ root@54.222.253.235:/etc/eks/ # 直接复制了eks的相关脚本和配置模板
scp -pr /var/lib/kubelet/kubeconfig root@54.222.253.235:/var/lib/kubelet/kubeconfig # 复制kubeletconfig配置文件模板
scp -pr /etc/kubernetes/ root@54.222.253.235:/etc/kubernetes/ # 复制 kubernetes 的配置文件
scp -pr /etc/kubernetes/kubelet/ root@54.222.253.235:/etc/kubernetes/kubelet/ # 上面的命令没有递归复制， 所以需要指定

# 设置对应的内核参数， 如果不做kubelet 会报错提示 这些参数不符合要求。
kernel.panic = 10
kernel.panic_on_oops = 1
vm.overcommit_memory = 1

Bootstrap 脚本内容记录一下脚本自动配置的内容， 大概就是 获取变量， aws的服务地址， ec2 元数据地址， 替换模板中的变量生成Kubelet配置文件 和 Containerd 的配置文件（这个替换是一次性的， 也就是说， bootstrap只能变更模板中的变量一次， 第二次执行只会生成刷新一次集群的信息， 以及重启服务）。

读取bootstrap后面给出的参数，设置变量， 例如： ClusterName etc.
查看Kubelet的版本， 决定Runtime， containerd | dockerd, 判断条件是 kubelet 版本 大于 1.24++ kubelet --version
++ grep -Eo '[0-9]\.[0-9]+\.[0-9]+'
+ KUBELET_VERSION=1.24.9
---
+ IS_124_OR_GREATER=true
+ DEFAULT_CONTAINER_RUNTIME=containerd
设置ECR以及Pause容器地址# 获取region以及aws service domain
+ AWS_DEFAULT_REGION=cn-north-1
+ AWS_SERVICES_DOMAIN=amazonaws.com.cn

# 调用脚本 /etc/eks/get-ecr-uri.sh cn-north-1 amazonaws.com.cn ''
+ ECR_URI=918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn
+ PAUSE_CONTAINER_IMAGE=918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/pause
+ PAUSE_CONTAINER=918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/pause:3.5
+ CA_CERTIFICATE_DIRECTORY=/etc/kubernetes/pki
+ CA_CERTIFICATE_FILE_PATH=/etc/kubernetes/pki/ca.crt
创建证书目录：+ mkdir -p /etc/kubernetes/pki
+ sed -i s,MASTER_ENDPOINT,https://CE0253A94B6B14215AE3282580CFA5E3.yl4.cn-north-1.eks.amazonaws.com.cn,g /var/lib/kubelet/kubeconfig
+ sed -i s,AWS_REGION,cn-north-1,g /var/lib/kubelet/kubeconfig
+ sed -i s,CLUSTER_NAME,NewClusterForManualJoin,g /var/lib/kubelet/kubeconfig
获取 VPC CIDR# imds shell script help to get metadata.
imds: /usr/bin/imds
++ imds latest/meta-data/local-ipv4
++ imds latest/meta-data/network/interfaces/macs/02:66:06:2e:48:08/vpc-ipv4-cidr-blocks
创建kubelet 配置, 计算预留的资源 和 MaxPod 等等参数的数值。/etc/kubernetes/kubelet/kubelet-config.json
+ mkdir -p /etc/systemd/system/kubelet.service.d
+ sudo mkdir -p /etc/containerd
+ sudo mkdir -p /etc/cni/net.d
+ sudo mkdir -p /etc/systemd/system/containerd.service.d
创建containerd 配置文件+ printf '[Service]\nSlice=runtime.slice\n'
+ sudo tee /etc/systemd/system/containerd.service.d/00-runtime-slice.conf
+ sudo sed -i s,SANDBOX_IMAGE,918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/pause:3.5,g /etc/eks/containerd/containerd-config.toml
kubelet配置和启动。+ sudo cp -v /etc/eks/containerd/kubelet-containerd.service /etc/systemd/system/kubelet.service
+ sudo chown root:root /etc/systemd/system/kubelet.service
+ sudo containerd config dump
+ systemctl enable kubelet
+ systemctl start kubelet

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/08/CheatSheet_Kubernetes/" title="CheatSheet_Kubernetes">CheatSheet_Kubernetes</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-08T05:57:57.000Z" title="发表于 2023-12-08 13:57:57">2023-12-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.954Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">移除所有失败的podkubectl delete pod --field-selector="status.phase==Failed"

查看证书信息查看 AWS LoadBalancer 证书的信息，检查证书的有效期： 
kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io aws-load-balancer-webhook -ojsonpath=&#123;.webhooks[0].clientConfig.caBundle&#125;  | base64 -d  | openssl x509 -noout -text

使用 Debug 容器# 给特定的容器附加一个Sidecar， 并启动shell。
kubectl debug -it --image=public.ecr.aws/amazonlinux/amazonlinux:latest aws-node-cpmck
# netshoot容器， 比较方便的用来进行网络部分的调试。
# 项目仓库地址： https://github.com/nicolaka/netshoot
kubectl debug mypod -it --image=nicolaka/netshoot
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/08/CheatSheet_Linux/" title="CheatSheet_Linux">CheatSheet_Linux</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-08T05:57:57.000Z" title="发表于 2023-12-08 13:57:57">2023-12-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.955Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">PS Command# Top CPU users
ps auxwww --sort -%cpu | head -20

# Top memory users
ps auxwww --sort -rss | head -20

ps auxf --width=200
or 
ps auxwwwf
Curl Commandcurl -o /dev/null -s -w "time_namelookup:%&#123;time_namelookup&#125;\ntime_connect: %&#123;time_connect&#125;\ntime_appconnect: %&#123;time_appconnect&#125;\ntime_redirect:  %&#123;time_redirect&#125;\ntime_pretransfer:  %&#123;time_pretransfer&#125;\ntime_starttransfer: %&#123;time_starttransfer&#125;\ntime_total: %&#123;time_total&#125;\n"  http://nginx.liarlee.site/Fedora-Workstation-Live-x86_64-38_Beta-1.3.iso
Sysctl Commandsysctl -a | egrep "rmem|wmem|tcp_mem|adv_win|moderate"</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/08/AWS/Opensearch_Filebeat%20%E8%BE%93%E5%87%BA%E6%97%A5%E5%BF%97%E5%88%B0%20Opensearch/" title="Filebeat 输出日志到 Opensearch">Filebeat 输出日志到 Opensearch</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-08T05:57:57.000Z" title="发表于 2023-12-08 13:57:57">2023-12-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.718Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Docker/">Docker</a></span></div><div class="content">这个最后基本上可以确认是一个兼容性问题，测试完成发现， 开启兼容模式的Opensearch+filebeat的组合， filebeat 还是会不定期重启。 

背景需求是，使用ES + filebeat 模式在收集日志。使用Supervisor作为容器的主进程管理工具，启动后分别运行 应用（这里用nginx代替） + filebeat
现在想要用ECS Fargate， 然后依旧还是这个模式， 尽可能新的变动之前的架构， ES 替换成 OpenSearch。
按照这个路数测试。 
创建Opensearch版本：OpenSearch 2.11 (latest)OpenSearch_2_11_R20231113-P1 (latest)Availability Zone(s)1-AZ without standby
构建Supervisor管理的容器创建dockerfile创建dockerfile的部分， 比较难的是 ， 需要找到合适的filebeat版本参考页面:  Agents and ingestion tools其他的步骤就下载安装就可以. 
# 使用官方Nginx作为基础镜像
FROM reg.liarlee.site/docker.io/nginx

# 安装Supervisor
RUN apt-get update &amp;&amp; apt-get install -y supervisor
RUN mkdir -p /var/log/supervisor
RUN mkdir -p /etc/filebeat/
#RUN curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-8.11.3-amd64.deb &amp;&amp; dpkg -i filebeat-8.11.3-amd64.deb
RUN curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-oss-7.12.1-amd64.deb &amp;&amp; dpkg -i filebeat-oss-7.12.1-amd64.deb


COPY filebeat.yml /etc/filebeat/filebeat.yml
COPY nginx.conf /etc/nginx/nginx.conf

# 将Supervisor配置文件复制到容器中
COPY supervisord.conf /etc/supervisor/conf.d/supervisord.conf

# 启动Supervisor来管理Nginx进程
CMD [ "/usr/bin/supervisord", "-n" ]
准备配置文件需要准备的配置文件一共3个： 

supervisord.conf  supervisor的管理配置， 决定了那些进程被管理。
> cat ./supervisord.conf
[unix_http_server]
file=/var/run/supervisor.sock   ; (the path to the socket file)
chmod=0700                       ; socket file mode (default 0700)
chown=nobody:nogroup             ; socket file uid:gid owner

[supervisord]
logfile_maxbytes=50MB                          ; 日志文件的最大大小
logfile_backups=10                             ; 日志文件的备份数
loglevel=info                                  ; 日志级别
nodaemon=false                                 ; 是否以守护进程模式启动Supervisor
minfds=1024                                    ; 可以打开的文件描述符的最小数量
minprocs=200                                   ; 可以创建的进程的最小数量

[program:nginx]
command=/usr/sbin/nginx -g "daemon off;"  ; 启动Nginx的命令
autostart=true                             ; 在Supervisor启动时自动启动
autorestart=true                           ; 程序异常退出后自动重启
stderr_logfile=/var/log/nginx/error.log    ; 错误日志文件路径
stdout_logfile=/var/log/access.log   ; 访问日志文件路径

[program:filebeat]
command=/usr/bin/filebeat -e -c /etc/filebeat/filebeat.yml  ; 启动Filebeat的命令
autostart=true
autorestart=true
stderr_logfile=/var/log/filebeat.err.log
stdout_logfile=/var/log/filebeat.out.log
filebeat.yml   filebeat的配置文件。  这配置文件 GPT 会直接写出一个可以用 output.opensearch:， 其实还是不能的， 只能使用原本的配置文件。 (也许是我选择的filebeats版本不正确,  所以不行吧
 filebeat本身是es序列里面的产品， 不支持opensearch也合理， 如果写成opensearch 会找不到output 的定义， 也说明并不支持这个字段。


2023-12-14T12:03:12.560Z	INFO	[publisher_pipeline_output]	pipeline&#x2F;output.go:145	Attempting to reconnect to backoff(elasticsearch(https://vpc-ecs-nginx-opensearch-qt7m5rmhddggkiuapyybcmz5oe.cn-north-1.es.amazonaws.com.cn:443)) with 7 reconnect attempt(s)
&#96;&#96;&#96;shell
&gt; cat .&#x2F;filebeat.yml
filebeat.inputs:
- type: filestream
  id: nginxaccesslog
  paths:
    - &#x2F;var&#x2F;log&#x2F;access.log
  fields:
    log_type: access

seccomp.enabled: false # 这个不关闭的话可能会是一个干扰。
logging.level: debug # 由于调试方便设置了DEBUG。 

# 这个配置段是关闭 xpack， xpack功能只在es里面提供， 商业版本。
ilm.enabled: false
setup.ilm.enabled: false
setup.pack.security.enabled: false
setup.xpack.graph.enabled: false
setup.xpack.watcher.enabled: false
setup.xpack.monitoring.enabled: false
setup.xpack.reporting.enabled: false

# output就是还用es
output.elasticsearch:
  enable: true
  hosts: [&quot;vpc-ecs-nginx-opensearch-qt7m5rmhddggkiuapyybcmz5oe.cn-north-1.es.amazonaws.com.cn:443&quot;] # 这个部分需要手动指定443, 因为是es的默认配置, 所以直接去 9200,就会连接不上.
  protocol: &quot;https&quot;

  xpack 报错的日志大概是这样的： 

 2023-12-14T12:03:12.560Z	ERROR	[publisher_pipeline_output]	pipeline&#x2F;output.go:154	Failed to connect to backoff(elasticsearch(https://vpc-ecs-nginx-opensearch-qt7m5rmhddggkiuapyybcmz5oe.cn-north-1.es.amazonaws.com.cn:443)): Connection marked as failed because the onConnect callback failed: request checking for ILM availability failed: 401 Unauthorized: {“Message”:”Your request: ‘&#x2F;_xpack’ is not allowed.”} 2023-12-14T12:03:12.560Z	INFO	[publisher_pipeline_output]	pipeline&#x2F;output.go:145	Attempting to reconnect to backoff(elasticsearch(https://vpc-ecs-nginx-opensearch-qt7m5rmhddggkiuapyybcmz5oe.cn-north-1.es.amazonaws.com.cn:443)) with 7 reconnect attempt(s)


nginx.conf   这个是nginx 应用文件， 模拟一个应用程序， 提供webserver服务。配置文件就是标准的配置文件, 修改一下日志输出的路径. access_log  /var/log/access.log  main;
 由于baseimage用的是nginx的， 所以nginx 的日志输出会软链接到&#x2F;dev&#x2F;stdout, filebeat 不收软链接的文件, 开了DEBUG会看到跳过这个文件的日志.

Buildstage接下来就可以Build镜像然后进行测试了。 
> dive build -t reg.liarlee.site/library/superv-nginx:v31 .
> docker push reg.liarlee.site/library/superv-nginx:v31
> docker run -it --name superv-nginx --rm  reg.liarlee.site/library/superv-nginx:v31
运行启动之后可以看到输出的日志是： 
2023-12-14 14:03:31,093 INFO success: filebeat entered RUNNING state, process has stayed up for > than 1 seconds (startsecs)
2023-12-14 14:03:31,093 INFO success: nginx entered R ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/08/Cheatsheet_AWS-CLI/" title="CheatSheet_awscli">CheatSheet_awscli</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-08T05:56:27.000Z" title="发表于 2023-12-08 13:56:27">2023-12-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.955Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/AWS/">AWS</a></span></div><div class="content">查看实例和对应实例的系统平台信息aws ec2 describe-instances --query "Reservations[*].Instances[*].&#123;InstanceId:InstanceId,PlatformDetails:Platform&#125;" --output table
查看实例和EBS的关联关系aws ec2 describe-volumes --query 'Volumes[*].[VolumeId, Attachments[0].InstanceId, Size]' --output table</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/12/05/AWS/EKS_Windows%20core%20%E8%8A%82%E7%82%B9%E7%AE%A1%E7%90%86/" title="Windows Core EKS 节点管理命令">Windows Core EKS 节点管理命令</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-12-05T02:11:22.000Z" title="发表于 2023-12-05 10:11:22">2023-12-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.716Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">windows 查看磁盘空间的使用情况: 
Get-PSDrive -Name C | Select-Object Name, Free

windows 实例的磁盘空间扩容
diskpart
list volume
select volume 0
extend
从ecr下载镜像
$ecrCreds = (Get-ECRLoginCommand).password

Write-Host $ecrCreds

ctr  -n k8s.io image pull -u AWS:$ecrCreds ecr link
pull image 需要使用节点的C盘空间, 　在节点的磁盘空间不足的情况下，会报错。　
rpc error: code = Unknown desc = failed to pull and unpack image 
failed to extract layer sha256:9ee7a25f1f619685e0c27cd1f08b34fd7a567f8f0fa789gf9aeb79c72169afa: hcsshim::ImportLayer failed in Win32: There is not enough space on the disk. (0x70): unknown</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/10/08/Other/NAS_TrueNas%20Scale%20%E5%86%85%E5%AD%98%E5%8D%A0%E7%94%A8%E7%9A%84%E5%8F%82%E6%95%B0/" title="TrueNas Scale 让 ZFS 占用更多内存作为 ARC">TrueNas Scale 让 ZFS 占用更多内存作为 ARC</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-10-08T05:57:57.000Z" title="发表于 2023-10-08 13:57:57">2023-10-08</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.962Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Tools-Toy/">Tools, Toy</a></span></div><div class="content">TrueNas ScaleSome param can use to tunning Memory Usage : 
Config the ARC Memory to 75% Total Memory :
root@haydentruenas:&#x2F;sys&#x2F;block# echo 2995556352 &gt; &#x2F;sys&#x2F;module&#x2F;zfs&#x2F;parameters&#x2F;zfs_arc_maxroot@haydentruenas:&#x2F;sys&#x2F;block# echo 268435456 &gt; &#x2F;sys&#x2F;module&#x2F;zfs&#x2F;parameters&#x2F;zfs_arc_sys_free
#!/bin/sh

PATH="/bin:/sbin:/usr/bin:/usr/sbin:$&#123;PATH&#125;"
export PATH

ARC_PCT="90"
ARC_BYTES=$(grep '^MemTotal' /proc/meminfo | awk -v pct=$&#123;ARC_PCT&#125; '&#123;printf "%d", $2 * 1024 * (pct / 100.0)&#125;')
echo $&#123;ARC_BYTES&#125; > /sys/module/zfs/parameters/zfs_arc_max

SYS_FREE_BYTES=$((8*1024*1024*1024))
echo $&#123;SYS_FREE_BYTES&#125; > /sys/module/zfs/parameters/zfs_arc_sys_free</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/09/06/Linux/Linux_AlwaysRunUserdata/" title="在EC2实例每次启动的时候都运行Userdata">在EC2实例每次启动的时候都运行Userdata</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-06T06:24:46.000Z" title="发表于 2023-09-06 14:24:46">2023-09-06</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.720Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">使用Cloud-init提供读取Userdata的功能。 
需要在Userdata中添加一个 MIME 的头部， 覆盖默认的行为。

https://cloudinit.readthedocs.io/en/latest/topics/format.html#mime-multi-part-archivehttps://repost.aws/zh-Hans/knowledge-center/execute-user-data-ec2

具体需要添加的MIME 部分： 
Content-Type: multipart&#x2F;mixed; boundary&#x3D;&quot;&#x2F;&#x2F;&quot;
MIME-Version: 1.0

--&#x2F;&#x2F;
Content-Type: text&#x2F;cloud-config; charset&#x3D;&quot;us-ascii&quot;
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename&#x3D;&quot;cloud-config.txt&quot;

#cloud-config
cloud_final_modules:
- [scripts-user, always]

--&#x2F;&#x2F;
Content-Type: text&#x2F;x-shellscript; charset&#x3D;&quot;us-ascii&quot;
MIME-Version: 1.0
Content-Transfer-Encoding: 7bit
Content-Disposition: attachment; filename&#x3D;&quot;userdata.txt&quot;

之后重新启动这个EC2 就可以了。 
Troubleshooting Guide
首先可以的查看Userdata ， 在控制台 或者 AWScli
查看实例内部日志 或者 Console的日志：&#x2F;var&#x2F;log&#x2F;cloud-init.log&#x2F;var&#x2F;log&#x2F;cloud-init-output.log
查看userdata 注入的脚本内容：&#x2F;var&#x2F;lib&#x2F;cloud&#x2F;instances&#x2F;i-09e08d362af7fa401&#x2F;scripts 在这个目录下。文件树 ： [root@ip-172-31-50-3 i-09e08d362af7fa401]# tree
.
├── boot-finished
├── cloud-config.txt
├── datasource
├── handlers
├── obj.pkl
├── scripts  # 这个
│   ├── part-001
│   └── part-002
├── sem
│   ├── config_amazonlinux_repo_https
│   ├── config_disk_setup
│   ├── config_keys_to_console
│   ├── config_locale
│   ├── config_mounts
│   ├── config_phone_home
│   ├── config_power_state_change
│   ├── config_resolv_conf
│   ├── config_rsyslog
│   ├── config_runcmd
│   ├── config_scripts_per_instance
│   ├── config_scripts_user
│   ├── config_set_hostname
│   ├── config_set_passwords
│   ├── config_ssh
│   ├── config_ssh_authkey_fingerprints
│   ├── config_timezone
│   ├── config_users_groups
│   ├── config_write_files
│   ├── config_write_metadata
│   ├── config_yum_add_repo
│   ├── config_yum_configure
│   └── consume_data
├── user-data.txt
├── user-data.txt.i
├── vendor-data.txt
└── vendor-data.txt.i

3 directories, 33 files

如果需要临时的改一些内容， 可以写在Userdata里面， 完成操作之后删除Userdata 就可以了。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/09/04/Linux/Linux_Kubernetes%E5%B8%B8%E7%94%A8debug%E5%91%BD%E4%BB%A4/" title="Linux_Kubernetes常用debug命令">Linux_Kubernetes常用debug命令</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-04T01:08:02.000Z" title="发表于 2023-09-04 09:08:02">2023-09-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.735Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">查看 AWS LoadBalancer 证书的信息，检查证书的有效期： 
kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io aws-load-balancer-webhook -ojsonpath=&#123;.webhooks[0].clientConfig.caBundle&#125;  | base64 -d  | openssl x509 -noout -text

使用 debug 容器： 
# 给特定的容器附加一个Sidecar， 并启动shell。
kubectl debug -it --image=public.ecr.aws/amazonlinux/amazonlinux:latest aws-node-cpmck
# netshoot容器， 比较方便的用来进行网络部分的调试。
# 项目仓库地址： https://github.com/nicolaka/netshoot
kubectl debug mypod -it --image=nicolaka/netshoot
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/08/20/Other/Books_%E6%AD%A3%E8%A7%81-%E4%BD%9B%E9%99%80%E7%9A%84%E8%AF%81%E6%82%9F-%E8%8A%82%E9%80%89-3/" title="正见——佛陀的证悟（三）诸法无我 (选摘)">正见——佛陀的证悟（三）诸法无我 (选摘)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-20T14:29:45.000Z" title="发表于 2023-08-20 22:29:45">2023-08-20</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.960Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Books/">Books</a></span></div><div class="content">所有这些不同的情绪及其结果，都来自于错误的理解，而这个误解来自一个源头，也就是所有无明的根源—-执着于自我。
自我只是另一个误解。当我们看着自己的身体（色）、感受（受）、想法（想）、行为（行）和意识（识）的时候，我们通常制造出一种自我的概念。人们受制约，把这种概念视为恒常而且真实的。
但是悉达多了悟到，不论是在身体里或外，都找不到一个独立存在的实体，足以被称为自我。如同火圈的视觉错幻一般，自我也是虚幻的。它是谬误的；基本上错误，而究竟上不存在。但是如同我们被火圈所迷惑一般，我们也全都被自我所迷惑了，执着于谬误的自我，是无明的荒谬行为。它不断地制造更多的无明，导致了各种痛苦和失望。
无明单纯的就是不了解事实，或对事实了解不正确，或认识得不完整。所有这些形式的无明，都导致误解和误判，高估和低估。假设你正在寻找一个朋友，忽然看到他在远方的田野中。一走近，却发现你误把一个稻草人当做是他了。你一定会感到失望。这并非有个恶作剧的稻草人或你的朋友试图偷偷摸摸误导你，而是你自己的无明背叛了你。任何源自无明所做的行为，都是冒险。我们在不了解或不完全了解的情况下行动，就不会有信心。我们根本的不安全感因此而生起。创造出所有这些有名或无名、已知或未知的各种情绪。
悉达多了悟到自我并非独立存在、自我只不过是一个标签、因而执着于自我就是无明，这可能是人类历史上最大的发现。然而，虽然自我这个标签或许毫无根据，要摧毁它却不是一个简单的任务。执着于这个称作自我的标签，是所有的概念中最难以破除的。
魔罗只不过是悉达多的我执。故事中，描述魔罗是个英俊威武、无役不克的战士，这个比喻相当适切。自我，如同魔罗一般，威力强大且贪得无厌、自我中心且虚伪欺诈、贪求众人目光、机敏伶俐且爱慕虚荣。我们很难记住，自我如同火圈的幻相一般，是和合而成、不独立存在而且善于改变。
骄慢和自怜息息相关。我执纯粹是一种自我纵容，认为自己的生命比其他人的都更艰难更悲哀。当自我发展出自怜的时候，便让其它人生起悲悯的空间消失了。
虽然我们不认为自己这么绝望，而且相信自己是受过教育、正常、清醒的，但是当我们看见及感受一切都是真实存在时，我们的行为就如同那位沙漠中的迷失者。我们急切地想要找到真实的伴侣关系、安全感、表扬、成功，或只是安详宁静。我们甚至能抓到与欲望相似的东西。但就象那位迷失者，当我们依赖外在的实体性时，终究会失望。事物并不如其所显现—–它们是无常的，而且不完全在我们的掌控之中。
如果我们象悉达多一样确实地去分析，就会发现诸如形体、时间、空间、方向、大小等附加的标签，都很容易被解构。悉达多了悟到，甚至自我都只存在于相对的层次，恰如海市蜃楼一般。他的体悟，终止了期待、失望与痛苦的循环。在证悟的时刻，他自忖，我已找到了一条深奥、安详、非极端、清晰、满愿又有如甘露一般之道。然而，如果我想表达它，如果我想教给他人，没有人有能力听闻了解。因此我将留在林中，安住于此祥和状态之中。
然而，悉达多并非不理性，他只是明确地指出一般的、理性的思惟是有限的。我们不能，或不愿超越我们自己的舒适区去了解。用昨日、今日、明日这种线性的概念来操作，比如说“时间是相对的”，来得实用得多。我们没有被设定成这么想的：我能不改变大小或形状而进入那牦牛角。我们不能破除大和小的概念；相反的，我们一直被世代传下来的安全而狭隘的观点所局限。然而，当这些观点被审视时，却都站不住脚。举例来说，这个世界如此依赖的线性时间观念，无法说明时间没有真正的起始也没有终止的事实。
因此当我们听到一个人不改变尺寸，就可容入牦牛角中时，我们没有太多选择—–我们可以很“理性”，认为这根本不可能而驳斥它；或者我们引用某种对法术的神秘信仰或盲目崇拜而说，当然，密勒日巴是多么伟大的瑜珈行者，当然他能这么做，甚至还不只是这些呢。这两种见解都是扭曲的，因为否认是一种低估，而盲信则是一种高估。
一条河，水在流，永远在变，然而我们仍然称它为河流。如果一年之后我们再度造访，会认为它是同一条河。但它是如何相同的呢？如果我们单独挑出一个面向或特性，这相同性就不成立了。水不同了，地球在银河中转动的位置也不同了，树叶已落，新叶又长出来了—-剩下的只是一个相似于我们上次见到的河流表象而已。以“表象”作为“真实”的基础是相当不可靠的。经由简单的分析，就能显示出我们一般对所谓真实的基础，都只是一些模糊的概括和假设。虽然悉达多也使用一般人定义“真实”时所用的字眼—-非想象的、确定的、不改变的、无条件的—-但他更精确地使用这些字眼，而非概括性的。在他的观点上，不改变必然意指在所有的方面都不改变，甚至经过彻底的分析后，仍然绝无例外。
悉达多发现，要确定某个东西真实存在的唯一办法，就是证明它独立存在，而且不需要诠释、不能造作或不会改变。对悉达多而言，我们日常生活上一切似乎能作用的机制，不论是身体的、情感的及概念的，都是由不稳固、不恒常的部分所聚合而成，因此它们随时都在改变。我们可以在惯常的世界中了解这个论点。举例来说，你可以说你在镜中反射的影像不是真实存在的，因为他需要依赖你站在镜子前面才行。类似的，事物要真实或独立地存在，就不能被制作或被创造的，因为这要依赖制造者。
许多对佛陀的教法不甚了解的人，认为佛教是病态的，他们认为佛教徒否定快乐，只想到痛苦。他们设想佛教徒排斥美丽及身体的享受，因为这些是诱惑；佛教徒应该是纯净而节制的。事实上，在悉达多的教法中，并不特别反对美丽和享乐甚于其它的任何概念－－只要我们不认为它们是真实存在的，而迷失其中。
虽然悉达多证悟了空性，但空性并不是由他或任何人所制造的。空性不是悉达多获得天启的结果，也不是为了让人们快乐所发展出来的理论。不论悉达多开示与否，空性即是如此。我们甚至不能说它一直都是如此，因为它超越时间，而且不具形式。空性也不应被解释为存在的否定（也就是说，我们不能说这个相对的世界不存在），因为要否定某个事物，你就要先承认有某个东西可以被否定才行。空性也不会消除我们日常的经验。悉达多从来没说过有什么可以取代我们所觉受的更壮丽、更美好、更纯粹或更神圣的东西。他也不是虚无主义者，否定世间存在事物的显现与功能。
悉达多会同意这两种情况－－当你醒着时，你不能飞；而当你睡着时，你能飞。这道理是在于因缘是否具足；要能飞翔的一个缘，是睡眠。当你没有它，你就不能飞，有了它，你就能飞。假设你梦见你能飞，而醒来后还继续相信你能飞，那就麻烦了。你会掉下来，而且会失望。悉达多说，即使在相对世界中醒着，我们还是在无明中沉睡，如同在他出走那夜的宫女一般。恰当的因缘聚合时，任何事情都可能出现。但当因缘消散，显现也就停止。
这也是为何在悉达多教法一千五百年后，一位叫帝洛巴的佛法继承者，对他的学生那洛巴说，不是显现（外相）困住了你，而是你对显现（外相）的执着困住了你。
如同环球小姐选美一样，在这个世界上，我们所做、所想的任何事物，都有是基于一个非常有限的共同逻辑系统。我们非常强调共识。如果大多数人同意某件事物是真实的，通常它就变成正当有效的。当我们看着一个小池塘，我们人类认为它只是个池塘，但对池里的鱼儿来说，这是它们的宇宙。如果我们采取民主的立场，那么水中族群一定会赢，历为它们比我们这些观池塘者为数多得多。多数决不见得永远都对。糟糕的大卖座影片可以赚得大量的利润，而一部独立制作的优秀影片却只有少数人观赏。而且由于我们依赖群体思考，这世界通常是被最短视而腐败的统治者所治理；民主制度只是诉诸于最小公约数而已。
然而了悟不必是致命的。我们不需要象井蛙一般，面对空性惊吓而亡。如果海蛙能够有稍微多点的慈悲心和善巧，也许它可以做一个更好的向导，井蛙也不至于吓死，也许它还会移居到海边也说不定。而我们也不需要有超自然的天赋才能了解空性。这和教育以及愿意观察事物所有的部分以及隐藏的因缘有关。有了这种洞见，我们就会象布景设计师或摄影助理在看电影。专业者能看见我们所看不见的东西。他们看见摄影机如何布置，以及其他观众们不知道的电影技巧，因此对他们而言，这幻相被拆解了。但专业者在看电影的时候，还是可以尽情享受。这就是悉达多超然的幽默。
悉达多没有一开始就用空性来惊吓大家，反而以一般的方式，诸如禅定以及行为规范－－做正确的事、勿盗窃、勿妄言等来教导众多的弟子。根据弟子的本性，他定下了不同程度的出离及苦行，从削发到不食肉等等。对一开始无法听闻或了解空性的人，以及天性适合苦修的人而言，这些状似宗教性的严格道路很有效。
业（Karma）这个字几乎和佛教成了同义字。通常它被理解为一种道德系统的报应—-恶业与善业。然而，业只是一种因果的法则，不应该与道德或伦理混淆。
你也行会认为这是矛盾的。佛陀自我矛盾，说他不存在，一切皆是空性，然后他又教导了道德和救赎。然而这些方法是必要的，以免吓走那些尚未准备好，还不能被引介空性的人。他们用了这些方法，因而变得祥和，易于接受真正的教法，这就如同说那里是有一条蛇。然后把领带丢到窗外一般。这些无限的方法就是道路。然而，道路本身终究也需要被抛弃，如同你抵达彼岸时，就得抛弃舟一般。你抵达时必须要下船。在完全证悟的那一刻，你必须抛弃佛教。精神之路是一个暂时解答，它是在空性被了悟之前所使用的安慰剂。
如同在电影院里的孩童，我们被幻相掳获了。从这儿开始，衍生了我们的虚荣、野心和不安全感。我们爱上了自己创造的幻相，发展出对自己的外表、财富和成就过度的骄慢。好比戴了面具，却骄傲地认为面具是真实的你。
了悟空性的悉达多，对菩提树下的忘忧草或宫殿里的丝绸坐垫没有好恶分别。金线织成的坐垫价值较高，完全是由人类的野心和欲望造作而来的。事实上，山中的隐士也许会觉得忘忧草比较柔软而干净，而且最大的好外是坐坏了也不需要担心。你不需要对它喷药，以免猫用爪子去抓它。宫廷生活充满了这类的“珍品”，需要相当多的维修保养。悉达多是属于比较喜欢草垫的人，因此他不需要常常回家去补充什么东西。
我们人类认为心胸宽广是一种美德。要扩展心胸，重要的是不要安于令我们舒适或习惯的东西。如果我们有勇气超越世俗，不被惯常逻辑的界线所限制，就能得到利益。如果我们能超越界线，就能了解空性是如此可笑地单纯。密勒日巴躲进牦牛角不会比某个人戴上手套还令你讶异。我们所要挑战的，是对惯常逻辑、文法、字母、数学公式的执着。如果能记得这些习惯的和合本性，我们就能断除它们。它们不是不能破除的，所需要的只是一个条件完全正确、资讯适时到位的情况，你可能突然发现所有依赖的工具都不是那么坚实，它们有弹性、可弯曲。你的观点会改变。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/08/20/Other/Books_%E6%AD%A3%E8%A7%81-%E4%BD%9B%E9%99%80%E7%9A%84%E8%AF%81%E6%82%9F-%E8%8A%82%E9%80%89-2/" title="正见——佛陀的证悟（二）诸漏皆苦(选摘)">正见——佛陀的证悟（二）诸漏皆苦(选摘)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-20T13:29:45.000Z" title="发表于 2023-08-20 21:29:45">2023-08-20</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.959Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Books/">Books</a></span></div><div class="content">悉达多没有继续前行，决定留在当地禅修。他在附近的一棵毕钵罗树下找到一块平坦的石头，铺上吉祥草当坐垫。他静默地立下誓言，此身可烂，我可能化为尘土。但直到找到答案，我绝不起身（我今若不证，无上大菩提，宁可碎此身，终不起此座）。
问一个佛教徒“什么是人生的目的？”是不恰当的。因为这个问题暗喻在某一个地方，也许在一个洞穴之中或者在一人山岭之上，存在着一个究竟的目的。仿佛我们可以透过追随圣者，阅读书籍以及熟悉秘教修行，来解开这个秘密。如果这问题是假设在亿万年以前，有某个人或神设计了一个人生目的图表，那么它就是一个有神论的观点。佛教徒不相信有个全能的创造者，而且他们不信为生命的目的已经、或需被决定和定义。
即使在个人身上，痛苦和快乐的定义也时有变动。一个轻佻的调情时刻，可能因为其中一个人想要更认真的关系而突然变调，期待转为恐惧。当你是个小孩的时候，在沙滩上堆筑沙堡就是快乐。在青少年时期，看着穿比基尼的女孩，和赤裸上身的男孩冲浪是快乐。在中年，金钱和事业是快乐。当你八十多岁的时候，收集陶瓷盐罐是快乐。对许多人而言，不断调适于这些无尽而又经常变化的快乐定义，即是“人生的目的”。
我们人类在追求快乐、止息痛苦上，用尽了无数的方法和工具，远超过任何其它的嗜好和职业。因此我们拥有电梯、笔记型电脑、充电电池、电动洗碗机、自动弹出完美土司的烤面包机、狗粪吸尘器、电动鼻毛修剪器、温热坐垫马桶、奴佛卡因麻醉药（ Novocaine）、行动电话、威而刚、整铺地毯子……，然而不可避免的，这些便捷也制造了等量的头痛。
悉达多当时也是在试图根除痛苦。但他不是梦想着诸如展开政治改革、移民到另一个星球或创造世界新经济；他甚至没有想到要创造一个宗教，或发展一套能带来安详与和谐的行为准则。他以开放的心灵来探索痛苦，透过勤奋不懈的沉思，悉达多发现，追本溯源，导致痛苦的是人的情绪。事实上上，情绪即是痛苦。不论如何，直接或间接的，一切情绪都是生于自私，也就是说，它们都与执着于自我有关。更进一步的，他也发现，情绪虽然看似真实，但不是一个人本具存在的一部分。它们不是与生俱来的，也不是某个人或某个神强加在我们身上的诅咒或植入。当某些特定的因与缘聚合在一起的时候，情绪就会生起，例如当你突然认为某个人在批评你、忽视你，或者剥夺你的利益时。然后，相对应的情绪就会接着生起，在接受、陷入这些情绪的当下，我们就失去了觉知和清明。我们“被鼓动”了。因此悉达多发现他的解决方法—-觉知。如果你认真地想要根除痛苦，你必须培养觉知，留心你的情绪，并且学会如何避免被鼓动起来。
如果你像悉达多一样地检视情绪，试图找出它们的起源，你将会发现它们根植于误解，因此根本上是错误的。基本上，所有的情绪都是一种偏见，在每一种情绪之中，都存在有分别心的成分。
看透了宫廷生活的表象，悉达多现在能够看见自己的身体是不具本质的。在他的眼中，火圈和身体具有相同的本性。如果有人相信其中之一真实存在—-不论是短暂的或恒常的——那么他的信念就是根源于误解；如此，便是失去了觉察，也就是佛教徒所说的无明。我们的情绪，就是从这无明所生起。从失去了觉察到情绪生起的过程，可以用四真谛完全解释。我们接下来会谈到。
魔罗想要争取更多追随者，因而聪明地鼓吹自由，但是如果有人真的行使自由，他不一定会喜欢。基本上，我们只想要让自己，而不想让他人拥有自由。难怪，我们真的行使所有的自由，就不会被邀请去参加任何派对了。
悉达多的解答是—-培养对情绪的觉察。如果情绪正在起的时候，你能够有所觉察，即使只是一点点，不能够限制它们的活动；它们变成象有监护人在旁的青少年。有人在监视着，魔罗的力量就会减弱。悉达多没有被毒箭所伤，因为他觉察到这些只不过是幻相。同样的，我们自己强大的情绪，也可以变成象花瓣一般地不具杀伤力。当天女接近悉达多的时候，他清楚地了解，她们如同火圈，只是和合而成的，因此她们失去了诱惑力，无法动他一丝一毫。同样的，只要了解我们所欲求的对象事实上是和合而成的现象，就能破除诱惑的魔咒。
当你开始注意到情绪所能够造成的损害，觉知就会开始发展。当你有了觉知—-举例来说，如果你知道自己正站在悬崖上行走不再那么恐怖，事实上，它反而是非常刺激的。不知才是恐惧的真正根源。觉知不会妨碍你的生活，反而让生命更加充实。如果你在享用一杯茶，而且了解短暂事物的甘与苦，你将能够真正的享受那杯茶。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/08/16/Linux/EBS_performance-tuning-advise/" title="EBS性能优化">EBS性能优化</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-16T06:17:31.000Z" title="发表于 2023-08-16 14:17:31">2023-08-16</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.720Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">EBS性能优化Agenda

磁盘调优方法论
常用的磁盘调优方法
磁盘压测工具
磁盘性能检测工具

目前比较常用的是 GP3。 
方法论应用程序 –》 操作系统 –》 虚拟机 –》 EBS 服务器
应用程序IO请求下发的路径。
三个方向， iops， 带宽， 延迟，按照这三个方面来观察这个EBS本身的性能。 
使用新版本的内核：
3.8 以上内核使用的Indirect descriptors
突破 I&#x2F;O size 44kiB限制， 可以一次请求下发 128KiB的IO大小。
建议使用 amazonlinux 2023 （悲
uname -r 查看内核的版本。

使用 Raid0使用 Raid0 可以提高带宽， 并且节省成本。

能更换磁盘类型达到性能， 可以使用Raid0 
不建议在EBS 做 Raid1 5 6 三种， 校验的方式会降低性能。
推荐使用 mdadm ， 不一定要用LVM。

关注队列长度SSD： 每 1000iops 队列长度是 1 ， SSD是合适的吧
​      观察队列长度， 如果能达到 IOPS 的情况下， 队列越低越好。
HDD： 1MiB的IOsize ， 建议的队列长度 4
​      降低io队列的方法,取决于发送IO的速度和线程数量。 或者提高磁盘的IOPS
HDD吞吐量设计增加预读取大小For st1&#x2F;sc1. 
blockdev --report /dev/nvme1n1 # 查看块设备的参数
blockdev --setra 2048 /dev/nvme1n1  # 设置块设备预读取数据量的大小
blockdev --getra /dev/nvme1n1 # 查看参数是否生效。

这个部分是为了增加吞吐量而在Read的同时将后面的一部分数据读取进入内存， 这样可以提高顺序读取的效率， 提高读取的效率， 让顺序读取的业务数据提前进入内存。
使用 Nitro 架构实例
之前使用zen的时候有一个Dom0来进行实例的管理。 nitro 做成 Hardware， nitro 管理VPC等等等， 尽可能把 X86 过度给客户来使用。
Refer to： https://zhuanlan.zhihu.com/p/270522703

开启EBS优化选项。 非优化的场景下，会与网络请求共享带宽， EBS优化这个功能单独隔离了EBS的网络请求。


使用正确的实例类型关注实例的限制， 例如实例的上限是 14000 ， 那么实例使用EBS的瓶颈不在磁盘本身， 在实例的性能上限。
压测工具
dd 

hdparm 

sysbench

iometer

iozone


推荐fio 
https://fio.readthedocs.io/en/latest/fio_doc.html
ebs设置默认的 Block Size是 16k ， 其他厂商可能是 4k的。 这个部分可能会在测试的时候造成差异。 
测试命令#!/bin/bash

# 这个命令的运行结果是， 一个 job 在 iodepth 为 1 的时候， 写入与底层一样的块大小，在iostat 中可以看到  avgqu-zs 的大小是 1。
# 添加一个process之后， 队列的size 变成了2 ； 添加一个 iodepth 之后， 队列的大小为 1 ~ 2 之间。 
# 大佬认为内核的蓄流将请求合并了， 所以没有造成对应的压力。 

fio --directory=/mnt --name fio_test_file --ioengine=libaio --direct=1 --rw=read --rate_iops=1 --bs=16k --size=200M --iodepth=1 --numjobs=1 --time_based --runtime=600 --group_reporting --norandommap


perf record -g result (libaio):
fio 319375 86964.196578:     250000 cpu-clock:pppH:
        ffffffff88355470 put_dec+0x0 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff8835584b number+0x33b (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff8835ac9d vsnprintf+0x44d (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff8835afe2 sprintf+0x62 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87efc94c dev_show+0x2c (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87efbc6c dev_attr_show+0x1c (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87abac9b sysfs_kf_seq_show+0xab (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87a2a7a3 seq_read_iter+0x123 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff879f4c06 vfs_read+0x1f6 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff879f59ff ksys_read+0x6f (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff88364360 do_syscall_64+0x60 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff884000f3 entry_SYSCALL_64+0xb3 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
                  103fdc read+0x4c (/usr/lib/libc.so.6)
                       0 [unknown] ([unknown])

perf record -g result (psync):
fio 324148 87167.271746:     250000 cpu-clock:pppH:
        ffffffff8835480c strlen+0xc (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87a2990b seq_puts+0x1b (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87aa94ee render_sigset_t+0x1e (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87aa9c96 proc_pid_status+0x716 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87aa1e34 proc_single_show+0x54 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87a2a7a3 seq_read_iter+0x123 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff87a2abe4 seq_read+0xd4 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff879f4abc vfs_read+0xac (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff879f59ff ksys_read+0x6f (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff88364360 do_syscall_64+0x60 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
        ffffffff884000f3 entry_SYSCALL_64+0xb3 (/lib/modules/6.4.10-arch1-1/build/vmlinux)
                  103fa1 read+0x11 (/usr/lib/libc.so.6)
                       0 [unknown] ([unknown])

观测[[Linux_iostat|Linux_iostat]]可以直接使用这个命令来观察磁盘的使用情况， 磁盘的使用率其实并不太准确，这个表示单位时间内cpu时间被io请求占用的时间。 
个别的 10ms 之上的IO请求的长尾现象是正常的， 这个无法避免。
关于长尾现象的说明： 

长尾请求一般是指明显高于均值的那部分占比较小的请求。  业界关于延迟有一个常用的P99标准， 也就是99%的请求延迟要满足在一定耗时以内， 1%的请求会大于这个耗时， 而这1%就可以认为是长尾请求。  
Refer to:  https://zhuanlan.zhihu.com/p/35516682

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/08/16/Linux/Linux_X11Forward/" title="X11 通过 ssh 转发图形化界面">X11 通过 ssh 转发图形化界面</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-16T06:17:31.000Z" title="发表于 2023-08-16 14:17:31">2023-08-16</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.743Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Ssh 转发 X11sudo yum install xorg-x11-xauth xorg-x11-fonts-* xorg-x11-font-utils xorg-x11-fonts-Type1 xclock

vim /etc/ssh/sshd_config
  X11Forwarding yes
  X11UseLocalhost no
运行图形化的命令 , 自动捕获请求， 查看mobexterm是否已经正常的启动了一个临时的窗口来进行数据的转发即可 。
Ssh 转发连接到远端ssh -L local-port:target-host:target-port tunnel-host
ssh -vvv -f -N -D 0.0.0.0:30890 hayden@localhost
Ssh 转发命令参数说明-L listen-port:host:port 指派本地的 port 到达端机器地址上的 port
    建立本地SSH隧道(本地客户端建立监听端口)
    将本地机(客户机)的某个端口转发到远端指定机器的指定端口.

-R listen-port:host:port 指派远程上的 port 到本地地址上的 port
    建立远程SSH隧道(隧道服务端建立监听端口)
    将远程主机(服务器)的某个端口转发到本地端指定机器的指定端口.

-D port 指定一个本地机器 "动态的" 应用程序端口转发.
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/08/13/Other/Other_InputMethodForRIME/" title="rime的配置">rime的配置</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-13T02:39:06.000Z" title="发表于 2023-08-13 10:39:06">2023-08-13</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.963Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Other/">Other</a></span></div><div class="content">https://github.com/iDvel/rime-ice
https://rime.im/
小狼毫输入法， 或者Fcitx5 挂载 rime 即可。 
后面如果还有输入法相关的内容会放在这个里面。 
小鹤双拼> cat ./default.custom.yaml
patch:
  schema_list:
    - schema: double_pinyin_flypy    # 小鹤双拼
  ascii_composer:
    good_old_caps_lock: true  # true | false
    switch_key:
      Caps_Lock: clear      # commit_code | commit_text | clear
      Shift_L: noop  # commit_code | commit_text | inline_ascii | clear | noop
      Shift_R: noop         # commit_code | commit_text | inline_ascii | clear | noop
      Control_L: noop       # commit_code | commit_text | inline_ascii | clear | noop
      Control_R: noop       # commit_code | commit_text | inline_ascii | clear | noop
输入法默认不切换全半角> cat ./default.custom.yaml
patch:
  switches:
    - name: ascii_mode
      states: [ 中 ]
      reset: 0
    - name: ascii_punct # 中英标点
      states: [ ¥, $ ]
      reset: 0
    - name: traditionalization
      states: [ 简, 繁 ]
      reset: 1
    - name: emoji
      states: [ 💀, 😄 ]
      reset: 1
    - name: full_shape
      states: [ 半角, 全角 ]
      reset: 0</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/08/05/AWS/EKS_ServiceAccountToken/" title="serviceAccount 获取 Token 以及权限的方式">serviceAccount 获取 Token 以及权限的方式</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-05T15:42:22.000Z" title="发表于 2023-08-05 23:42:22">2023-08-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.716Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">手动获取Token 并发送请求给容器接口。
/var/run/secrets/kubernetes.io/serviceaccount $ pwd


/var/run/secrets/kubernetes.io/serviceaccount $ cat ./token
eyJhbGciOiJSUzI1NiIsImtpZCI6ImE3N2JhOGMwY2FjYWEwNDk0MWU4MWM0ODk1Y2JiZjBiOTU2NTA1OTYifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjIl0sImV4cCI6MTcyMTk3ODkxOCwiaWF0IjoxNjkwNDQyOTE4LCJpc3MiOiJodHRwczovL29pZGMuZWtzLmNuLW5vcnRoLTEuYW1hem9uYXdzLmNvbS5jbi9pZC9DMEE5NzJGNERCMThBOTMxN0RBMzk0Q0MwRDMzMjU3RiIsImt1YmVybmV0ZXMuaW8iOnsibmFtZXNwYWNlIjoibW9uaXRvcmluZyIsInBvZCI6eyJuYW1lIjoicHJvbWV0aGV1cy1rOHMtMCIsInVpZCI6ImE4YzdkMGY2LWUyMmEtNGY2Ny04NzYzLWNiNGQwYzM5MWFlMCJ9LCJzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoicHJvbWV0aGV1cy1rOHMiLCJ1aWQiOiJmN2M3YWMzZC1iZmY4LTRmODctYTlkNy04MjJjYWM4ZDVjMjkifSwid2FybmFmdGVyIjoxNjkwNDQ2NTI1fSwibmJmIjoxNjkwNDQyOTE4LCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6bW9uaXRvcmluZzpwcm9tZXRoZXVzLWs4cyJ9.D3a7b93ZnBrmM-hjFrklce8upr8gUhUYYwmjGzZKOKb4EWUCoQXY1p197UoViUmYzbiOhchbehKosFcOjiL2GVGXqDRIXPmFU9p8l80Z9fuilB2TmexB6uBavuEF6blKHkH0VjyVQ3Kg39WzjD0Atrw-G6N4QZ_m0TOcKFfqRPebbH-Q-vzz5UuNPUYRKF4XZIT84QqYLYiBOkkeFIS_abo18YddMbn2AZQcbWamxQV2XPXjNKKwHeQvP1Xdr4hZLt0oGudE2XZtQp60ZCrfDmQteh-KhMWRvsZpoNvE4n8HieAhTTrFq_TjjX9IHlXrPmcONAkQ33jVgRIaw_r_0g


export TOKEN="Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6ImE3N2JhOGMwY2FjYWEwNDk0MWU4MWM0ODk1Y2JiZjBiOTU2NTA1OTYifQ.eyJhdWQiOlsiaHR0cHM6Ly9rdWJlcm5ldGVzLmRlZmF1bHQuc3ZjIl0sImV4cCI6MTcyMTk3ODkxOCwiaWF0IjoxNjkwNDQyOTE4LCJpc3MiOiJodHRwczovL29pZGMuZWtzLmNuLW5vcnRoLTEuYW1hem9uYXdzLmNvbS5jbi9pZC9DMEE5NzJGNERCMThBOTMxN0RBMzk0Q0MwRDMzMjU3RiIsImt1YmVybmV0ZXMuaW8iOnsibmFtZXNwYWNlIjoibW9uaXRvcmluZyIsInBvZCI6eyJuYW1lIjoicHJvbWV0aGV1cy1rOHMtMCIsInVpZCI6ImE4YzdkMGY2LWUyMmEtNGY2Ny04NzYzLWNiNGQwYzM5MWFlMCJ9LCJzZXJ2aWNlYWNjb3VudCI6eyJuYW1lIjoicHJvbWV0aGV1cy1rOHMiLCJ1aWQiOiJmN2M3YWMzZC1iZmY4LTRmODctYTlkNy04MjJjYWM4ZDVjMjkifSwid2FybmFmdGVyIjoxNjkwNDQ2NTI1fSwibmJmIjoxNjkwNDQyOTE4LCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6bW9uaXRvcmluZzpwcm9tZXRoZXVzLWs4cyJ9.D3a7b93ZnBrmM-hjFrklce8upr8gUhUYYwmjGzZKOKb4EWUCoQXY1p197UoViUmYzbiOhchbehKosFcOjiL2GVGXqDRIXPmFU9p8l80Z9fuilB2TmexB6uBavuEF6blKHkH0VjyVQ3Kg39WzjD0Atrw-G6N4QZ_m0TOcKFfqRPebbH-Q-vzz5UuNPUYRKF4XZIT84QqYLYiBOkkeFIS_abo18YddMbn2AZQcbWamxQV2XPXjNKKwHeQvP1Xdr4hZLt0oGudE2XZtQp60ZCrfDmQteh-KhMWRvsZpoNvE4n8HieAhTTrFq_TjjX9IHlXrPmcONAkQ33jVgRIaw_r_0g"

curl -k -H "$TOKEN" https://127.0.0.1:10250/metrics

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/08/05/Linux/Linux_VIM_Neovim-Lua-from-scratch/" title="Neovim 全 Lua 配置">Neovim 全 Lua 配置</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-08-04T16:36:04.000Z" title="发表于 2023-08-05 00:36:04">2023-08-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.742Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">写这个是为了记录一下我新的VIM配置和VIM的快捷键，看起来lua的版本比之前的scripts要好的多， 无论是使用的方式还是启动的速度， 都比之前快。 
我的需求：
打开代码， 会显示代码高亮。 
vim可以默认满足的要求， 尽可能少的使用鼠标。
代码错误提示 LSP ， 挂了Language Server. ( Already Config, but not work. Thinking About this.
主题， 目前使用的是 catppuccin-frappe
Markdown Preview （ Pending….
代码补全（Already Done.

我的快捷键清单：


Mappings
Actions



 &lt;Leader-1&gt;
高亮第 1 列， 高亮列， 首字母缩进检查


 &lt;Leader-2&gt;
高亮第 3 列， 高亮列， 双空格缩进检查


 &lt;Leader-3&gt;
高亮第 5 列， 高亮列， 4空格缩进检查


 &lt;Leader-4&gt;
高亮第 9 列， 高亮列， 8空格缩进检查


 &lt;Leader-a&gt;
搭配Visual block mode 进行bash shell 的注释，行首添加#


 &lt;Leader-x&gt;
同上，删除注释。


 &lt;Leader-r&gt;
KubeApply (Fixed, Ready to use.)


 &lt;Leader-e&gt;
KubeDelete (Fixed, Ready to use.)


&lt;Leader-dr&gt;
KubeApplyDIr (Fixed, Ready to use.)


&lt;Leader-de&gt;
KubeDeleteDir (Fixed, Ready to use.)


&lt;Leader-ff&gt;
Telescope Find FIles 查找文件


&lt;Leader-fg&gt;
Telescope Find Live grep 过滤文件中的关键字


&lt;Leader-fb&gt;
Telescope Find Buffer 查看Buffer中的数据。VIMbuffer


&lt;Leader-fh&gt;
Telescope Find Help（ Maybe not use， Just record


&lt;Leader-ps&gt;
This is Alias for :PackerSync.


&lt;Leader-ms&gt;
This is Alias for :Mason.


&lt;C-n&gt;
Telescope PageDown 在Insert模式下面的上下移动。


&lt;C-p&gt;
Telescope PageUp 在Insert模式下面的上下移动。


&lt;j / k&gt;
Telescope NORMAL Up&#x2F;Down Normal模式下的上下移动。


&lt;C-x&gt;
Telescope Go to file selection as a split 找到的文件直接水平开新窗口（下方


&lt;C-v&gt;
Telescope Go to file selection as a vsplit 找到的文件直接垂直开新窗口（右侧


&lt;C-t&gt;
Telescope Go to a file in a new tab 找到的文件开新的VIM tab， 感觉不是非常的好用，垂直会经常被用到。


&lt;C-/&gt;
Telescope Show mappings for picker actions (insert mode)  帮助


?
Telescope Show mappings for picker actions (normal mode) 帮助


&lt;M-f&gt;
Scroll left in results window


&lt;M-b&gt;
Scroll right in results window










其他的还在配置和学习中， 先这样把。。 之前用的功能不太多， 有时间继续看。。。。
About NerdFonts.Nerd 类型的字体实际代表了 带有Icon字符集的字体。https://www.nerdfonts.com/#home
其他技巧
不换行显示文本或者代码
set nowrap
直接grep文本的内容并替换当前buffer中的内容.是的，当前打开的文件在VIM里面其实是叫buffer， 因为并未完成实际的写入。
:%! grep KEYWORD 
粘贴的时候保留格式。 
set paste
set nopaste
或者是关闭autoindent
set ai
set noai

压缩多个空格为一个
:%s@  *@ @g
替换所有的空格 为 tab 
:% s@ @\t@g

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/25/Linux/Linux_EC2MultiNetworkCase/" title="EC2 路由表以及多网卡相关问题">EC2 路由表以及多网卡相关问题</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-25T14:00:04.000Z" title="发表于 2023-07-25 22:00:04">2023-07-25</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.730Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">在EC2实例中， 可以使用多个不同的网卡， 但是虚拟网卡其实是共享实例整体带宽的。 假如EC2实例本身有10Gbps的带宽， 那么无论多少个网卡都应该只能有10Gbps的带宽， 其实添加了多个也不会扩展网络上限。 
但是某些大规格的实例会有这种情况， 需要添加多个网卡，并且底层其实提供了多个NetworkCard。 这种情况少见， 但是确实有。 
如果是物理的机器， 那么最好的办法就是链路聚合， 将多个网卡合并使用来扩充这个物理服务器的网络能力。 
这样的实例添加网卡之后其实会有一定的难度，描述一下这个场景： 
环境一台ec2 ， 三张网卡， OS是 Ubuntu 18.04 ， 三张网卡分别是 ens5 ens6 ens7 , 实例中命令展示网卡如下 ： 
root@ip-172-31-43-121:~# ip ad
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link&#x2F;loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1&#x2F;8 scope host lo
       valid_lft forever preferred_lft forever
2: ens5: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000
    link&#x2F;ether 02:4d:73:35:16:8a brd ff:ff:ff:ff:ff:ff
    inet 172.31.43.121&#x2F;20 brd 172.31.47.255 scope global dynamic ens5
       valid_lft 1957sec preferred_lft 1957sec
3: ens6: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000
    link&#x2F;ether 02:5d:02:b4:37:fa brd ff:ff:ff:ff:ff:ff
    inet 172.31.37.95&#x2F;20 brd 172.31.47.255 scope global dynamic ens6
       valid_lft 1955sec preferred_lft 1955sec
4: ens7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 9001 qdisc mq state UP group default qlen 1000
    link&#x2F;ether 02:cf:10:d8:12:80 brd ff:ff:ff:ff:ff:ff
    inet 172.31.40.130&#x2F;20 brd 172.31.47.255 scope global dynamic ens7
       valid_lft 1956sec preferred_lft 1956sec

三张网卡中， ens5上面有一个公网ip地址的绑定， elastic ip , 操作系统看不到， 但是ens5 能找到公网IP地址， 其他网卡都不能。
多网卡路由策略配置多网卡的情况下，手动配置了所有网卡的路由策略之后，公网访问没了， 看看 ip route show： 
看路由表root@ip-172-31-43-121:~# ip r s
default via 172.31.32.1 dev ens7 proto dhcp src 172.31.40.130 metric 100
default via 172.31.32.1 dev ens6 proto dhcp src 172.31.37.95 metric 100
default via 172.31.32.1 dev ens5 proto dhcp src 172.31.43.121 metric 100
172.31.32.0&#x2F;20 dev ens7 proto kernel scope link src 172.31.40.130
172.31.32.0&#x2F;20 dev ens6 proto kernel scope link src 172.31.37.95
172.31.32.0&#x2F;20 dev ens5 proto kernel scope link src 172.31.43.121
172.31.32.1 dev ens7 proto dhcp scope link src 172.31.40.130 metric 100
172.31.32.1 dev ens6 proto dhcp scope link src 172.31.37.95 metric 100
172.31.32.1 dev ens5 proto dhcp scope link src 172.31.43.121 metric 100

ip route show 命令显示的是系统路由表，也就是记录了目的网络和下一跳地址的信息的路由表。它的每个字段的含义是：

default 表示这个路由项是一个默认路由，也就是当没有匹配目的地址的路由项时，就使用这个路由项来发送数据包。
via 表示这个路由项的下一跳地址，也就是数据包要经过的网关地址。
dev 表示这个路由项对应的网卡设备名称，例如 lo, eth0, ens5 等。
proto 表示这个路由项的协议来源，例如 kernel, static, dhcp 等。
src 表示这个路由项的源地址，也就是本机发送数据包时使用的IP地址。
metric 表示这个路由项的度量值，也就是到达目的网络所需的跳数或开销。

这里面显示 我有三个默认路由， 他们的权重是一样的， 那么按照通常的逻辑， 哪个条目在靠前就会把数据包交给谁， 这时候问题出现了。 

如果我的主网卡上面有公网IP地址， 这时候我去访问公网例如baidu， 会发现这个数据是出不去的， 因为默认匹配了第一个条目之后， 从 ens7 这个没有公网ip地址绑定的网卡走了。

看抓包测试一下， 直接开一个Ping命令在后台， ping www.baidu.com  ， 另一个窗口开 tcpdump -i any icmp看结果。
root@ip-172-31-43-121:~# tcpdump -i any icmp -nn
tcpdump: verbose output suppressed, use -v or -vv for full protocol decode
listening on any, link-type LINUX_SLL (Linux cooked), capture size 262144 bytes
14:49:31.480896 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 12, length 64
14:49:32.504867 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 13, length 64
14:49:33.528873 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 14, length 64
14:49:34.552872 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 15, length 64
14:49:35.576877 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 16, length 64
14:49:36.600873 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 17, length 64
14:49:37.624863 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 18, length 64
14:49:38.648871 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 19, length 64
14:49:39.672865 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 20, length 64
14:49:40.696876 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 21, length 64
14:49:41.720872 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 22, length 64
14:49:42.744873 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 23, length 64
14:49:43.768868 IP 172.31.40.130 &gt; 39.156.66.14: ICMP echo request, id 15074, seq 24, length 64


能直接看到这个数据包是从 ens7 的网卡出去的， 确实匹配到了默认的路由条目， 第一条直接发走， 这本来是正确的逻辑， 但是一旦我需要访问公网且该实例的ens5才是公网ip的指定网卡， 那么这个数据包就只有出去， 没有回来的了。
这确实避免了VPC内部访问的网络限制问题， 但是公网的访问也没有了。 这是一个路由条目的优先级问题， 基于这个问题应该如何修复呢？ 
调整路由条目的优先级调整默认路由的优先级， 让数据匹配默认路由的时候优先匹配ens5的条目。 
150  ip route del default via 172.31.32.1 dev ens5 proto dhcp src 172.31.43.121 metric 100
151  ip route add default via 172.31.32.1 dev ens5 proto dhcp src 172.31.43.121 metric 10

上面的命令可以将ens5 的默认路由metric 设置成 10 ，整体的路由表会变成这样： 
root@ip-172-31-43-121:&#x2F;etc&#x2F;netplan# ip r s
default via 172.31.32.1 dev ens5 proto dhcp src 172.31.43.121 metric 10
default via 172.31.32.1 dev ens7 proto dhcp src 172.31.40.130 metric 100
default via 172 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/25/Linux/Linux_MultiNetworkCard-RoutePolicy/" title="Linux配置网卡策略路由">Linux配置网卡策略路由</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-25T08:41:36.000Z" title="发表于 2023-07-25 16:41:36">2023-07-25</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.735Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">会遇到Linux多个网卡的时候， 网络并不会保证源地址进网卡 ， 源地址出网卡。 
大佬说， IP地址的其实关联的是操作系统，并不是特定的网卡， 所以对于os来说， 邻居子系统会选择一下网络数据从哪个物理网卡出。 
这样就会导致，数据包会在网卡之间Forward一下，而转发出来的数据包， 会被aws vpc 丢包， 因为出入栈的地址不一样了。 
对于这个行为， 需要配置操作系统的路由策略来解决， 让流量从相同的网卡进出。 
记录一下这个配置。
UbuntuUbuntu 直接使用的netplan ，在配置文件里面直接指定policy就可以了。 
配置文件位置： &#x2F;etc&#x2F;netplan&#x2F;* 
配置文件内容大致如下： 
:~# cat /etc/netplan/50-cloud-init.yaml
# This file is generated from information provided by the datasource.  Changes
# to it will not persist across an instance reboot.  To disable cloud-init's
# network configuration capabilities, write a file
# /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg with the following:
# network: &#123;config: disabled&#125;
network:
    ethernets:
        ens4:
            dhcp4: true
            dhcp6: false
            match:
                macaddress: 02:ab:8e:b1:ba:36
            set-name: ens4
            routes:
             - to: 0.0.0.0/0
               via: 172.31.48.1
               table: 1000
             - to: 172.31.51.248
               via: 0.0.0.0
               scope: link
               table: 1000
            routing-policy:
              - from: 172.31.51.248
                table: 1000
        ens6:
            dhcp4: true
            dhcp6: false
            match:
                macaddress: 02:64:69:51:00:f2
            set-name: ens6
            routes:
             - to: 0.0.0.0/0
               via: 172.31.48.1
               table: 1001
             - to: 172.31.59.199
               via: 0.0.0.0
               scope: link
               table: 1001
            routing-policy:
              - from: 172.31.59.199
                table: 1001
    version: 2

Redhat&#x2F;CentOS&#x2F;Fedora&#x2F;AmazonLinux2旧方式设置设置开机配置路由的方式， 使用旧脚本得方式需要禁用NetworkManager，完全手动配置networkd的脚本。

安装旧的脚本管理工具
yum install -y network-scripts

确认配置文件是否正确
[root@ip-172-31-11-110 network-scripts]# ll
total 244
-rw-r--r--. 1 root root   174 Dec 17 12:30 ifcfg-eth0
-rw-r--r--. 1 root root   278 Dec 17 15:56 ifcfg-eth1
-rw-r--r--. 1 root root   254 Feb 15  2021 ifcfg-lo
-rwxr-xr-x. 1 root root  2123 Feb 15  2021 ifdown
-rwxr-xr-x. 1 root root   646 Feb 15  2021 ifdown-bnep
-rwxr-xr-x. 1 root root  6419 Feb 15  2021 ifdown-eth
-rwxr-xr-x. 1 root root   769 Feb 15  2021 ifdown-ippp
-rwxr-xr-x. 1 root root  4536 Feb 15  2021 ifdown-ipv6
lrwxrwxrwx. 1 root root    11 Feb 15  2021 ifdown-isdn -> ifdown-ippp
-rwxr-xr-x. 1 root root  2064 Feb 15  2021 ifdown-post
-rwxr-xr-x. 1 root root   870 Feb 15  2021 ifdown-routes
-rwxr-xr-x. 1 root root  1458 Feb 15  2021 ifdown-sit
-rwxr-xr-x. 1 root root  1621 Jul 26  2020 ifdown-Team
-rwxr-xr-x. 1 root root  1556 Jul 26  2020 ifdown-TeamPort
-rwxr-xr-x. 1 root root  1462 Feb 15  2021 ifdown-tunnel
-rwxr-xr-x. 1 root root  5463 Feb 15  2021 ifup
-rwxr-xr-x. 1 root root 12270 Feb 15  2021 ifup-aliases
-rwxr-xr-x. 1 root root   906 Feb 15  2021 ifup-bnep
-rwxr-xr-x. 1 root root 13776 Feb 15  2021 ifup-eth
-rwxr-xr-x. 1 root root 12068 Feb 15  2021 ifup-ippp
-rwxr-xr-x. 1 root root 11891 Feb 15  2021 ifup-ipv6
lrwxrwxrwx. 1 root root     9 Feb 15  2021 ifup-isdn -> ifup-ippp
-rwxr-xr-x. 1 root root   643 Feb 15  2021 ifup-plip
-rwxr-xr-x. 1 root root  1057 Feb 15  2021 ifup-plusb
-rwxr-xr-x. 1 root root  5000 Feb 15  2021 ifup-post
-rwxr-xr-x. 1 root root  2001 Feb 15  2021 ifup-routes
-rwxr-xr-x. 1 root root  3303 Feb 15  2021 ifup-sit
-rwxr-xr-x. 1 root root  1755 Jul 26  2020 ifup-Team
-rwxr-xr-x. 1 root root  1876 Jul 26  2020 ifup-TeamPort
-rwxr-xr-x. 1 root root  2879 Feb 15  2021 ifup-tunnel
-rwxr-xr-x. 1 root root  1836 Feb 15  2021 ifup-wireless
-rwxr-xr-x. 1 root root  5421 Feb 15  2021 init.ipv6-global
-rw-r--r--. 1 root root 20431 Feb 15  2021 network-functions
-rw-r--r--. 1 root root 31037 Feb 15  2021 network-functions-ipv6
-rw-r--r--. 1 root root    40 Dec 17 16:00 route-eth0
-rw-r--r--. 1 root root    40 Dec 17 16:00 route-eth1
-rw-r--r--. 1 root root    28 Dec 17 15:40 rule-eth0
-rw-r--r--. 1 root root    28 Dec 17 15:41 rule-eth1

书写文件内容，需要配置的文件如下： 
[root@ip-172-31-11-110 network-scripts]$ cat ifcfg-eth0
# Created by cloud-init on instance boot automatically, do not edit.
#
BOOTPROTO=dhcp
DEVICE=eth0
HWADDR=02:20:22:29:a4:0c
ONBOOT=yes
STARTMODE=auto
TYPE=Ethernet
USERCTL=no
[root@ip-172-31-11-110 network-scripts]$ cat ifcfg-eth1
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
BOOTPROTO=dhcp
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=eth1
UUID=80caddf5-1347-4246-827e-5e0146c7f2c5
DEVICE=eth1
ONBOOT=yes

[root@ip-172-31-11-110 network-scripts]$ cat route-eth0
default via 172.31.0.1 dev eth0 table 1

[root@ip-172-31-11-110 network-scripts]$ cat route-eth1
default via 172.31.0.1 dev eth1 table 2

[root@ip-172-31-11-110 network-scripts]$ cat rule-eth0
from 172.31.11.110 lookup 1

[root@ip-172-31-11-110 network-scripts]$ cat rule-eth1
from 172.31.11.124 lookup 2

[root@ip-172-31-11-110 network-scripts]# cat /etc/iprout ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/12/Linux/Linux_Git%E5%91%BD%E4%BB%A4%E5%AD%A6%E4%B9%A0/" title="Git常见的命令">Git常见的命令</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-11T22:52:18.000Z" title="发表于 2023-07-12 06:52:18">2023-07-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.732Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">远端创建， Clone本地Step 1Github上面创建一个新的仓库， 页面创建即可， 然后记录下URL。
https://github.com/LiarLee/vps-init.git

Step 2本地创建目录， 并初始化本地的仓库路径。
mkdir vps-init
cd ./vps-init
git clone https://github.com/LiarLee/vps-init.git

关联远端仓库创建一个本地仓库。
mkdir vps-init
cd ./vps-init
git init .
touch README
echo "For init server use DroneCI."
git add -A
git commit -m "init"
git remote add origin https://github.com/LiarLee/vps-init.git
git -u origin master

配置代理# 设置代理
ec2-user@arch ~> gitc config --global http.proxy socks5://1.1.1.1:7890
ec2-user@arch ~> git config --global https.proxy socks5://1.1.1.1:7890
# 取消代理
git config --global --unset http.proxy git config --global --unset https.proxy

取消git的特定文件追踪最好的办法是直接使用gitignore忽略这个文件， 但是开始创建仓库的时候可能想不到， 所以已经追踪的文件需要取消。 
查看这个哪些文件会受到影响： 
git rm -r -n --cached ./plugin 

删除这些文件的追踪： 
git rm -r --cached ./plugin


</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/12/Linux/Linux_Redhat-9-MySQL%E5%86%85%E5%AD%98%E9%97%AE%E9%A2%98/" title="Linux Redhat 9 oom不触发">Linux Redhat 9 oom不触发</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-11T22:52:18.000Z" title="发表于 2023-07-12 06:52:18">2023-07-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.741Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">测试环境Amazon AMI ID: ami-0e54fe8afeb8fa59a
Operating System: Red Hat Enterprise Linux 9.2 (Plow)
Kernel: Linux 5.14.0-284.11.1.el9_2.x86_64
MySQL Version:  Ver 8.0.33 for Linux on x86_64 (MySQL Community Server - GPL)
测试步骤：
使用上面的AMI启动一个新的实例， 在实例启动之后ssh连接进去。

按照MySQL官方的 repo 安装一个社区的版本。
# https://dev.mysql.com/downloads/repo/yum/ 
# (mysql80-community-release-el9-1.noarch.rpm)

wget https://dev.mysql.com/get/mysql80-community-release-el9-1.noarch.rpm

dnf install -y ./mysql80-community-release-el9-1.noarch.rpm

dnf makecache -y 

dnf update -y

# Refer this doc to install mysql-community packages.
# https://dev.mysql.com/doc/refman/8.0/en/linux-installation-yum-repo.html
dnf install -y mysql-community-server

# Check the installation success.
systemctl status mysqld

编辑MySQL配置文件， 添加如下的参数:
vim /etc/my.cnf
#设置buffer pool 的参数大于物理内存。 例如 os 本身由可用的内存是 16G， 那么设置一个更大的值即可。
innodb_buffer_pool_size = 40G

sudo systemctl enable --now mysqld

sudo systemctl status mysqld

# 获取root用户的临时初始化密码： 
sudo grep 'temporary password' /var/log/mysqld.log

# 使用root用户登录， 并查看配置已经生效。
mysql -u root -p

MySQL [(none)]> show variables like "%buffer_pool_size%";
+-------------------------+-------------+
| Variable_name           | Value       |
+-------------------------+-------------+
| innodb_buffer_pool_size | 42949672960 |
+-------------------------+-------------+
1 row in set (0.005 sec)

创建database 以及 table：
create database test;

CREATE TABLE tt1(
id int NOT NULL AUTO_INCREMENT PRIMARY KEY,
person_id tinyint not null ,
person_name1 VARCHAR(3000) ,
person_name2 VARCHAR(3000) ,
person_name3 VARCHAR(3000) ,
person_name4 VARCHAR(3000) ,
person_name5 VARCHAR(3000) ,
gmt_create datetime ,
gmt_modified datetime
) ;	

insert into tt1 (person_id, person_name1, person_name2, person_name3, person_name4, person_name5, gmt_create, gmt_modified)
values (1, lpad(&#39;&#39;,3000,&#39;*&#39;), lpad(&#39;&#39;,3000,&#39;*&#39;), lpad(&#39;&#39;,3000,&#39;*&#39;), lpad(&#39;&#39;,3000,&#39;*&#39;), lpad(&#39;&#39;,3000,&#39;*&#39;), now(), now());

# 反复执行这个命令， 复制全表的数据， 大概执行 10 次左右。
insert into tt1 (person_id, person_name1, person_name2, person_name3, person_name4, person_name5, gmt_create, gmt_modified)
select person_id, person_name1, person_name2, person_name3, person_name4, person_name5, now(), now() from tt1;


# 查看当前table的信息 ，确认一下数据量。 
show table status like &#39;tt1&#39;\G

在另一个机器上面 select 这个表格。
select count(*) from tt1;

如果数据量足够大的话， 那么就会占用超过物理内存的空间，导致OOM。



问题其他版本的os上面都会触发oom， 然后MySQL会被干掉重启， 只有Redhat 9 这个版本的os是不会触发的。
测试了Ubuntu 22， amazonlinux2， amazonlinux2023 都没由这个问题， 他们的内核都是 5.10+ 。
Redhat 9 这个版本的表现是， 在接近物理内存容量极限的时候， os 开始非常频繁的扫描并尝试回收内存的空间， 导致命令的响应变慢，我开了sar的监控命令，返回数据的速度也会便的比较慢， 大部分的进程会慢慢变成 Uninterreptable状态， 并且数量越来越多， 最终会导致实例的网络子系统不工作， 完全不响应任何的网络报文，实例的健康检查失败。CPU使用率依旧还在， EBS的监控显示这个卷满速度读取输出， 并且非常平稳, 响应时间以及队列的长度也没有异常。
如果这个时候尝试取手动触发oom， 是可以成功的， 在杀掉Mysqld之后， os就恢复了正常。
分析思路可以确定的是 磁盘的工作是正常的， CPU使用率慢慢增长但是也是正常的。 这部分的工作没有问题。 
在发生问题的时候因为网络无法正常的工作，ssh已经断开， 无法确定当时的情况。 
初步判断是内存的问题， 但是具体的差异是哪里。
第一sar命令中的记录
第二尝试对比与行为正常的os的差异， 获取sysctl -a 并记录到文件中。 
对比完关于内存部分的参数， 完全没有任何的差别， 不是设置或者配置文件导致的这个问题。
min 水位的设置是默认的， 与其他的发行版本一直， 测试的时候用的相同的规格， 所以不存在这类的差异。
第三触发一个kdump 看看网络不相应的时候 os 当时的状态，  这个不太会。
第四查看内核的编译选项， 看看有什么不同。
第五学习os 内存的部分， 看看具体这个版本的内核如何进行内存的回收的， 或者触发oom的条件有哪些。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/12/Linux/Linux_%E5%8D%87%E7%BA%A7%E5%92%8C%E6%B8%85%E7%90%86%E5%86%85%E6%A0%B8%E7%9A%84%E6%AD%A5%E9%AA%A4/" title="升级以及清理内核的步骤">升级以及清理内核的步骤</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-11T22:52:18.000Z" title="发表于 2023-07-12 06:52:18">2023-07-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.748Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">通常情况下升级内核版本的步骤:
yum makecache -y 

yum update -y 

grub2-editenv list

grub2-set-default 'CentOS Linux (3.10.xxxxx.el7.elrepo.x86_64) 7 (Core)' # entry_name

systemctl reboot

清理旧版本的步骤； 
rpm -qa  kernel* 
# 这个命令会列出所有当前已经安装的版本的内核， 然后手动使用命令移除对应的软件包即可。 
# 我记得是有一个命令可以完全移除全部未使用的内核， 可能记错了， 这命令应该只有在ubuntu 上面是存在的。

yum groupinstall -y "Development Tools"
categories: Linux
yum install -y kernel kernel-devel kernel-debug

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/11/Linux/Linux_MemoryManagement/" title="Linux内存管理笔记">Linux内存管理笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-11T06:37:43.000Z" title="发表于 2023-07-11 14:37:43">2023-07-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.735Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">
内存管理部分的笔记Crash命令的使用使用这个命令需要有debuginfo 以及kernel debug 的数据包， 同时可能需要gdb。 
需要在配置文件里面开启这个 仓库： rhel-8-baseos-rhui-debug-rpms
具体的步骤也可以看这个文档， 来自Redhat 官方： https://access.redhat.com/solutions/9907
 yum install -y kernel-debuginfo 
# 使用这个命令就可以安装， 但是尺寸非常的大。
crash /boot/vmlinuz-$(uname -a)

使用命令crash来进行 PM 和 VM的对应关系：
内核的debug文件在： &#x2F;var&#x2F;lib&#x2F;debug&#x2F;lib&#x2F;modules&#x2F;kernel-version&#x2F;
 使用crash命令： 
~ # ❯❯❯ crash

crash 7.3.2-4.el8
Copyright (C) 2002-2022  Red Hat, Inc.
Copyright (C) 2004, 2005, 2006, 2010  IBM Corporation
Copyright (C) 1999-2006  Hewlett-Packard Co
Copyright (C) 2005, 2006, 2011, 2012  Fujitsu Limited
Copyright (C) 2006, 2007  VA Linux Systems Japan K.K.
Copyright (C) 2005, 2011, 2020-2022  NEC Corporation
Copyright (C) 1999, 2002, 2007  Silicon Graphics, Inc.
Copyright (C) 1999, 2000, 2001, 2002  Mission Critical Linux, Inc.
This program is free software, covered by the GNU General Public License,
and you are welcome to change it and/or distribute copies of it under
certain conditions.  Enter "help copying" to see the conditions.
This program has absolutely no warranty.  Enter "help warranty" for details.

GNU gdb (GDB) 7.6
Copyright (C) 2013 Free Software Foundation, Inc.
License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html>
This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.  Type "show copying"
and "show warranty" for details.
This GDB was configured as "x86_64-unknown-linux-gnu"...

WARNING: kernel relocated [592MB]: patching 107327 gdb minimal_symbol values

      KERNEL: /usr/lib/debug/lib/modules/4.18.0-477.13.1.el8_8.x86_64/vmlinux  [TAINTED]
    DUMPFILE: /proc/kcore
        CPUS: 2
        DATE: Tue Jul 11 17:07:01 CST 2023
      UPTIME: 01:04:34
LOAD AVERAGE: 0.15, 0.03, 0.01
       TASKS: 226
    NODENAME: center
     RELEASE: 4.18.0-477.13.1.el8_8.x86_64
     VERSION: #1 SMP Thu May 18 10:27:05 EDT 2023
     MACHINE: x86_64  (2199 Mhz)
      MEMORY: 7.9 GB
         PID: 7657
     COMMAND: "crash"
        TASK: ffff9ce7d835a800  [THREAD_INFO: ffff9ce7d835a800]
         CPU: 0
       STATE: TASK_RUNNING (ACTIVE)

crash> vm -p [pid]

PID: 913      TASK: ffff9ce7c75fd000  CPU: 0    COMMAND: "sshd"
       MM               PGD          RSS    TOTAL_VM
ffff9ce7c11b8000  ffff9ce7c75ae000  7604k    76644k
      VMA           START       END     FLAGS FILE
ffff9ce7c759f828 55a7fcc7b000 55a7fcd4c000 8000875 /usr/sbin/sshd
VIRTUAL     PHYSICAL
55a7fcc7b000  12026b000
55a7fcc7c000  1201df000
55a7fcc7d000  1201ec000
55a7fcc7e000  1200c7000
55a7fcc7f000  120c43000
55a7fcc80000  10fa79000
55a7fcc81000  11fdd3000
55a7fcc82000  11087f000
55a7fcc83000  11fa8d000
55a7fcc84000  10fe05000
55a7fcc85000  110870000
55a7fcc86000  10fa2c000
55a7fcc87000  10f9fc000
55a7fcc88000  10fdab000
55a7fcc89000  11f296000
55a7fcc8a000  1117ec000
55a7fcc8b000  10fdac000
55a7fcc8c000  120c65000
55a7fcc8d000  12011b000
55a7fcc8e000  110714000
55a7fcc8f000  110c83000
55a7fcc90000  110c90000
55a7fcc91000  110d2b000
55a7fcc92000  120730000
55a7fcc93000  12076f000
55a7fcc94000  1207e8000
55a7fcc95000  110c2f000
55a7fcc96000  110c3c000
55a7fcc97000  120650000
55a7fcc98000  1206c1000
55a7fcc99000  120c67000
55a7fcc9a000  120c0f000
55a7fcc9b000  FILE: /usr/sbin/sshd  OFFSET: 20000
55a7fcc9c000  11d46d000
55a7fcc9d000  10fe01000
55a7fcc9e000  10fdb9000
55a7fcc9f000  10fde7000
55a7fcca0000  FILE: /usr/sbin/sshd  OFFSET: 25000
# 结果省略了后面的部分， 太长了。。 。。 


可以看到内存的映射关系， notmapped 表示没有被映射到物理内存的部分。 
一般来说 后面的三位是一样的， 如果是THP的话， 那么后面的五位是一样的。

这个vtop 可以直接查看里面保存的内容以及具体的映射关系。 
crash> vtop 55d5473fc000
VIRTUAL     PHYSICAL
55d5473fc000  (not accessible)

rd命令可以读取指定的内存虚拟地址之后的偏移量。
crash> rd 55d54879d000 100
rd: invalid user virtual address: 55d54879d000  type: "64-bit UVADDR"


超过内存申请容量的使用， 会导致 访问内存越界， 例如申请了1G的内存，但是尝试写入超出的数据量， 会导致数据写到后续不属于这个进程的空间上， 而这个时候内核会触发一个 segfault， 来终止这个进程。 
这个报错不是立刻发生的，可能确实会溢出一部分。
匿名页面 实际上是 mmap with MAP_ANONYMOUS flag映射出来的虚拟内存地址， 当需要第一次去写匿名页面的时候， 会将物理内存的地址映射到虚拟内存并将其中填0.
overcommit 0 可以所有的地址，   1 无限制，虚拟内存没有限制， 2  按照一定的比例进行计算， 最终的结果。 
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/11/AWS/EKS_Add-redhat-release-to-eks/" title="添加一个Redhat到EKS集群， 基于Packer的步骤">添加一个Redhat到EKS集群， 基于Packer的步骤</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-11T03:29:53.000Z" title="发表于 2023-07-11 11:29:53">2023-07-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.714Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">Copy From zhojiew 的私有仓库文档， 已经经过授权  ~ 

官方提供了基于packer工具的构建脚本
这里手动把相关的步骤执行下，基于redhat9构建一个自定义ami。据称eks优化的ami也是通过以下步骤完成的
手动构建ami拉仓库
cd /home/ec2-suer
sudo yum install git -y
git clone https://github.com/awslabs/amazon-eks-ami.git

配置环境变量
KUBERNETES_VERSION=1.26.4 
KUBERNETES_BUILD_DATE=2023-05-11 
BINARY_BUCKET_NAME=amazon-eks
BINARY_BUCKET_REGION=cn-north-1
DOCKER_VERSION=20.10.23-1.amzn2.0.1
CONTAINERD_VERSION=1.6.*
RUNC_VERSION=1.1.5-1.amzn2
CNI_PLUGIN_VERSION=v0.8.6
PULL_CNI_FROM_GITHUB=true
SONOBUOY_E2E_REGISTRY=""
PAUSE_CONTAINER_VERSION=3.5
CACHE_CONTAINER_IMAGES=false
WORKING_DIR=/tmp/worker
TEMPLATE_DIR=/home/ec2-user/amazon-eks-ami

复制文件更新内核（可以跳过）
mkdir -p $WORKING_DIR
mkdir -p $WORKING_DIR/log-collector-script
mkdir -p $WORKING_DIR/bin

mv $TEMPLATE_DIR/files/* $WORKING_DIR/
mv $TEMPLATE_DIR/log-collector-script/linux/eks-log-collector.sh $WORKING_DIR/log-collector-script/
sudo chmod -R a+x $WORKING_DIR/bin/
sudo mv /tmp/worker/bin/* /usr/bin/

# sudo bash $TEMPLATE_DIR/scripts/upgrade_kernel.sh
KERNEL_VERSION=5.10
sudo grubby \
  --update-kernel=ALL \
  --args="psi=1"

sudo grubby \
  --update-kernel=ALL \
  --args="clocksource=tsc tsc=reliable"

sudo reboot

构建的主要逻辑在脚本install-worker.sh中
# sudo bash $TEMPLATE_DIR/scripts/install-worker.sh
export AWS_DEFAULT_OUTPUT="json"
ARCH="amd64"

sudo yum update -y
sudo yum install -y \
  chrony \
  conntrack \
  curl \
  ethtool \
  ipvsadm \
  jq \
  nfs-utils \
  socat \
  unzip \
  wget \
  yum-utils \
  yum-plugin-versionlock \
  mdadm \
  pigz

# Remove any old kernel versions.
sudo package-cleanup --oldkernels --count=1 -y

# Remove the ec2-net-utils package
if yum list installed | grep ec2-net-utils; then sudo yum remove ec2-net-utils -y -q; fi

sudo mkdir -p /etc/eks/
sudo mv $WORKING_DIR/configure-clocksource.service /etc/eks/configure-clocksource.service

# iptables 
sudo mv $WORKING_DIR/iptables-restore.service /etc/eks/iptables-restore.service

# awscli
sudo yum install less unzip jq -y
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
sudo ./aws/install --update
complete -C '/usr/local/bin/aws_completer' aws

# systemd
sudo mv "$&#123;WORKING_DIR&#125;/runtime.slice" /etc/systemd/system/runtime.slice

编译安装runc
# install runc and lock version
# sudo yum install -y runc-$&#123;RUNC_VERSION&#125;
sudo yum install libseccomp-devel.x86_64 golang -y
go env -w GOPROXY=https://goproxy.io,direct
git clone https://github.com/opencontainers/runc
cd runc
make
sudo make install

安装containerd
# install containerd and lock version

sudo yum install -y yum-utils device-mapper-persistent-data lvm2
# sudo yum install -y containerd-$&#123;CONTAINERD_VERSION&#125;
yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
sudo yum install -y containerd # 1.6.21

配置containerd
sudo mkdir -p /etc/eks/containerd
sudo mv $WORKING_DIR/containerd-config.toml /etc/eks/containerd/containerd-config.toml

# containerd and related service
sudo mv $WORKING_DIR/kubelet-containerd.service /etc/eks/containerd/kubelet-containerd.service
sudo mv $WORKING_DIR/sandbox-image.service /etc/eks/containerd/sandbox-image.service
sudo mv $WORKING_DIR/pull-sandbox-image.sh /etc/eks/containerd/pull-sandbox-image.sh
sudo mv $WORKING_DIR/pull-image.sh /etc/eks/containerd/pull-image.sh
sudo chmod +x /etc/eks/containerd/pull-sandbox-image.sh
sudo chmod +x /etc/eks/containerd/pull-image.sh

sudo mkdir -p /etc/systemd/system/containerd.service.d
cat &lt;&lt; EOF | sudo tee /etc/systemd/system/containerd.service.d/10-compat-symlink.conf
[Service]
ExecStartPre=/bin/ln -sf /run/containerd/containerd.sock /run/dockershim.sock
EOF

cat &lt;&lt; EOF | sudo tee -a /etc/modules-load.d/containerd.conf
overlay
br_netfilter
EOF

cat &lt;&lt; EOF | sudo tee -a /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
EOF

# skip docker

日志轮换配置
# logrotate
sudo mv $WORKING_DIR/logrotate-kube-proxy /etc/logrotate.d/kube-proxy
sudo mv $WORKING_DIR/logrotate.conf /etc/logrotate.conf
sudo chown root:root /etc/logrotate.d/kube-proxy
sudo chown root:root /etc/logrotate.conf
sudo mkdir -p /var/log/journal

下载kubelet和aws-iam-authenticator
## download bin in china region
S3_DOMAIN="amazonaws.com.cn"

S3_PATH="s3://amazon-eks/1.26.4/2023-05-11/bin/linux/amd64"

# Verify that the aws-iam-authenticator is at last v0.5.9 or greater
BINARIES=(
  kubelet
  aws-iam-authenticator
)

for binary in $&#123;BINARIES[*]&#125;; do
    aws s3 cp $S3_PATH/$binary . --region cn-north-1
    sudo chmod +x $binary
    sudo mv $binary /usr/bin/
done

继续配置服务
# kubernetes
sudo mkdir -p /etc/kubernetes/manifests
sudo mkdir -p /var/lib/kubernetes
sudo mkdir -p /var/lib/kubelet
sudo mkdir -p /opt/cni/bin

CNI_PLUGIN_FILENAME="cni-plugins-linux-$&#123;ARCH&#125;-$&#123;CNI_PLUGIN_VERSION&#125;"
aws s3 cp --region $BINARY_BUCKET_REGION $S3_PATH/$&#123;CNI_PLUGIN_FILENAME&#125;.tgz .
su ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/10/Linux/Linux_Perf-DirectIO-BufferIO/" title="BufferIO与DirectIO的比较">BufferIO与DirectIO的比较</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-10T03:34:36.000Z" title="发表于 2023-07-10 11:34:36">2023-07-10</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.737Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">测试方法使用BufferIO的方式， 测试文件的写入： 
#!/bin/bash

perf record -T -C 0 -- taskset -c 0 dd if=/dev/zero of=./a.dat bs=4k count=16384

使用DirectIO的方式， 测试文件的写入:
#!/bin/bash

perf record -T -C 0 -- taskset -c 0 dd if=/dev/zero of=./a.dat bs=4k count=16384 oflag=direct

运行结果BufferIO:
[root@ip-172-31-53-200 perf_records]# ./start_test_bufferio.sh
16384+0 records in
16384+0 records out
67108864 bytes (67 MB, 64 MiB) copied, 0.118848 s, 565 MB/s
[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.225 MB perf.data (485 samples) ]

ll -h 
-rw-r--r--. 1 root root  64M Jun  1 13:45 a.dat

[root@ip-172-31-53-200 ~]# dstat -tf
----system---- -----cpu0-usage----------cpu1-usage----------cpu2-usage----------cpu3-usage---- dsk/nvme0n1 ---net/lo-----net/eth0- ---paging-- ---system--
     time     |usr sys idl wai stl:usr sys idl wai stl:usr sys idl wai stl:usr sys idl wai stl| read  writ| recv  send: recv  send|  in   out | int   csw
01-06 13:35:48|  2   0  99   0   0:  1   0  99   0   0:  0   1  98   0   0:  1   0  99   0   0|8192B   35k|1096B 1096B: 968B  828B|   0     0 | 712   971
01-06 13:35:49| 25   9  60   5   0:  0   0  99   0   0:  4  12  84   0   0:  4   8  90   0   0|   0    64M|1096B 1096B: 576B  756B|   0     0 |2283  1311
01-06 13:35:50|  6   1  94   0   0:  1   1  99   0   0: 16   2  83   0   0:  0   1  99   0   0|   0     0 |1096B 1096B: 156B  418B|   0     0 | 954  1018

DirectIO:
[root@ip-172-31-53-200 perf_records]# ./start_test_directio.sh
16384+0 records in
16384+0 records out
67108864 bytes (67 MB, 64 MiB) copied, 10.4225 s, 6.4 MB/s
[ perf record: Woken up 9 times to write data ]
[ perf record: Captured and wrote 2.417 MB perf.data (41489 samples) ]

[root@ip-172-31-53-200 ~]# dstat -tf
----system---- -----cpu0-usage----------cpu1-usage----------cpu2-usage----------cpu3-usage---- dsk/nvme0n1 ---net/lo-----net/eth0- ---paging-- ---system--
     time     |usr sys idl wai stl:usr sys idl wai stl:usr sys idl wai stl:usr sys idl wai stl| read  writ| recv  send: recv  send|  in   out | int   csw
01-06 13:36:36|  0   1  99   0   0:  1   1  99   0   0:  0   1 100   0   0:  0   1 100   0   0|   0     0 |1097B 1097B: 688B  624B|   0     0 | 622   930
01-06 13:36:37|  3   4  32  61   0:  4  14  81   0   0:  1   0  97   0   0:  0   5  94   0   0|   0  4277k|1095B 1095B: 332B  338B|   0     0 |6434  3133
01-06 13:36:38|  3   3   0  92   0:  1   1  99   0   0:  3   1  96   0   0:  0   0  99   0   0|   0  6421k|1096B 1096B:  52B  174B|   0     0 |8767  4148
01-06 13:36:39|  4   4   0  92   0:  0   0  99   0   0:  4   1  96   0   0:  2   0 100   0   0|   0  6431k|1096B 1096B:  52B  150B|   0     0 |8790  4191
01-06 13:36:40|  4   4   0  91   0:  0   1  99   0   0:  2   1  96   0   0:  0   1  99   0   0|   0  6320k|1096B 1096B:  52B  142B|   0     0 |8744  4092
01-06 13:36:41|  4   4   0  92   0:  1   0  99   0   0:  3   0  96   0   0:  0   0 100   0   0|   0  6216k|1096B 1096B:  52B  142B|   0     0 |8662  4103
01-06 13:36:42|  3   4   0  92   0:  1   1  99   0   0:  2   2  96   0   0:  0   0  99   0   0|   0  7492k|1576B 1576B:  52B  134B|   0     0 |8756  4099
01-06 13:36:43|  3   3   0  91   0:  1   0  99   0   0:  4   1  96   0   0:  0   0 100   0   0|   0  6284k|1096B 1096B:  52B  134B|   0     0 |8720  4077
01-06 13:36:44|  4   2   0  92   0:  0   0  99   0   0:  2   1  96   0   0:  0   0  99   0   0|   0  6296k|1096B 1096B:  52B  134B|   0     0 |8788  4067
01-06 13:36:45|  4   5   0  91   0:  1   0  99   0   0:  4   0  96   0   0:  1   0  99   0   0|   0  6368k|1096B 1096B:  52B  134B|   0     0 |8792  4071
01-06 13:36:46|  3   5   0  92   0:  1   1  99   0   0:  4   1  96   0   0:  0   0 100   0   0|   0  5904k|1096B 1096B:  52B  134B|   0     0 |8576  3893
01-06 13:36:47| 25   7   0  69   0:  0   0  99   0   0:  2   1  96   0   0:  0   0 100   0   0|   0  4811k|1097B 1097B: 364B  763B|   0     0 |7035  3360
01-06 13:36:48|  4   0  96   0   0:  1   0  99   0   0: 22   3  75   1   0:  0   1 100   0   0|2642k  109k|1095B 1095B: 208B  472B|   0     0 | 977  1008
01-06 13:36:49|  0   1  99   0   0:  0   0 100   0   0:  0   0  98   0   0:  0   0  99   0   0|   0     0 |1096B 1096B: 104B  276B|   0     0 | 640   903

Perf 采样结果BufferIO：

DirectIO：

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/07/05/AWS/AWS_VPC%20Flowlog%E8%A7%A3%E6%9E%90/" title="VPCFlowlog解析">VPCFlowlog解析</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-05T03:33:43.000Z" title="发表于 2023-07-05 11:33:43">2023-07-05</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.713Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/AWS/">AWS</a></span></div><div class="content">记录VPC Flow Log 怎么看https://docs.amazonaws.cn/vpc/latest/userguide/flow-logs.html#flow-log-recordshttps://docs.amazonaws.cn/vpc/latest/userguide/flow-logs-records-examples.html#flow-log-example-tcp-flag  
vpc flow log里的tcp-flags记录的不是某个单个tcp包头里的flag，而是单次观察的时间窗口里这条flow的所有tcp包出现过的tcp flag的合计。  
TCP flags can be OR-ed during the aggregation interval. For short connections, the flags might be set on the same line in the flow log record, for example, 19 for SYN-ACK and FIN, and 3 for SYN and FIN. For an example, see TCP flag sequence.For general information about TCP flags (such as the meaning of flags like FIN, SYN, and ACK), see TCP segment structureon [Wikipedia].
这个记录里面的值， 是这样计算出来的， 从右向左 ， 从 0 次方开始计算。

FIN  2^0
SYN 2^1 
RST 2^2
PSH 2^3
ACK 2^4
URG 2^5
ECE 2^6
CWR 2^7

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/06/28/Linux/Linux_DebugLinuxCrashOnEC2/" title="发送一个NMI unknown 事件给OS">发送一个NMI unknown 事件给OS</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-06-28T09:12:22.000Z" title="发表于 2023-06-28 17:12:22">2023-06-28</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.727Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">在ec2触发linux的crash发送一个诊断请求给EC2， 触发os本身NMI Unknown事件，这个时间会触发Kdump记录当时的现场。
aws ec2 send-diagnostic-interrupt --region cn-north-1 --instance-id i-********************

记录下来的现场文件保存在 &#x2F;var&#x2F;crash&#x2F; 

[root@mysql 5.14.0-284.11.1.el9_2.x86_64]# ll /var/crash/
total 0
drwxr-xr-x. 2 root root 67 Jun  6 05:22 127.0.0.1-2023-06-06-05:22:11
drwxr-xr-x. 2 root root 67 Jun  6 08:58 127.0.0.1-2023-06-06-08:58:20
drwxr-xr-x. 2 root root 67 Jun  9 09:39 127.0.0.1-2023-06-09-09:39:56


使用Crash命令进行分析， 需要安装kernel-debug 和 kernel-debuginfo kernel-devel
[root@mysql 5.14.0-284.11.1.el9_2.x86_64]#  crash /usr/lib/debug/lib/modules/5.14.0-284.11.1.el9_2.x86_64/vmlinux /var/crash/127.0.0.1-2023-06-09-09\:39\:56/vmcore

相关文档： 
New – Trigger a Kernel Panic to Diagnose Unresponsive EC2 Instances
发送诊断中断（适用于高级用户）
主要的问题是怎么解读这个结果， 目前我的理解是 找大佬。
Cscope 查看内核源代码# 下载源代码
yum install -y yum-utils
yum 
yum download --source kernel
# 解压代码包
rpm2cpio ./kernel-5.14.0-284.11.1.el9_2.src.rpm | cpio -div
tar xf ./linux-5.14.0-284.11.1.el9_2.tar.xz
# 使用命令查看源代码
make cscope ARCH=x86
# 读取并标记tag
make tags ARCH=x86
# 查看
cscope -d

Dracut的使用和命令# 添加驱动程序到ramfs
]$ dracut -fv --add-drivers "nvme ena" /boot/initramfs-$(uname -r).img $(uname -r)
# 查看是否有模块在ramfs中
]$ lsinitrd /boot/initramfs-$(uname -r).img | grep -E "nvme|ena"

安全软件引起的用户空间进程失去响应：Redhat关于这个问题的文档说明： 

https://access.redhat.com/solutions/5201171

https://access.redhat.com/solutions/2838901


使用Ftrace的方法，和一部分命令的使用方法： 
[root@ip-172-31-51-167 ~]$ echo 'func fanotify_get_response +p' > /sys/kernel/debug/dynamic_debug/control

追踪这个系统调用， 并输出 callgraph.内核的DynamicTracing， 这是一个古老的方式了， 出现在Kprobe之前。会直接将追踪的结果输出到dmesg中。 
[root@ip-172-31-51-167 ~]$ perf trace -s -p 2688
[root@ip-172-31-51-167 ~]$ cd /var/crash/127.0.0.1-2023-08-11-06:53:10
[root@ip-172-31-51-167 ~]$ crash /usr/lib/debug/lib/modules/6.1.34-59.116.amzn2023.x86_64/vmlinux  vmcore
[root@ip-172-31-51-167 127.0.0.1-2023-08-11-06:53:10]$ ll /var/crash
total 0
drwxr-xr-x. 2 root root 67 Aug 10 05:52 127.0.0.1-2023-08-10-05:52:16
drwxr-xr-x. 2 root root 67 Aug 10 06:13 127.0.0.1-2023-08-10-06:13:44
drwxr-xr-x. 2 root root 67 Aug 11 05:45 127.0.0.1-2023-08-10-13:03:27
drwxr-xr-x. 2 root root 91 Aug 12 15:05 127.0.0.1-2023-08-11-04:57:13
drwxr-xr-x. 2 root root 67 Aug 11 08:41 127.0.0.1-2023-08-11-06:53:10
drwxr-xr-x. 2 root root 67 Aug 11 20:56 badstop
drwxr-xr-x. 2 root root 41 Aug 11 20:46 crash

Grubby 命令简单的用法设置内核参数： 
# 查看所有内核的参数
$ grubby --info=ALL
# 设置默认的启动内核
$ grubby --set-default-index=1
# 查看当前的默认启动内核
$ grubby --default-kernel
# 移除所有内核的参数
$ grubby --update-kernel=ALL --remove-args="systemd.log_level=debug systemd.log_target=kmsg log_buf_len=1M loglevel=8 crashkernel=512M"
# 更新所有内核的参数
$ grubby --update-kernel=ALL --args="systemd.log_level=debug systemd.log_target=kmsg log_buf_len=1M loglevel=8 crashkernel=512M"
# 为特定的内核添加参数。
$ grubby --update-kernel=/boot/vmlinuz-5.9.1-1.el8.elrepo.x86_64 --args=“systemd.log_level=debug systemd.log_target=kmsg log_buf_len=1M loglevel=8 crashkernel=512M”

[root@ip-172-31-0-170 ~]# sudo kdumpctl status
kdump: Kdump is operational

[root@ip-172-31-0-170 ~]# sudo kdumpctl showmem
kdump: Reserved 256MB memory for crash kernel

[root@ip-172-31-0-170 ~]# cat /proc/cmdline
BOOT_IMAGE=(hd0,gpt1)/boot/vmlinuz-6.1.34-59.116.amzn2023.x86_64 root=UUID=483d7075-a0f8-4ba8-a951-a668fa079cac ro console=tty0 console=ttyS0,115200n8 nvme_core.io_timeout=42949672
95 rd.emergency=poweroff rd.shell=0 selinux=1 security=selinux quiet systemd.log_level=debug systemd.log_target=kmsg log_buf_len=1M loglevel=8 crashkernel=512M


</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/05/25/Linux/Linux_fast-create-prometheus-and-grafana/" title="快速启动一个 prometheus 和 grafana">快速启动一个 prometheus 和 grafana</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-25T07:52:20.000Z" title="发表于 2023-05-25 15:52:20">2023-05-25</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.744Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Docker/">Docker</a></span></div><div class="content">快速创建一个可用的 prometheus 和 grafana 进行测试， 并将数据保留在当前的目录中， 在重启之后数据不会丢失： 

创建一个目录.
mkdir /opt/monitor
mkdir /opt/monitor/grafana
mkdir /opt/monitor/grafana_data
mkdir /opt/monitor/prometheus
mkdir /opt/monitor/prometheus_data
touch /opt/monitor/docker-compose.yaml

创建docker-compose 文件
---
version: "3"
services:
  prometheus:
    container_name: prometheus
    image: reg.liarlee.site/docker.io/prom/prometheus:latest
    restart: always
    network_mode: host
    environment:
      - TZ=Asia/Shanghai
    volumes:
      # - /opt/monitor/prometheus/prometheus.yaml:/etc/prometheus/prometheus.yml
      - /opt/monitor/prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=90d'
  grafana:
    container_name: grafana
    image: reg.liarlee.site/docker.io/grafana/grafana-oss:main-ubuntu
    restart: always
    network_mode: host
    environment:
      - TZ=Asia/Shanghai
    volumes:
      - /opt/monitor/grafana_data:/var/lib/grafana
      - /opt/monitor/grafana/datasource:/etc/grafana/provisioning/datasources
      # - /opt/monitor/grafana/grafana.ini:/etc/grafana/grafana.ini
      - /etc/localtime:/etc/localtime:ro
    user: '472'

准备基础配置文件
docker compose up -d 
docker cp grafana:/etc/grafana/grafana.ini /opt/monitor/grafana/grafana.ini
docker cp prometheus:/etc/prometheus/prometheus.yml /opt/monitor/prometheus/prometheus.yaml

chown -R 472:472 /opt/monitor/grafana_data
chown -R 472:472 /opt/monitor/grafana

chown -R nobody:nobody /opt/monitor/prometheus_data
chown -R nobody:nobody /opt/monitor/prometheus

docker compose down --remove-orphans

准备prometheus 作为默认的Datasource
touch /opt/monitor/grafana/datasource/datasource.yml
---
apiVersion: 1

datasources:
- name: Prometheus
  type: prometheus
  url: http://localhost:9090 
  isDefault: true
  access: proxy
  editable: true

修改配置文件中需要的参数, 取消配置文件中的注释， 然后重启即可。
docker compose down --remove-orphans &amp;&amp; docker compose up -d

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/05/19/Database/Databases_MySQL-why-2000w/" title="数据库单表的测试">数据库单表的测试</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-19T10:10:52.000Z" title="发表于 2023-05-19 18:10:52">2023-05-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.719Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Database/">Database</a></span></div><div class="content">基于这个问题的测试为什么MySQL单表不要超过2000w行？
测试过程：CREATE TABLE test(
id int NOT NULL AUTO_INCREMENT PRIMARY KEY comment &#39;主键&#39;,
person_id int not null comment &#39;用户id&#39;,
person_name VARCHAR(200) comment &#39;用户名称&#39;,
gmt_create datetime comment &#39;创建时间&#39;,
gmt_modified datetime comment &#39;修改时间&#39;
) comment &#39;人员信息表&#39;;

插入数据：
insert into test values(1,1,&#39;user_1&#39;, NOW(), now());

insert into test (person_id, person_name, gmt_create, gmt_modified)
select (@i:&#x3D;@i+1) as rownum, person_name, now(), now() from test, (select @i:&#x3D;100) as init;
set @i&#x3D;1;

&#x2F;&#x2F;测试 SQL,记录他们的运行时间
select count(*) from test;
select count(*) from test where id&#x3D;XXX;

查看这个表格的数据量大小： 
show table status like &#39;test&#39;\G

200w行表： 
mysql&gt; describe table test;
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows    | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
|  1 | SIMPLE      | test  | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 2092640 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
1 row in set, 1 warning (0.00 sec)

MySQL [test]&gt; select count(*) from test;
1 row in set (0.045 sec)
1 row in set (0.050 sec)
1 row in set (0.050 sec)

400w：
mysql&gt; describe table test;
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows    | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
|  1 | SIMPLE      | test  | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 4185280 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
1 row in set, 1 warning (0.00 sec)

MySQL [test]&gt; select count(*) from test;
1 row in set (0.126 sec)
1 row in set (0.120 sec)
1 row in set (0.119 sec)

800w：
mysql&gt; describe table test;
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows    | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
|  1 | SIMPLE      | test  | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 8370120 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+------+---------+------+---------+----------+-------+
1 row in set, 1 warning (0.00 sec)

MySQL [test]&gt; select count(*) from test;
1 row in set (0.266 sec)
1 row in set (0.266 sec)
1 row in set (0.253 sec)

1600w：
mysql&gt; describe table test;
+----+-------------+-------+------------+------+---------------+------+---------+------+----------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows     | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+----------+----------+-------+
|  1 | SIMPLE      | test  | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 16337090 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+------+---------+------+----------+----------+-------+
1 row in set, 1 warning (0.00 sec)

MySQL [test]&gt; select count(*) from test;
1 row in set (0.544 sec)
1 row in set (0.524 sec)
1 row in set (0.523 sec)

3200w：
mysql&gt; describe table test;
+----+-------------+-------+------------+------+---------------+------+---------+------+----------+----------+-------+
| id | select_type | table | partitions | type | possible_keys | key  | key_len | ref  | rows     | filtered | Extra |
+----+-------------+-------+------------+------+---------------+------+---------+------+----------+----------+-------+
|  1 | SIMPLE      | test  | NULL       | ALL  | NULL          | NULL | NULL    | NULL | 32665301 |   100.00 | NULL  |
+----+-------------+-------+------------+------+---------------+------+---------+------+----------+----------+-------+
1 row in set, 1 warning (0.00 sec)

MySQL [test]&gt; select count(*) from test;
1 row in set (1.068 sec)
1 row in set (1.057 sec)
1 row in set (1.044 sec)

这个结果基本上都是线性的， 感觉数据量实在是太小了。
mysql&gt; show table status like &#39;test&#39;\G
*************************** 1. row ***************************
           Name: test
         Engine: InnoDB
        Version: 10
     Row_format: Dynamic
           Rows: 4182365
 Avg_row_length: 48
    Data_length: 202063872
Max_data_length: 0
    ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/05/12/Network/Network-RFC1180/" title="Network 相关知识不知道放那儿">Network 相关知识不知道放那儿</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-12T14:48:45.000Z" title="发表于 2023-05-12 22:48:45">2023-05-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.749Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Network/">Network</a></span></div><div class="content">Origin Version:https://datatracker.ietf.org/doc/html/rfc1180
Chinese Version:http://arthurchiao.art/blog/rfc1180-a-tcp-ip-tutorial-zh/

Tuning initcwnd for optimum Performance: 
https://www.cdnplanet.com/blog/tune-tcp-initcwnd-for-optimum-performance/
https://www.kawabangga.com/posts/5217

Other Linux Network Stack Explaination:https://www.clockblog.life/article/2023/7/4/44.html
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/05/12/Linux/Linux_free-command-issue/" title="buffer/cache 无法释放">buffer/cache 无法释放</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-12T14:15:35.000Z" title="发表于 2023-05-12 22:15:35">2023-05-12</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.744Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">问题看到了一个案例， 这个案例的问题是： 为什么我的buffer&#x2F;cache在echo 3 之后， 还是不能回收， 内存的占用很大。
命令如下： 
root@ip-172-31-47-174 ~# free -h
               total        used        free      shared  buff/cache   available
Mem:           7.5Gi       1.3Gi       3.5Gi       2.1Gi       2.6Gi       3.8Gi
Swap:             0B          0B          0B
root@ip-172-31-47-174 ~# echo 3 > /proc/sys/vm/drop_caches
root@ip-172-31-47-174 ~# free -h
               total        used        free      shared  buff/cache   available
Mem:           7.5Gi       1.3Gi       3.7Gi       2.1Gi       2.4Gi       3.9Gi
Swap:             0B          0B          0B
root@ip-172-31-47-174 ~# free
               total        used        free      shared  buff/cache   available
Mem:         7833520     1376608     3917368     2160524     2539544     4059944
Swap:              0           0           0

分析和答案分析： 开始的时候我并没有发现具体有什么问题， 认为是应用程序确实无法回收cache的空间，因为正在使用。
例如Firefox启动的时候就会使用Cache的空间来进行数据的存储。  
答案： 第二天看到了大佬的更新， 这个问题的原因是： 内核会将 shared 的空间， 一并统计在 buffer&#x2F;cache 中， 所以free命令的输出是正常的， 实际cache已经释放了一部分，没有大幅度变化的原因是因为那部分是 shmem的空间， 所以。。。释放不掉。  

初见这个结论是有些震惊的， 我一直都认为 shared 字段里面统计的内存是独立的， 仔细看看上面的命令， 确实 shared空间基本上与 buffer&#x2F;cache的空间是差不多的。  
测试一部分信息：   

OS: Arch Linux x86_64  

Kernel: 6.3.1-arch2-1



Software Version: free from procps-ng 3.3.17

测试的方法， 我只是尝试证明free命令的统计方式变化， 所以直接简化了， 直接扩大ShareMemory。  
# 直接创建一个临时的目录
# 其实直接使用 /dev/shm 也行， 但是可用空间会被限制到 物理内存的一半。
sudo mkdir /mnt/tmpfs/ 

# 挂载到 /mnt/tmpfs/
sudo mount -t tmpfs -o size=5000m shared /mnt/tmpfs/

# 创建一个文件占用那个部分的内存。
sudo fallocate -l 4G /mnt/tmpfs/file

按照上面的步骤测试， 可以发现 share memory 确实也同时被统计在了 buffer&#x2F;cache 里面， 与客户的现象完全一致。这个命令确实就是这样工作的。  
按照这个思路应该看看meminfo 以及 内核的文档，Mark一下准备开始走一遍这个思路。   
立刻查看free命令的manpage， 发现果然没有更新， 说明摘要： 

sharedMemory used (mostly) by tmpfs (Shmem in &#x2F;proc&#x2F;meminfo)
buffersMemory used by kernel buffers (Buffers in &#x2F;proc&#x2F;meminfo)
cacheMemory  used  by  the  page  cache  and  slabs  (Cached  and SReclaimable in &#x2F;proc&#x2F;meminfo)
buff&#x2F;cacheSum of buffers and cache

看 &#x2F;proc&#x2F;meminfo ， 发现确实是取值取到了 shmem， 这个值是对的， 现在的问题就是为什么内核提供了这样的一个值。
sudo cat /proc/meminfo | grep -Ei "mem|cache|buffer|active"
MemTotal:        7833520 kB
MemFree:         4417692 kB
MemAvailable:    4615060 kB
Buffers:               0 kB
Cached:          2550368 kB  # 这个的统计就。。。。 free是对的
SwapCached:            0 kB
Active:           830560 kB
Inactive:        2397160 kB
Active(anon):     444220 kB
Inactive(anon):  2385016 kB
Active(file):     386340 kB
Inactive(file):    12144 kB
Shmem:           2151884 kB  # 这个是创建的文件大小，转换成文件的大小， 差不多是 2G 左右。

现在的问题变成， 具体是什么时候 meminfo里面的值变更了统计方式呢？ 为什么这样统计呢？ 
不会找具体是什么时候commit的变更， 直接看代码吧。 希望我看的是对的。 
// https://elixir.bootlin.com/linux/latest/source/fs/proc/meminfo.c
static int meminfo_proc_show(struct seq_file *m, void *v)
&#123;
  struct sysinfo i;
    ...
  cached = global_node_page_state(NR_FILE_PAGES) -
              total_swapcache_pages() - i.bufferram;
  if (cached &lt; 0)
        cached = 0
    ...
    show_val_kb(m, "MemTotal:       ", i.totalram);
  show_val_kb(m, "MemFree:        ", i.freeram);
  show_val_kb(m, "MemAvailable:   ", available);
  show_val_kb(m, "Buffers:        ", i.bufferram);
  show_val_kb(m, "Cached:         ", cached); 
  show_val_kb(m, "SwapCached:     ", total_swapcache_pages());
  show_val_kb(m, "Active:         ", pages[LRU_ACTIVE_ANON] +
             pages[LRU_ACTIVE_FILE]);
  show_val_kb(m, "Inactive:       ", pages[LRU_INACTIVE_ANON] +
             pages[LRU_INACTIVE_FILE]);
  show_val_kb(m, "Active(anon):   ", pages[LRU_ACTIVE_ANON]);
  show_val_kb(m, "Inactive(anon): ", pages[LRU_INACTIVE_ANON]);
  show_val_kb(m, "Active(file):   ", pages[LRU_ACTIVE_FILE]);
  show_val_kb(m, "Inactive(file): ", pages[LRU_INACTIVE_FILE]);
  show_val_kb(m, "Unevictable:    ", pages[LRU_UNEVICTABLE]);
  show_val_kb(m, "Mlocked:        ", global_zone_page_state(NR_MLOCK));


可以看到从一个sysinfo 的 struct 取出, 然后进行计算， 存储在cached 变量里面，然后输出： 
&#x2F;&#x2F; https:&#x2F;&#x2F;elixir.bootlin.com&#x2F;linux&#x2F;latest&#x2F;source&#x2F;include&#x2F;uapi&#x2F;linux&#x2F;sysinfo.h#L14

  __kernel_ulong_t bufferram;	&#x2F;* Memory used by buffers *&#x2F;

好了 我看不动了，Cached 的最后结果： os所有可用的文件页面 - SwapCached的数值 - Buffers的数值，大概应该是这个意思， 按照这么算的话， 确实会加入 shmem 的部分，邮件归档以及解释

That’s a reasonable position to take.
Another point of view is that everything in tmpfs is part of the page cache and can be written out to swap, so keeping it as part of Cached is not misleading.
I can see it both ways, and personally, I’d lean towards clarifyingthe documentation about how shmem is accounted rather than changing how the memory usage is reported.

这是上面的邮件链接中的一部分， 解释了为什么将 shmem 计算到 Cached 中的原因，看起来现在应该由一个新的指标数值来处理这个了。
更多的部分看参考链接吧，我顺便看了讲解，大佬讲的清楚。
参考链接https://zhuanlan.zhihu.com/p/586107891
https://www.cnblogs.com/tsecer/p/16290025.html
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/05/09/Linux/Linux_RDS%20QPS%20%E4%B8%8B%E9%99%8D%E5%BC%95%E5%8F%91%E7%9A%84%E7%BD%91%E7%BB%9C%E6%B5%81%E6%8E%A7%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95/" title="RDS QPS 下降引发的网络流控分析记录">RDS QPS 下降引发的网络流控分析记录</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-08T17:34:00.000Z" title="发表于 2023-05-09 01:34:00">2023-05-09</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.740Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Topic 1 现象看到一个朋友的问题， 由 RDS QPS 下降引发的网络问题分析： 
引用原文的问题描述：

这个问题一开始是在进行RDS实验的时候发现的。最初的情景是，多台机器同时对数据库进行select和insert操作时，会发现insert操作会造成select操作qps大幅下降，且insert操作结束之后select操作的qps仍不能回升。起初以为是RDS的问题，但是在复现问题、监控RDS之后发现RDS的压力其实很小。于是开始怀疑是网络的问题，在简化了场景和操作之后，发现能在过去做tcp实验的机器上复现，于是用这个更简单的场景进行问题复现和分析。

正确答案以及复盘： https://yishenggong.com/2023/05/06/why-does-my-network-speed-drop-cn/
感谢大佬@plantegg 提供的这个案例和知识星球， tcp协议和 os 网络系统的分析我之前真是一句都说不出来， 这次确实完整的走了一遍网络的部分。

下面是我的一些思路和资料整理： 
看完了问题的描述， 我基本上可以给出一个经验的判断，这个问题是aws t2实例的流控， 因为表现和之前的测试非常一致， 为了确认这个问题我又进行了相关的测试，基准信息如下：
实例类型：t2.micro
实例的基准带宽: 64Mbps
实例的突增带宽: 1Gbps
实例的EBS规格： 3000 IOPS， 100GB根卷存储， 125MiB&#x2F;s
实例的OS ： AmazonLinux2023. 
Congestion: Cubic
具体的测试方法：

启动4个实例  t2.micro, hostname 分别是 nginx \ client1 \ client2 ， 方便测试和区分实例； 启动一个额外的实例Client3 来尝试验证是不是流控。
nginx上面启动一个Nginx server， 发布一个 2GB 大小的文件； Client 1、 Client2 两个实例上面使用curl命令下载。
尝试保持这个流量一段时间之后， 带宽会被限制到64 Mbps。
由于是流控的问题， 所以现象会稳定出现。


Summary： 
我理解这个应该是一个对tcp流的限制， 所以验证这个问题的方法是启动一个iperf的服务并重新生成流量， 如果是流控的话， 那么新的流量应该不受限制， 简单的证明方式就是在Nginx Server上面启动一个Iperf3 的 Server ， 然后在发生降低速度的时刻， 使用 Client3 iperf 测试 Nginx Server， 这个时候观察到的现象是两个原有网络链接的速度还是很低， 但是iperf3 的测试网络吞吐量是比较高的。
t系列的实例都是突增类型， 可以允许一定的时间使用超过实例基准性能的资源， 在网络流量超过基准性能一段时间之后， 实例的带宽会被限制。 之前真的一直都没有思考过实例的带宽限制都是怎么做的， 让我在描述更多的细节我也确实无法提供， 借着这个机会，深入研究一下。

重新开始分析这个问题： 

分析网络抓包看看，被限速的是Server的实例， 所以直接看Server的抓包.
贴一个我自己抓包截图，两个不同的流:

Server to Client 1:



Server to Client 2:


对与上面的两个截图来看， 斜率表越大，下载速度越快， 在斜率较大的前半段， 并未发生任何的TCP异常； 后半段斜率变小的期间，一直有规律的红色标记， 那个标记对应到抓包的结果中，是快速重传的数据包， 并且发生快速重传的时间比较规律， 这类规律的快速重传影响了传输的速度。

查看RTT的变化， 截图如下： 
Server to Client 1: 

Server to Client 2:

上面的这个部分可以看出整体增大了RTT ，基于之前的tc流控测试，rtt增大的链路上， 发送和接收的窗口需要相应的增大， 才能保证带宽的利用率较高。 按照实验更增加 wmem 和 rmem， 三个实例的设置都增加， 增加之后的速度并没有改变， 依旧还是被限制的水平。RTT从抓包的结果上面来看就是有明显的变化。

查看Server 实例的CWND， 这个CWND的记录直接使用命令行看：

左侧是Server上面的命令行执行结果 命令是 ss -4tni ， 命令输出的结果中， 有当前的TCP链接的 RTT， CWND的值， 可以观察到， 到两个Server的链接中RTT的值 ~60ms 左右， 我使用了watch 0.1s来执行ss命令和观察， 重点关注上面的几个参数， 可以发现这几个数值的范围如下：
RTT ~ 50ms ~ 70ms 之间变化的频率基本上与cwnd的变化频率一致。
CWND 40 - 64 之间变化， 到达64之后回落到40左右， 然后快速提高到64再回落。
引用一个描述比较清晰的中文文章，其中也介绍了常见的几种拥塞控制算法的模式， 帮我大概理解了下网络的部分： 

TCP 连接建立后先经过 Slow Start 阶段，每收到一个 ACK，CWND 翻倍，数据发送率以指数形式增长，等出现丢包，或达到 ssthresh，或到达接收方 RWND 限制后进入 Congestion Avoidance 阶段。1

从另一个文章中找到了如下的一个公式： 

因此想要充分利用带宽，必须让窗口大小接近 BDP 的大小，才能确保最大吞吐量。
我们通过如下的例子来讨论一下，究竟 rwnd 和 cwnd 的值与理论最大使用带宽的关系是什么？
min(cwnd, rwnd) = 16 KB
RTT = 100 ms
16 KB = 16 X 1024 X 8 = 131072 bits
131072 / 0.1 = 1310720 bits/s
1310720 bits/s = 1310720 / 1000000 = 1.31 Mbps

因此，无论发送端和接收端的实际带宽为多大，当窗口大小为 16 KB 时，传输速率最大只能为 1.31 Mbps 。2

可以看到上面的公式中的第一个， 在计算的时候会在cwnd， rwnd 两个指标中选取最小的生效， 那么基于上面的变动， 我们已经把wmem ， rmem 都改到了较大的值， 那么这个时候较小 的就是 cwnd 了， 这个值增加不上去就无法充分的利用带宽。
那么现在的问题变成了为什么 cwnd 无法继续增长？

Cubic 在 BIC 的基础上，一个是通过三次函数去平滑 CWND 增长曲线，让它在接近上一个  CWND 最大值时保持的时间更长一些，再有就是让 CWND 的增长和 RTT 长短无关，即不是每次 ACK 后就去增大 CWND，而是让  CWND 增长的三次函数跟时间相关，不管 RTT 多大，一定时间后 CWND 一定增长到某个值，从而让网络更公平，RTT 小的连接不能挤占  RTT 大的连接的资源。1

如果cwnd的大小在拥塞避免之后是基于时间来进行增长的，那么就可以结合上面我观察到的现象， 基本上过一段时间就会出现一定量的快速重传（抓包结果）， 周期性的RTT 与 cwnd的变化（命令输出结果）比较吻合了。
基于上面的命令输出结果可以认为确实是通过RTT的增加 + 丢包控制 CWND， 压着CWND的值上不去的原因就是快速重传， 让cwnd随时间增大之后快速重传， 然后触发拥塞避免， 回退到较低的值， 然后开始循环。



感谢Shuo Chen 和 Hao Chen 大佬的测试分析思路以及工具，反复的理解这个Thread里面的说法， 现在基本上可以抓住这个问题的逻辑了。 
走着一圈之后， 写完了上面自己的总结， 现在复制大佬的答案： 

@bnu_chenshuo 从发生 retransmit 的间隔看，太规律了。我现在怀疑是 aws 有意限速，通过非常巧妙的少量丢包或乱序（图一中粗线下挂的小黑点），并控制 RTT，让 Linux TCP sender 的 cwnd 钳制在几十 KB，RTT 在 10ms，进而限制吞吐量在几千 KB&#x2F;s。
我在 t3.micro 上用 FreeBSD 13 复现。观察到 AWS 先用对付 Linux 的办法：故意丢包乱序 + RTT&#x3D;10ms，发现不灵之后，恼羞成怒，包也不丢了，直接卡脖子。总之就是不让你白嫖网速。对 Linux 是智斗，每秒钟丢你一两个包，RTT&#x3D;10ms，让你自己cwnd小、速度上不去，你也不好说啥。对 FreeBSD 就上武力了。

？？？对于测试的结果， 我这边确实不同，我读取到的RTT 60ms 明显要比大佬测试的时候的RTT 10ms 更高. 我把这个理解为 中国区AWS 与 Global的差异， 这个问题也许还能继续分析?
调整 rmem ， wmem 的大小到 MTU * CWND &#x3D; 9001 * 60 &#x3D; 540600 ，这个数值x2 设置到发送和接收的窗口， 重新抓包， 理论上应该看不到快速重传了。并且拥塞窗口应该是稳定在 60 左右。 
测试的过程中观察， 设置 rmem wmem 最终的值为 1000000， ss命令中的 CWND稳定在 57 ， 抓包的结果中没有快速重传， RTT也比较稳定了， 不再有跳跃和突然增加到 100+ 的RTT， 速度还是被限制了， 这时候较小的是 RWND， 所以也是为什么 CWND 可以比较稳定不再变化的原因。

Topic 2 如何使用wireshark查看丢包率？如何查看实例在限速状态的 丢包率 和 重传率？ 
在客户端的抓包结果里面， 使用字段 tcp.stream eq 0 and tcp.analysis.fast_retransmission 调整到对应的 tcp stream， 然后在Conversion
s窗口中统计的 Percent Filter 里面的百分比就是重传率， 丢包应该是 ： tcp.analysis.lost_segment
或者就直接使用IO Graphs， 然后可以看到下面的图像，可以看到这个丢包比较小， 所以我在Y轴设置 100 的 Factor， 这样看起来清楚些。
如图：

基于这个抓包结果里面的基本上是乱序和快速重传，Server是发送数据的源头， 客户端没有收到认为这个是丢包。
那么感觉丢包率看Client的抓包结果应该就可以。 查看重传率 从Server这测的抓包结果来看。 

Topic 3 RTT的变长是不是流控可以设计的？这个rtt确实无法区分到底是底层的流控带来的， 或者是设计故意拖长的，更新额外测试的结果:

将三个实例完全换成 c5.large,  后续没有RTT非常高的的问题了， 如图:


尝试处理掉CPU 的 IOwait， 也就是将这个文件缩小到 500MB ，之后nginx的数据都从内存取出， 发现RTT增加到了120ms， 维持了较长的时间之后还会回到60ms， 不知道这个原因是什么， 但是看起来RTT不稳定可能确实和os本身没什么关系。



试试其他规格的实例： 尝试在c5的实例上面直接使用tc卡住网桥设备的带宽， 来模拟链路中的网络设备？ 观察rtt看看是否有任何的变化。 
测试的方法使用docker-compose启动两个pod， 文件如下：
---
version: "3.8"
services:
  pod1:
    image: 111.dkr.ecr.cn-north-1.amazonaws.com.cn/haydenarchlinux:latest
    restart: always
  pod2:
    image: 111.dkr.ecr.cn-north-1.amazonaws.com.cn/haydenarchlinux:latest
    restart: always

在docker容器里面放入 Shuo Chen 大佬的tcpper ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/04/20/AWS/EKS_%E6%B7%BB%E5%8A%A0ArchLinux%E5%88%B0EKS%E4%BD%9C%E4%B8%BA%E8%8A%82%E7%82%B9/" title="将ArchLinux作为节点加入EKS UnmanagedNode">将ArchLinux作为节点加入EKS UnmanagedNode</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-19T16:42:22.000Z" title="发表于 2023-04-20 00:42:22">2023-04-20</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.717Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">添加一个自管理的节点添加这个自管理的节点是直接添加进入集群的， 并未使用EKS节点组的概念， 所以这个节点是可以被重启， 或者健康检查失败的， 并不具有任何的扩展或者弹性管理的能力。

创建集群，启动一个新的 EC2， 登录到已经启动的 EKS 优化 OS 内，准备复制一些脚本过来。

添加EC2的标签： kubernetes.io&#x2F;cluster&#x2F;clusterName  owned

配置EC2的Instance Profile

控制台获取 Kubernetes APIServer的Endpoint URL

获取 apiserver b64 CA : cat ~/.kube/config 这个文件里面可以找到 ，或者是通过EKS的控制台上面， 找到 Base64 的 CA。

编辑 userdata， 或者 ssh 登录到ec2上面创建一个bash脚本用来调用 bootstrap.sh
mkdir ~/eks; touch ~/eks/start.sh
---
#!/bin/bash
set -ex

B64_CLUSTER_CA=
API_SERVER_URL=
K8S_CLUSTER_DNS_IP=10.100.0.10

/etc/eks/bootstrap.sh $&#123;ClusterName&#125; --b64-cluster-ca $&#123;B64_CLUSTER_CA&#125; --apiserver-endpoint $&#123;API_SERVER_URL&#125;

集群里面没有节点组 ， 也不会创建aws-auth configmap 所以节点无法正常的加入集群， 需要手动创建。
apiVersion: v1
data:
  mapRoles: |
    - groups:
      - system:bootstrappers
      - system:nodes
      rolearn: [CLUSTER_ROLE]
      username: system:node:&#123;&#123;EC2PrivateDNSName&#125;&#125;
  mapUsers: |
    []
kind: ConfigMap
metadata:
  name: aws-auth
  namespace: kube-system

需要复制的文件sudo pacman -S containerd # 安装Containerd
scp /etc/eks/bootstrap.sh root@54.222.253.235:/etc/eks/bootstrap.sh # 复制bootstrap
scp /usr/bin/imds root@54.222.253.235:/usr/bin/imds # shell 脚本， 用来帮忙调用ec2 metadata 获取实例和VPC子网的信息
scp -pr /etc/eks/ root@54.222.253.235:/etc/eks/ # 直接复制了eks的相关脚本和配置模板
scp -pr /var/lib/kubelet/kubeconfig root@54.222.253.235:/var/lib/kubelet/kubeconfig # 复制kubeletconfig配置文件模板
scp -pr /etc/kubernetes/ root@54.222.253.235:/etc/kubernetes/ # 复制 kubernetes 的配置文件
scp -pr /etc/kubernetes/kubelet/ root@54.222.253.235:/etc/kubernetes/kubelet/ # 上面的命令没有递归复制， 所以需要指定

# 设置对应的内核参数， 如果不做kubelet 会报错提示 这些参数不符合要求。
kernel.panic = 10
kernel.panic_on_oops = 1
vm.overcommit_memory = 1

Bootstrap 脚本内容分析记录一下脚本自动配置的内容， 大概就是 获取变量， aws的服务地址， ec2 元数据地址， 替换模板中的变量生成Kubelet配置文件 和 Containerd 的配置文件（这个替换是一次性的， 也就是说， bootstrap只能变更模板中的变量一次， 第二次执行只会生成刷新一次集群的信息， 以及重启服务）。

读取bootstrap后面给出的参数，设置变量， 例如： ClusterName etc.
查看Kubelet的版本， 决定Runtime， containerd | dockerd, 判断条件是 kubelet 版本 大于 1.24++ kubelet --version
++ grep -Eo '[0-9]\.[0-9]+\.[0-9]+'
+ KUBELET_VERSION=1.24.9
---
+ IS_124_OR_GREATER=true
+ DEFAULT_CONTAINER_RUNTIME=containerd
设置ECR以及Pause容器地址# 获取region以及aws service domain
+ AWS_DEFAULT_REGION=cn-north-1
+ AWS_SERVICES_DOMAIN=amazonaws.com.cn

# 调用脚本 /etc/eks/get-ecr-uri.sh cn-north-1 amazonaws.com.cn ''
+ ECR_URI=918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn
+ PAUSE_CONTAINER_IMAGE=918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/pause
+ PAUSE_CONTAINER=918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/pause:3.5
+ CA_CERTIFICATE_DIRECTORY=/etc/kubernetes/pki
+ CA_CERTIFICATE_FILE_PATH=/etc/kubernetes/pki/ca.crt
创建证书目录：+ mkdir -p /etc/kubernetes/pki
+ sed -i s,MASTER_ENDPOINT,https://CE0253A94B6B14215AE3282580CFA5E3.yl4.cn-north-1.eks.amazonaws.com.cn,g /var/lib/kubelet/kubeconfig
+ sed -i s,AWS_REGION,cn-north-1,g /var/lib/kubelet/kubeconfig
+ sed -i s,CLUSTER_NAME,NewClusterForManualJoin,g /var/lib/kubelet/kubeconfig
获取 VPC CIDR# imds shell script help to get metadata.
imds: /usr/bin/imds
++ imds latest/meta-data/local-ipv4
++ imds latest/meta-data/network/interfaces/macs/02:66:06:2e:48:08/vpc-ipv4-cidr-blocks
创建kubelet 配置, 计算预留的资源 和 MaxPod 等等参数的数值。/etc/kubernetes/kubelet/kubelet-config.json
+ mkdir -p /etc/systemd/system/kubelet.service.d
+ sudo mkdir -p /etc/containerd
+ sudo mkdir -p /etc/cni/net.d
+ sudo mkdir -p /etc/systemd/system/containerd.service.d
创建containerd 配置文件+ printf '[Service]\nSlice=runtime.slice\n'
+ sudo tee /etc/systemd/system/containerd.service.d/00-runtime-slice.conf
+ sudo sed -i s,SANDBOX_IMAGE,918309763551.dkr.ecr.cn-north-1.amazonaws.com.cn/eks/pause:3.5,g /etc/eks/containerd/containerd-config.toml
kubelet配置和启动。+ sudo cp -v /etc/eks/containerd/kubelet-containerd.service /etc/systemd/system/kubelet.service
+ sudo chown root:root /etc/systemd/system/kubelet.service
+ sudo containerd config dump
+ systemctl enable kubelet
+ systemctl start kubelet

制作 NodeTemplate AMI
如果使用已经加入过集群的实例直接制作AMI， 节点会无法加入， kubelet报错是APIserver拒绝这个节点的加入。
2023-04-19T15:10:13Z kubelet-eks.daemon[11577]: E0419 15:10:13.325154   11577 event.go:267] Server rejected event &#39;&amp;v1.Event&#123;TypeMeta:v1.TypeMeta&#123;Kind:&quot;&quot;, APIVersion:&quot;&quot;&#125;, ObjectMeta:v1.ObjectMeta&#123;Name:&quot;ip-10-0-1-249.cn-northwest-1.compute.internal.17575e9c08aefd8d&quot;, GenerateName:&quot;&quot;, Namespace:&quot;default&quot;, SelfLink:&quot;&quot;, UID:&quot;&quot;, ResourceVersion:&quot;&quot;, Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:&lt;nil&gt;, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ZZZ_DeprecatedClusterName:&quot;&quot;, ManagedFields:[]v1.ManagedFieldsEntry(nil)&#125;, InvolvedObject:v1.ObjectReference&#123;Kind:&quot;Node&quot;, Namespace:&quot;&quot;, Name:&quot;ip-10-0-1-249.cn-northwes ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/04/14/Network/Network_tc%E6%8E%A7%E5%88%B6%E6%B5%81%E9%87%8F-update/" title="Linux OS 网络流量控制测试">Linux OS 网络流量控制测试</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-14T09:41:03.000Z" title="发表于 2023-04-14 17:41:03">2023-04-14</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.750Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">内核参数的说明对于 TCP 来说，会遇到如下的几个参数。
如果我们需要查看一下当前OS内与tcp相关的 KernelParam， 命令如下：
]$ sysctl -a | egrep "rmem|wmem|tcp_mem|adv_win|moderate"

其中主要需要关注的是:
net.ipv4.tcp_rmem &#x3D; 4096	131072	6291456
net.ipv4.tcp_wmem &#x3D; 4096	16384	4194304

这两个参数其实表示的是当前内核预留的 Socket Buffer， 单位是Bytes， 也是具体指 内存 的大小。具体的说明我找到 Kernel文档的说明如下：

tcp_rmem - vector of 3 INTEGERs: min, default, max
  min: Minimal size of receive buffer used by TCP sockets.
  It is guaranteed to each TCP socket, even under moderate memory
  pressure.
  Default: 4K

  default: initial size of receive buffer used by TCP sockets.
  This value overrides net.core.rmem_default used by other protocols.
  Default: 87380 bytes. This value results in window of 65535 with
  default setting of tcp_adv_win_scale and tcp_app_win:0 and a bit
  less for default tcp_app_win. See below about these variables.

  max: maximal size of receive buffer allowed for automatically
  selected receiver buffers for TCP socket. This value does not override
  net.core.rmem_max.  Calling setsockopt() with SO_RCVBUF disables
  automatic tuning of that socket&#39;s receive buffer size, in which
  case this value is ignored.
  Default: between 87380B and 6MB, depending on RAM size.
  
tcp_wmem - vector of 3 INTEGERs: min, default, max
  min: Amount of memory reserved for send buffers for TCP sockets.
  Each TCP socket has rights to use it due to fact of its birth.
  Default: 4K

  default: initial size of send buffer used by TCP sockets.  This
  value overrides net.core.wmem_default used by other protocols.
  It is usually lower than net.core.wmem_default.
  Default: 16K

  max: Maximal amount of memory allowed for automatically tuned
  send buffers for TCP sockets. This value does not override
  net.core.wmem_max.  Calling setsockopt() with SO_SNDBUF disables
  automatic tuning of that socket&#39;s send buffer size, in which case
  this value is ignored.
  Default: between 64K and 4MB, depending on RAM size.


这文档中的说明，默认的三个值分别是： 最小， 默认， 最大。测试了一下， 如果只是需要实际调整的话， 调整那个最大值即可， 在高延迟的链路中， 调整默认值或者最大值就可以生效。 在测试的过程中， 将三个值都固定到预期，控制变量。
对于TCP协议的接收与发送两方， 各自有自己的 RecvBuffer 和 SendBuffer， 发送方会考虑链路上面可以承载的数据量（带宽）， 以及 对方可以承载的数据量（rmem）。
实际上， 只是更新 max 的值， 并不会更新 Recvbuffer.如果想增大 receive buffer 的大小, 可以增加 tcp_rmem 的 default 的值大小.
测试环境两个 EC2 c5.2xlarge， 其中一个部署 Nginx ， 并使用如下配置文件部分设置 ,  发布一个 Fedora ISO， 大小大约 2G。另一个上面只是客户端， 使用的访问客户端是Curl。
http &#123;
    server &#123;
      autoindex on;
      autoindex_localtime on;
        &#125;
    &#125;
&#125;
这个配置文件中的其他部分就省略吧。

测试准备测试的过程其实涉及了四个部分的 延迟 问题:

发送方的应用程序性能。
发送方的发送缓冲区大小， SendBuffer
接收方的接收缓冲区大小， RecvBuffer
接收方的应用程序性能。

调整延迟和控制带宽 ， 这两个部分控制的都是网络传输部分的性能，测试的过程中默认认为 发送 以及接收方的性能都完全没有问题。
基准两个机器的内核参数使用默认值， 先通过 tc 流量控制注入一些延迟， 查看并分析RTT对于传输速度的影响。
两个server之间默认的内核参数：

服务端参数
net.ipv4.tcp_rmem = 4096	131072	6291456
net.ipv4.tcp_wmem = 4096	16384	4194304

客户端参数
net.ipv4.tcp_wmem = 4096	131072	6291456
net.ipv4.tcp_rmem = 4096	16384	4194304

curl 的 测试命令 如下：（更新版本， 之前的测试版本少了一些参数 
~]$ curl -o /dev/null -s -w "time_namelookup:%&#123;time_namelookup&#125;\ntime_connect: %&#123;time_connect&#125;\ntime_appconnect: %&#123;time_appconnect&#125;\ntime_redirect:  %&#123;time_redirect&#125;\ntime_pretransfer:  %&#123;time_pretransfer&#125;\ntime_starttransfer: %&#123;time_starttransfer&#125;\ntime_total: %&#123;time_total&#125;\n"  http://nginx.liarlee.site/Fedora-Workstation-Live-x86_64-38_Beta-1.3.iso 

如果使用默认的参数， 那么2G的 ISO 文件可以快速的传完， 这是两个实例的基准表现， 这也是创建了一个TCP Connection 的较好的性能表现， EC2 之间的带宽较大 10Gbps。
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0*   
Trying 172.31.48.133:80...
100 1967M  100 1967M    0     0  1116M      0  0:00:01  0:00:01 --:--:-- 1116M
* Connection #0 to host nginx.liarlee.site left intact
time_connect: 0.005461
time_starttransfer: 0.005797
time_total: 1.762040

总体看起来使用了 2s 不到的时间， 就传输完成了2G的文件。
仅控制带宽在添加了带宽 控制， 带宽控制在 1000Mbps， 带宽监控结果： 

Curl命令的结果： 
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 1967M  100 1967M    0     0   118M      0  0:00:16  0:00:16 --:--:--  118M
time_connect: 0.002070
time_starttransfer: 0.002304
time_total: 16.633562

在仅仅控制带宽的前提下， 由于延迟忽略不计， 所以传输的数据量可以使用全部的带宽，这个场景下面传输的速度取决于网络带宽的大小， 网络带宽越大， 传输的数据量和速度都会相应的增加。
控制带宽+延迟给服务端添加一个 50ms 的 延迟， 使用 tc 工具， 参考文档， 命令如下：
tc qdisc del dev eth0 root
tc qdisc add dev eth0 root handle 1:0 htb default 1
tc class add dev eth0 parent 1:0 classid 1:1 htb rate 1000mbit
tc qdisc add dev eth0 parent 1:1 handle 2:0 netem delay 50ms

测试的时候使用curl命令， 将结果直接输出到 &#x2F;dev&#x2F;null, 这个场景下， 客户端收写数据的速度非常快，这样的测试排除了大文件落硬盘速度慢的问题， 但是也让客户端收到这批数据之后立刻可以发送ack给服务端（Client - ACK——&gt; Server）， 告知服务端我这边的数据已经处理完了， 回收 RecvBuffer 空间.
在添加带宽控制和 50ms 延迟之后， 带宽使用：
ping 命令 确认 rtt ： 
root@arch ~# ping nginx.liarlee.site
PING nginx.liarlee.site (172.31.48.133) 56(84) bytes  ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/03/23/AWS/EKS_%E5%88%A0%E9%99%A4%E6%89%80%E6%9C%89%E9%9D%9ERunning%E7%8A%B6%E6%80%81%E7%9A%84Pod/" title="删除所有非Running状态的Pod">删除所有非Running状态的Pod</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-23T15:42:22.000Z" title="发表于 2023-03-23 23:42:22">2023-03-23</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.716Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">背景 如果所有的节点上面都有Taint， 然后这个没有Taint的节点磁盘满了， 会导致当前的节点上面留下许多状态不正常的Pod， 这些Pod大概率是停留在了Evicted状态， 或者是Completed, 甚至是 Unknown 。 
这种状态的Pod Deployment默认的情况下不会自动回收， 所以需要人工操作一下。 
Knowledge Source
处理方法记录一个命令来处理这个类型的Pod。 
kubectl delete pod --field-selector="status.phase==Failed"

测试方法
集群内两个节点， 其中一个节点Taint  kubectl taint nodes ip-172-31-60-181.cn-north-1.compute.internal app=grafana:NoSchedule
启动30个Grafana
登录到节点上面， 创建一个巨大的文件， 触发DiskPressure。  fallocate -l 72G ./large.file
等待节点的DiskPressure被识别， 然后触发驱逐。  
删除文件， 取消DiskPressure状态， 等待 30 个新的Pod Ready。rm -rf ./large.file
使用命令清除所有不是Running状态的Pod。~$ kubectl delete pod --field-selector="status.phase==Failed"
Over.  

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/03/19/Linux/Linux_bpftrace/" title="bpftrace 使用tracepoint 追踪 tcp 状态变化">bpftrace 使用tracepoint 追踪 tcp 状态变化</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-03-18T16:08:52.000Z" title="发表于 2023-03-19 00:08:52">2023-03-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.743Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">入门教程大佬的博客， 入门教程： http://arthurchiao.art/blog/bpf-advanced-notes-1-zh/
记录基础的bpftrace使用方法单行程序的使用方法
[root@localhost-live ~]# bpftrace -e 'tracepoint:syscalls:sys_enter_execve &#123; printf("%s %s\n", comm, str(args->filename));&#125;'

Tracepoint如何获取可用参数的解释[root@localhost-live sys_enter_execve]# pwd
/sys/kernel/tracing/events/syscalls/sys_enter_execve
[root@localhost-live sys_enter_execve]# grep -ri .
format:name: sys_enter_execve
format:ID: 742
format:format:
format:	field:unsigned short common_type;	offset:0;	size:2;	signed:0;
format:	field:unsigned char common_flags;	offset:2;	size:1;	signed:0;
format:	field:unsigned char common_preempt_count;	offset:3;	size:1;	signed:0;
format:	field:int common_pid;	offset:4;	size:4;	signed:1;
format:	field:int __syscall_nr;	offset:8;	size:4;	signed:1;
format:	field:const char * filename;	offset:16;	size:8;	signed:0;
format:	field:const char *const * argv;	offset:24;	size:8;	signed:0;
format:	field:const char *const * envp;	offset:32;	size:8;	signed:0;
format:print fmt: "filename: 0x%08lx, argv: 0x%08lx, envp: 0x%08lx", ((unsigned long)(REC->filename)), ((unsigned long)(REC->argv)), ((unsigned long)(REC->envp))
trigger:# Available triggers:
trigger:# traceon traceoff snapshot stacktrace enable_event disable_event enable_hist disable_hist hist
filter:none
id:742
enable:0

记录尝试追踪tcp状态变化的方法关于bpftrace追踪的总结, 追踪tcp状态的方法， 通过使用特定的tracepoint的来获取tcp状态的变化： 
~$ bpftrace -e 'tracepoint:sock:inet_sock_set_state &#123; printf("%s %d %d\n", comm, pid, args->newstate); &#125;'
~$ bpftrace -e 'tracepoint:sock:inet_sock_set_state &#123; printf("%s - %d -> %d - %d - %s\n",strftime("%H:%M:%S.%L", nsecs), args->oldstate, args->newstate, pid, comm); &#125;'

关于返回值的说明： 

https://gitlab.com/redhat/centos-stream/src/kernel/centos-stream-9/-/blob/main/include/net/tcp_states.h  

tcp_set_state 是一个内核函数，用于设置 TCP 套接字的状态。它有 12 个可能的返回值，分别对应 TCP 协议中定义的 12 种状态1。这些状态是：

1 TCP_ESTABLISHED：连接已建立2 TCP_SYN_SENT：主动打开连接，已发送 SYN 包3 TCP_SYN_RECV：被动打开连接，已收到 SYN 包4 TCP_FIN_WAIT1：主动关闭连接，已发送 FIN 包5 TCP_FIN_WAIT2：主动关闭连接，已收到对方的 ACK 包6 TCP_TIME_WAIT：主动关闭连接，等待一段时间以确保对方收到最后一个 ACK 包7 TCP_CLOSE：连接已关闭8 TCP_CLOSE_WAIT：被动关闭连接，已收到 FIN 包9 TCP_LAST_ACK：被动关闭连接，已发送最后一个 ACK 包10 TCP_LISTEN：监听状态，等待被动打开连接11 TCP_CLOSING：双方同时关闭连接，交换 FIN 和 ACK 包的过程中12 TCP_NEW_SYN_RECV：临时状态，用于处理 SYN 队列溢出的情况

追踪点理解起来还是比较简单的， 查看追踪点可用的参数，在这个部分可以看。 
~$ cat /sys/kernel/debug/tracing/events/syscalls/sys_enter_sendmsg/format
name: sys_enter_sendmsg
ID: 1252
format:
        field:unsigned short common_type;       offset:0;       size:2; signed:0;
        field:unsigned char common_flags;       offset:2;       size:1; signed:0;
        field:unsigned char common_preempt_count;       offset:3;       size:1; signed:0;
        field:int common_pid;   offset:4;       size:4; signed:1;
        field:unsigned char common_preempt_lazy_count;  offset:8;       size:1; signed:0;

        field:int __syscall_nr; offset:12;      size:4; signed:1;
        field:int fd;   offset:16;      size:8; signed:0;
        field:struct user_msghdr * msg; offset:24;      size:8; signed:0;
        field:unsigned int flags;       offset:32;      size:8; signed:0;

print fmt: "fd: 0x%08lx, msg: 0x%08lx, flags: 0x%08lx", ((unsigned long)(REC->fd)), ((unsigned long)(REC->msg)), ((unsigned long)(REC->flags))

如果是使用 kprobe 的话， 不能使用 args， 需要使用确定的arg0 - argN， 这个部分还在摸索， 目前尝试用这个参数还是失败。 取到对应的得值， 但是print不出来。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/01/09/Other/Books_%E8%AF%97%E8%AF%8D%E6%94%B6%E9%9B%86/" title="诗词收集">诗词收集</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-08T17:45:39.000Z" title="发表于 2023-01-09 01:45:39">2023-01-09</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.961Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Books/">Books</a></span></div><div class="content">收集一些自己平时见到的诗句：
Update 2018-12-24
邻人焉有许多鸡，乞丐何曾有二妻。当时尚有周天子，何事纷纷说魏齐。
算不尽芸芸众生微贱命，回头看五味杂陈奈何天。
何时杖策相随去，任性逍遥不学禅。
百无一用是情深，万般奈何为情困。
桃李春风一杯酒，江湖夜雨十年灯。
杀人放火金腰带，修桥补路无尸骸。
残雪凝辉冷画屏，落梅横笛已三更，更无人处月胧明。我是人间惆怅客，知君何事泪纵横，断肠声里忆平生。 纳兰性德《浣溪沙·残雪凝辉冷画屏》
曾经沧海难为水，除却巫山不是云。取次花丛懒回顾，半缘修道半缘君。
人生到处应何似，应似飞鸿踏雪泥。
山有木兮木有枝，心悦君兮君不知。
嫦娥应悔偷灵药，碧海青天夜夜心。
满堂花醉三千客，一剑霜寒十四州。
早岁读书无甚解，晚年省事有奇功。
车遥遥，马憧憧。君游东山东复东，安得奋飞逐西风。愿我如星君如月，夜夜流光相皎洁。月暂晦，星常明。留明待月复，三五共盈盈。 范成大《车遥遥篇》  
一身诗意千寻瀑，万古流芳四月天。
未曾相逢先一笑，初会便已许平生。
三千年读史不过功名利禄，九万里悟道终归诗酒田园。
君埋泉下泥销骨，我寄人间雪满头。
伤心桥下春波绿，曾是惊鸿照影来。
醉后不知天在水，满船清梦压星河。
秋风吹尽旧庭柯，黄叶丹枫客里过。一点禅灯半轮月，今宵寒较昨宵多。
细雨生寒未有霜，庭前木叶半青黄。小春此去无多日，何处梅花一绽香。
岁久人无千日好，春深花有几时红。是非入耳君须忍，半作痴呆半作聋。
清风不识字，何故乱翻书。
三过平山堂下，半生弹指声中。十年不见老仙翁，壁上龙蛇飞动。欲吊文章太守，仍歌杨柳春风。休言万事转头空，未转头时皆梦。
山月不知心里事，水风空落眼前花。
未若锦囊收艳骨，一抔冷土掩风流。
莫言下岭便无难，赚得行人空喜欢。正入万山圈子里，一山放过一山拦。
有美一人兮婉如清扬，识曲别音兮令姿煌煌。绣袂捧琴兮登君子堂，如彼萱草兮使我忧忘。欲赠之以紫玉尺，白银珰，久不见之兮湘水茫茫。
生如逆旅，一苇以航。

Update 2023-01-09
回头万里，故人长绝。易水萧萧西风冷，满座衣冠似雪。《贺新郎·别茂嘉十二弟》辛弃疾
洛阳城里春光好，洛阳才子他乡老。《菩萨蛮·洛阳城里春光好》韦庄
伤心桥下春波绿，曾是惊鸿照影来。《沈园二首·其一》陆游
谁见幽人独往来，缥缈孤鸿影。《卜算子·黄州定慧院寓居作》苏轼
琵琶弦上说相思。当时明月在，曾照彩云归。《临江仙·梦后楼台高锁》晏几道
此后锦书休寄，画楼云雨无凭。《清平乐·留人不住》晏几道
美人自刎乌江岸，战火曾烧赤壁山，将军空老玉门关。
食肉何曾尽虎头，卅年书剑海天秋。文章幸未逢黄祖，襆被今犹窘马周。自是汝才难用世，岂真吾相不当侯。须知少日拏云志，曾许人间第一流。 清代吴庆坻《题三十计小象》 
时人不识凌云木，直待凌云始道高。
总有千古，横有八荒，前途似海，来日方长。
鹏北海，风朝阳，又携书剑路茫茫。明年此日青云去，却笑人间举子忙。

Update 2023-03-01 派克直播间飞花令
不是逢人苦誉君，亦狂亦侠亦温文。照人胆似秦时月，送我情如岭上云。 
一生一代一双人，争教两处销魂。相思相望不相亲，天为谁春？
也信美人终作土，不堪幽梦太匆匆。
银字笙调。心字香烧。料芳悰、乍整还凋。待将春恨，都付春潮。过窈娘堤，秋娘渡，泰娘桥。 
中心藏之，何日忘之!

Update 2023-03-05 派克直播间飞花令
钓鱼台，十年不上野鸥猜。白云来往青山在，对酒开怀。欠伊周济世才，犯刘阮贪杯戒，还李杜吟诗债。酸斋笑我，我笑酸斋。晚归来，西湖山上野猿哀。二十年多少风流怪，花落花开。望云霄拜将台。袖星斗安邦策，破烟月迷魂寨。酸斋笑我，我笑酸斋。  
知君用心如日月，事夫誓拟同生死。还君明珠双泪垂，恨不相逢未嫁时。  
千古风流八咏楼，江山留与后人愁。水通南国三千里，气压江城十四州。  
问余何意栖碧山，笑而不答心自闲。桃花流水窅然去，别有天地非人间。  
德也狂生耳。偶然间，缁尘京国，乌衣门第。有酒惟浇赵州土，谁会成生此意。不信道、竟逢知己。青眼高歌俱未老，向尊前、拭尽英雄泪。君不见，月如水。共君此夜须沉醉。且由他，蛾眉谣诼，古今同忌。身世悠悠何足问，冷笑置之而已。寻思起、从头翻悔。一日心期千劫在，后身缘、恐结他生里。然诺重，君须记。 纳兰性德《金缕曲·赠梁汾》
恨君不似江楼月，南北东西，南北东西，只有相随无别离。恨君却似江楼月，暂满还亏，暂满还亏，待得团圆是几时？ 吕本中《采桑子·恨君不似江楼月》  
自君之出矣，不复理残机。思君如满月，夜夜减清辉。 张九龄《赋得自君之出矣》

Update 2023-03-08 派克直播间飞花令， “江” 字飞花令
江东子弟今虽在，肯与君王卷土来？ 王安石《叠题乌江亭》
江雨霏霏江草齐，六朝如梦鸟空啼。 韦庄《台城》
残灯无焰影幢幢，此夕闻君谪九江。垂死病中惊坐起，暗风吹雨入寒窗。元稹《闻乐天授江州司马》
少年听雨歌楼上，红烛昏罗帐。壮年听雨客舟中，江阔云低、断雁叫西风。而今听雨僧庐下，鬓已星星也。悲欢离合总无情，一任阶前、点滴到天明。 蒋捷《虞美人·听雨》
杀尽江南百万兵，腰间宝剑血犹腥！老僧不识英雄汉，只管哓哓问姓名。 朱元璋 《不惹庵示僧》

Update 2023-08-11 历史记录
缺月挂疏桐，漏断人初静。谁见幽人独往来，缥缈孤鸿影。惊起却回头，有恨无人省。拣尽寒枝不肯栖，寂寞沙洲冷。  ——《卜算子·黄州定会院寓居作》

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2023/01/03/Other/Other_Malloc-docker-image-md/" title="制作一个可用的malloc image">制作一个可用的malloc image</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-01-03T04:58:47.000Z" title="发表于 2023-01-03 12:58:47">2023-01-03</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.963Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">写一个melloc的C程序#include &lt;stdio.h>
#include &lt;stdlib.h>
#include &lt;string.h>
 
#include &lt;signal.h>
#include &lt;unistd.h>
 
#define SIGTERM_MSG "SIGTERM received.\n"
 
void sig_term_handler(int signum, siginfo_t *info, void *ptr)
&#123;
    write(STDERR_FILENO, SIGTERM_MSG, sizeof(SIGTERM_MSG));
&#125;
 
void catch_sigterm()
&#123;
    static struct sigaction _sigact;
 
    memset(&amp;_sigact, 0, sizeof(_sigact));
    _sigact.sa_sigaction = sig_term_handler;
    _sigact.sa_flags = SA_SIGINFO;
 
    sigaction(SIGTERM, &amp;_sigact, NULL);
&#125;

int main(int argc, char *argv[])
&#123;
    if ( argc != 2 )
    &#123;
        printf("ERROR ELEMENT COUNTS.");
        return 1;
    &#125;

    // printf ("%ld\n", atol(argv[1]));

    int *p;
    long n = atol(argv[1]) * 1024 * 1024;

    // printf ("%ld\n", n);
    
    printf("Allocate Memory Size: %ld MB.\n", atol(argv[1]));
    
    // Allocate Memory.
    p = (int *)malloc(n);
    
    if (p != NULL)
        printf("SUCCESS.");
    else
        printf("FAILED.");
    
    memset(p, 0, n);

    catch_sigterm();

    free(p);
    
    sleep(3000);

    return 0;
&#125;

写一个DockerfileFROM alpine
RUN apk add build-base
COPY mem.c .
RUN gcc -o mem mem.c
# or RUN gcc -static -o mem mem.c

FROM alpine
COPY --from=0 ./mem .
ENTRYPOINT [ "/mem" ]

进行一个很新的测试Build Image.
dive build -t liarlee-malloc:latest .
OR
docker build -t liarlee-malloc:latest .
Docker RUN.docker run --name malloc --rm -dt liarlee-malloc:latest 200
# docker run --name malloc --rm -dt liarlee-malloc:latest [MemorySize(MB)]
Kubernetes Deployment RUN.---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: liarlee-malloc
spec:
  selector:
    matchLabels:
      app: malloc
  replicas: 1 
  template: 
    metadata:
      labels:
        app: malloc
    spec:
      containers:
      - name: malloc
        image: liarlee-malloc:latest
        args: [ "200" ]
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/12/27/Linux/Linux_Nginx-Performance-Tuning-Pre-md/" title="Nginx性能调整（不一定对">Nginx性能调整（不一定对</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-27T04:53:02.000Z" title="发表于 2022-12-27 12:53:02">2022-12-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.736Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Nginx 性能优化看了不少的文档和说明， 尝试调整一下Nginx，看看与默认的设置性能表现能有多大的差距,顺便记录一下步骤，不记录的话自己会忘记的。 
Sysctl 参数~]$ cat /etc/sysctl.d/99-hayden.cof
net.ipv4.tcp_wmem = 8192 4194304 8388608
net.ipv4.tcp_rmem = 8192 4194304 8388608

net.core.somaxconn = 262144

net.core.default_qdisc=fq
net.ipv4.tcp_congestion_control=bbr
Nginx Config~]$ cat /etc/nginx/nginx.conf

user nginx;
worker_processes 1;
worker_cpu_affinity 10;
# error_log /var/log/nginx/error.log;
pid /run/nginx.pid;

# Load dynamic modules. See /usr/share/doc/nginx/README.dynamic.
include /usr/share/nginx/modules/*.conf;

events &#123;
    worker_connections 10240;
&#125;
...# 后面的都是默认值。

Systemd Nginx Service~]$ systemctl cat nginx
# /usr/lib/systemd/system/nginx.service
[Unit]
Description=The nginx HTTP and reverse proxy server
After=network-online.target remote-fs.target nss-lookup.target
Wants=network-online.target

[Service]
Type=forking
PIDFile=/run/nginx.pid
# Nginx will fail to start if /run/nginx.pid already exists but has the wrong
# SELinux context. This might happen when running `nginx -t` from the cmdline.
# https://bugzilla.redhat.com/show_bug.cgi?id=1268621
ExecStartPre=/usr/bin/rm -f /run/nginx.pid
ExecStartPre=/usr/sbin/nginx -t
ExecStart=/usr/sbin/nginx
ExecReload=/usr/sbin/nginx -s reload
KillSignal=SIGQUIT
TimeoutStopSec=5
KillMode=process
PrivateTmp=true
LimitAS=infinity
LimitRSS=infinity
LimitCORE=infinity
LimitNOFILE=65536
LimitNPROC=65535
Nice=-20

[Install]
WantedBy=multi-user.target
CPU Isolate~]$ cat /etc/default/grub
GRUB_CMDLINE_LINUX_DEFAULT="console=tty0 console=ttyS0,115200n8 net.ifnames=0 biosdevname=0 nvme_core.io_timeout=4294967295 rd.emergency=poweroff rd.shell=0 isolcpus=1 nohz_full=1 rcu_nocbs=1"
GRUB_TIMEOUT=0
GRUB_DISABLE_RECOVERY="true"
GRUB_TERMINAL="ec2-console"
GRUB_X86_USE_32BIT="true"

调整中断~]$ cat /proc/interrupts
           CPU0       CPU1
 27:          0        845   PCI-MSI 81920-edge      ena-mgmnt@pci:0000:00:05.0
 28:    1455005          0   PCI-MSI 81921-edge      eth0-Tx-Rx-0
 29:          0    1870959   PCI-MSI 81922-edge      eth0-Tx-Rx-1
LOC:     119785      15755   Local timer interrupts

~]$ echo 0 > /proc/irq/29/smp_affinity_list
~]$ echo 0 > /proc/irq/28/smp_affinity_list
~]$ echo 0 > /proc/irq/27/smp_affinity_list

关闭 Irqbalance~]$ sudo systemctl stop irqbalance.service
~]$ sudo systemctl disable irqbalance.service
Removed symlink /etc/systemd/system/multi-user.target.wants/irqbalance.service.

测试这些都做完之后，启动一个新的实例，安装nginx之后不做任何的动作， 使用ab命令测试默认页 index.html。 对比
调整后的实例结果如下使用命令： ab -c 3000 -n 50000 http://172.31.37.166:80/index.html
第一次Document Path:          &#x2F;index.html
Document Length:        732 bytes

Concurrency Level:      3000
Time taken for tests:   2.225 seconds
Complete requests:      50000
Failed requests:        0
Total transferred:      48250000 bytes
HTML transferred:       36600000 bytes
Requests per second:    22474.64 [#&#x2F;sec] (mean)
Time per request:       133.484 [ms] (mean)
Time per request:       0.044 [ms] (mean, across all concurrent requests)
Transfer rate:          21179.71 [Kbytes&#x2F;sec] received

Connection Times (ms)
              min  mean[+&#x2F;-sd] median   max
Connect:        0   67 139.7     47    1124
Processing:    25   63  16.1     61     274
Waiting:        0   47  13.7     46     254
Total:         57  130 143.2    106    1191

Percentage of the requests served within a certain time (ms)
  50%    106
  66%    125
  75%    131
  80%    135
  90%    142
  95%    149
  98%    165
  99%   1173
 100%   1191 (longest request)
第二次Document Path:          &#x2F;index.html
Document Length:        732 bytes

Concurrency Level:      3000
Time taken for tests:   2.232 seconds
Complete requests:      50000
Failed requests:        0
Total transferred:      48250000 bytes
HTML transferred:       36600000 bytes
Requests per second:    22397.25 [#&#x2F;sec] (mean)
Time per request:       133.945 [ms] (mean)
Time per request:       0.045 [ms] (mean, across all concurrent requests)
Transfer rate:          21106.78 [Kbytes&#x2F;sec] received

Connection Times (ms)
              min  mean[+&#x2F;-sd] median   max
Connect:        0   67 141.3     47    1092
Processing:    22   63  15.8     60     106
Waiting:        0   47  13.5     46      84
Total:         60  130 143.5    108    1175

Percentage of the requests served within a certain time (ms)
  50%    108
  66%    122
  75%    131
  80%    134
  90%    141
  95%    149
  98%    165
  99%   1150
 100%   1175 (longest request)
第三次Document Path:          &#x2F;index.html
Document Length:        732 bytes

Concurrency Level:      3000
Time taken for tests:   2.220 seconds
Complete requests:      50000
Failed requests:        0
Total transferred:      48250000 bytes
HTML transferred:       36600000 bytes
Requests per second:    22526.08 [#&#x2F;sec] (mean)
Time per request:       133.179 [ms] (mean)
Time per request:       0.044 [ms] (mean, across all concurrent requests)
Transfer rate:          21228.19 [Kbytes&#x2F;sec] received

Connection  ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/12/20/Other/Other_Docker-daemon-proxy-setting-md/" title="Docker/Containerd/Harbor 配置代理">Docker/Containerd/Harbor 配置代理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-12-20T04:39:07.000Z" title="发表于 2022-12-20 12:39:07">2022-12-20</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.963Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span></div><div class="content">记录一下 Docker Daemon &#x2F; Containerd 配置代理的步骤，尽管能用的时候不太多。
Update - 2023-07-10 Add Harbor.
Docker创建文件]$ mkdir -pv /etc/systemd/system/docker.service.d
]$ touch /etc/systemd/system/docker.service.d/proxy.conf

写入内容[Service]
Environment="HTTP_PROXY=socks5://&lt;-->:&lt;-->/"
Environment="HTTPS_PROXY=socks5://&lt;-->:&lt;-->/"
Environment="NO_PROXY=localhost,127.0.0.1"

重启DockerDaemon]$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker

Containerd创建环境变量文件]$ mkdir -pv /etc/systemd/system/containerd.service.d
]$ touch /etc/systemd/system/containerd.service.d/proxy.conf

写入内容[Service]
Environment="HTTP_PROXY=socks5://&lt;-->:&lt;-->/"
Environment="HTTPS_PROXY=socks5://&lt;-->:&lt;-->/"
Environment="NO_PROXY=localhost,127.0.0.1"

重启 Containerd]$ sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart docker

Harbor 仓库加代理另一个方法是直接给harbor仓库添加代理， 让Harbor来进行代理访问 ，帮忙pull镜像， 集群直接指向这个仓库即可。
harbor我是直接使用docker-compose 的方式部署的， 这样简单一些。 
我的软件安装目录是在: /opt/harbor

找到harbor 的配置文件： /opt/harbor/harbor.yml

配置文件部分如下： 
189	# Global proxy
190	# Config http proxy for components, e.g. http://my.proxy.com:3128
191	# Components doesn't need to connect to each others via http proxy.
192	# Remove component from `components` array if want disable proxy
193	# for it. If you want use proxy for replication, MUST enable proxy
194	# for core and jobservice, and set `http_proxy` and `https_proxy`.
195	# Add domain to the `no_proxy` field, when you want disable proxy
196	# for some special registry.
197	proxy:
198	  http_proxy: socks5://ip:port
199	  https_proxy: socks5://ip:port
200	  no_proxy:
201	  components:
202	    - core
203	    - jobservice
204	    - trivy

启动服务之后进入容器进行确认： 
core]$ dc exec -it core bash

harbor [ /harbor ]$ env | grep -i proxy
PERMITTED_REGISTRY_TYPES_FOR_PROXY_CACHE=docker-hub,harbor,azure-acr,aws-ecr,google-gcr,quay,docker-registry,github-ghcr
NO_PROXY=redis,chartmuseum,portal,db,.internal,.local,127.0.0.1,localhost,core,trivy-adapter,nginx,postgresql,registryctl,log,jobservice,registry,notary-server,notary-signer,exporter
HTTPS_PROXY=socks5://ip:port
HTTP_PROXY=socks5://ip:port

环境变量已经生效了， 然后直接在控制台创建项目， 创建新的仓库缓存代理。
我创建的仓库如下，除了列出的其他留空：



提供者
目标名
目标URL



Quay
registry.k8s.io
https://registry.k8s.io


GtihubGHCR
ghcr.io
https://ghcr.io


Quay
k8s.gcr.io (Archived)
https://k8s.gcr.io


Quay
gcr.io
https://gcr.io


Quay
quay.io
https://quay.io


Docker Hub
docker.io
https://hub.docker.com


DockerRegistry
public.ecr.aws
https://public.ecr.aws




所有仓库的健康检查是通过的， 然后去创建项目， 就行了。 
创建完成项目之后， 就可以通过当前这个仓库的项目地址来pull 镜像了。 
docker pull reg.liarlee.site/docker.io/library/nginx:latest
docker pull reg.liarlee.site/registry.k8s.io/metrics-server/metrics-server@sha256:1ab8d2722ce57979eb05ec0594cb9173e07ace16a253c747bb94c31b138a07dc
docker pull reg.liarlee.site/public.ecr.aws/amazonlinux/amazonlinux:minimal
docker pull reg.liarlee.site/quay.io/prometheus/prometheus
docker pull reg.liarlee.site/quay.io/argoproj/argocd
docker pull reg.liarlee.site/ghcr.io/dexidp/dex

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/10/11/Linux/Linux_Isolate-cpu-and-taskset-to-it/" title="完全隔离CPU的方法">完全隔离CPU的方法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-10-11T05:05:26.000Z" title="发表于 2022-10-11 13:05:26">2022-10-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.734Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">完全隔离CPU， 并排空CPU所有的进程，分配指定的任务到CPU上。

当前的情况下， 我有CPU 0 - 3， 我希望隔离出CPU3 来进行指定的任务运行（方法是传递内核参数并重启）
vim /etc/default/grub

GRUB_CMDLINE_LINUX='................isolcpu=3 nohz_full=3'

grub2-mkconfig -o /boot/grub2/grub.cfg
sync &amp;&amp; systemctl reboot

重启之后，CPU3 就已经从CFS的调度列表上面拿掉了， 可以通过如下的参数证明。
# 查看设置是否已经正确的生效， 分离的CPU会表示为序号， 一般是： 0-2 ， 3 。 这两种表示方式。
cat /sys/devices/system/cpu/isolated
3
cat /sys/devices/system/cpu/nohz_full
3

关于nohz_full这个参数， 需要编译内核的时候就启用这个功能。
# 需要启用的参数如下：
CONFIG_NO_HZ_COMMON=y
CONFIG_NO_HZ_FULL=y
CONFIG_NO_HZ=y
这部分的内容还包括的Kernel的Tickless等等知识， 这个部分的内容我在Redhat的文档中有看到。但是文档比较旧了， 这个设置是基于Redhat 7 版本的说明， 可能现在有更好的方法，我不太确定。

重启之后， CPU3上面已经完全不会有用户空间的进程被调度上去了，同时， 由于已经配置了NOHZ的参数，CPU3 上面也不会有Kernel Timer Interrept触发，因此也少了一部分中断。

Redhat的文档中定义了如下的方式进行验证， 我直接抄下面的内容了。

关于如何验证Cpu隔离的文档： https://access.redhat.com/solutions/3875421配置隔离CPU的方法已经确认功能是否激活的方法：https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/index

perf stat -C 1 -e irq_vectors:local_timer_entry taskset -c 3 sleep 3
先记录这么多吧， 这个功能用到的地方实在是有限，基本上不怎么需要。并且需要知道是现在已经不怎么需要使用这个工具进行调优了， Redhat有 Tuned 守护进程， 可以通过默认的profile对OS进行调优。


</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/08/19/AWS/EKS_CiliumInstallation&amp;Summary/" title="Cilium 踩坑总结">Cilium 踩坑总结</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-08-19T14:21:48.000Z" title="发表于 2022-08-19 22:21:48">2022-08-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.715Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">测试环境
KVM  - hayden@HaydenArchDesktop ~&gt; virsh version

 Compiled against library: libvirt 8.6.0 Using library: libvirt 8.6.0 Using API: QEMU 8.6.0 Running hypervisor: QEMU 7.0.0


VM OS： root@fedora ~# cat &#x2F;etc&#x2F;os-release

 NAME&#x3D;”Fedora Linux” VERSION&#x3D;”36 (Thirty Six)” ID&#x3D;fedora VERSION_ID&#x3D;36


KVM 虚拟机两台


Master： hostname: fedora
Node： hostname: knode1



kubernetes 版本： v1.24.3

kubernetes 安装方式： kubeadm 

docker版本:  docker:&#x2F;&#x2F;20.10.17 

NOTE: (尝试使用Containerd， 但是翻车了， 控制平面的Pod启动不了， 所以放弃了，遂使用CRI-Dockerd，配置了Docker runtime， Containerd使用默认的参数无法正常的启动， 看起来即使真的升级到了1.24 迁移还是一个问题)


Kernel Version: 5.17.5-300.fc36.x86_64


Helm参数如果是在KVM启动的虚拟机，可以通过这个安装参数来开启更多功能，但是受限于我的KVM虚拟网卡驱动不能attach xdp 程序， 所以。。。。xdp 加速无法启用，但是其他的高级特性均可开启， 集群状态正常。
helm upgrade -i cilium cilium/cilium \
  --namespace kube-system \
  --set tunnel=disabled \
  --set autoDirectNodeRoutes=true \
  --set loadBalancer.mode=dsr \
  --set kubeProxyReplacement=strict \
  --set enableIPv4Masquerade=false \
  --set loadBalancer.algorithm=maglev \
  --set devices=enp1s0 \
  --set k8sServiceHost=192.168.31.100 \
  --set k8sServicePort=6443 \
  --set hubble.relay.enabled=true \
  --set hubble.ui.enabled=true
Cilium Clitouch .&#x2F;install_cilium_cli.sh
CILIUM_CLI_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt)
CLI_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then CLI_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/cilium-cli/releases/download/$&#123;CILIUM_CLI_VERSION&#125;/cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz&#123;,.sha256sum&#125;
sha256sum --check cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz.sha256sum
sudo tar xzvfC cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz /usr/local/bin
rm cilium-linux-$&#123;CLI_ARCH&#125;.tar.gz&#123;,.sha256sum&#125;
Hubble Clitouch .&#x2F;install_hubble_client.sh
HUBBLE_VERSION=$(curl -s https://raw.githubusercontent.com/cilium/hubble/master/stable.txt)
HUBBLE_ARCH=amd64
if [ "$(uname -m)" = "aarch64" ]; then HUBBLE_ARCH=arm64; fi
curl -L --fail --remote-name-all https://github.com/cilium/hubble/releases/download/$HUBBLE_VERSION/hubble-linux-$&#123;HUBBLE_ARCH&#125;.tar.gz&#123;,.sha256sum&#125;
sha256sum --check hubble-linux-$&#123;HUBBLE_ARCH&#125;.tar.gz.sha256sum
sudo tar xzvfC hubble-linux-$&#123;HUBBLE_ARCH&#125;.tar.gz /usr/local/bin
rm hubble-linux-$&#123;HUBBLE_ARCH&#125;.tar.gz&#123;,.sha256sum&#125;

Cilium Cli 安装插件命令cilium upgrade # 原地升级

# 安装并直接替换kubeproxy
cilium install --version 1.14.1 --set kubeProxyReplacement=true --set=ipam.operator.clusterPoolIPv4PodCIDRList="10.42.0.0/16" --set k8sServiceHost=kube3s.liarlee.site --set k8sServicePort=6443


# 升级
hayden@arch ~> cilium upgrade
🔮 Auto-detected Kubernetes kind: K3s
ℹ️  Using Cilium version 1.14.2
🔮 Auto-detected cluster name: default

# 看状态
cilium status
# 使用hubble
cilium hubble port-forward &amp;

cilium hubble enable --ui

hubble observe --since=1m -t l7 -



特性以及状态检查默认的安装完成之后开启特性如下：

Kubeproxy Bypass
Iptables Bypass
LoadBalancer 算法： Meglav
LoadBalancer 特性： DSR
报文Masquerade 封装： Disabled
Hubble ： Enable
隧道封包： Disabled


下面是 CIlium Status的命令返回结果：
root@fedora ~# cilium status
    /¯¯\
 /¯¯\__/¯¯\    Cilium:         OK
 \__/¯¯\__/    Operator:       OK
 /¯¯\__/¯¯\    Hubble:         OK
 \__/¯¯\__/    ClusterMesh:    disabled
    \__/

Deployment        cilium-operator    Desired: 2, Ready: 2/2, Available: 2/2
Deployment        hubble-relay       Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet         cilium             Desired: 2, Ready: 2/2, Available: 2/2
Deployment        hubble-ui          Desired: 1, Ready: 1/1, Available: 1/1
Containers:       hubble-relay       Running: 1
                  hubble-ui          Running: 1
                  cilium             Running: 2
                  cilium-operator    Running: 2
Cluster Pods:     14/14 managed by Cilium
Image versions    hubble-ui          quay.io/cilium/hubble-ui:v0.9.0@sha256:0ef04e9a29212925da6bdfd0ba5b581765e41a01f1cc30563cef9b30b457fea0: 1
                  hubble-ui          quay.io/cilium/hubble-ui-backend:v0.9.0@sha256:000df6b76719f607a9edefb9af94dfd1811a6f1b6a8a9c537cba90bf12df474b: 1
                  cilium             quay.io/cilium/cilium:v1.12.0@sha256:079baa4fa1b9fe638f96084f4e0297c84dd4fb215d29d2321dcbe54273f63ade: 2
                  cilium-operator    quay.io/cilium/operator-generic:v1.12.0@sha256:bb2a42eda766e5d4a87ee8a5433f089db81b72dd04acf6b59fcbb445a95f9410: 2
                  hubble-relay       quay.io/cilium/hubble-relay:v1.12.0@sha256:ca8033ea8a3112d838f958862fa76c8d895e3c8d0f5590de849b91745af5ac4d: 1

ds&#x2F;cilium 中的命令返回结果：
root@fedora:/home/cilium# cilium status
KVStore:                 Ok   Disabled
Kubernetes:              Ok   1.24 (v1.24.3) [linux/amd64]
Kubernetes APIs:         ["cilium/v2::CiliumClusterwideNetworkPolicy", "cilium/v2::CiliumEndpoint", "cilium/v2::CiliumNetworkPolicy", "cilium/v2::CiliumNode", "core/v1::Namespace", "core/v1::Node", "core/v1::Pods", "core/v1 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/AWS/EKS_ConfigureNodeGC/" title="如何配置kubelet的节点自动回收资源">如何配置kubelet的节点自动回收资源</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.715Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">配置Node节点按照磁盘阈值回收空间
https://aws.amazon.com/cn/premiumsupport/knowledge-center/eks-worker-nodes-image-cache/



修改Kubelet参数
Kubelet默认提供了GC的参数  --image-gc-high-threshold 参数用于定义触发映像垃圾收集的磁盘使用百分比。默认值为 85%。
--image-gc-low-threshold 参数用于定义映像垃圾收集尝试释放的磁盘使用百分比。默认值为 80%。  
如果是自己管理的Node，最好的方式是直接配置kubelet命令行的参数，将上面的参数指定需要的阈值，然后重启kubelet即可。配置文件一般在 ： &#x2F;etc&#x2F;kubernetes&#x2F;kubelet.config"imageGCHighThresholdPercent": 70, 
"imageGCLowThresholdPercent": 50,  

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_CalcStealtime/" title="由 CPU Steal Time 指标解释">由 CPU Steal Time 指标解释</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.723Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">观察vmstat命令输出的内容， 或者 top 命令 最上面输出的内容。可以看到Stealtime增加，大部分时候都是因为虚拟机的超卖， 总结来说st指示了vCPU的繁忙程度。
进程相关参数的说明https://www.kernel.org/doc/html/latest/scheduler/sched-stats.html

schedstats also adds a new &#x2F;proc&#x2F;&#x2F;schedstat file to include some of the same information on a per-process level. There are three fields in this file correlating for that process to:1 time spent on the cpu2 time spent waiting on a runqueue3 # of timeslices run on this cpu  

执行命令的输出结果hayden@VM-16-6-ubuntu /p/3720475> cat schedstat
2236062 223986 22

命令中的三个数值说明:  

2236062 进程在CPU的时间
223986 进程在CPU调度上面等待的时间
22 在这个CPU运行的时间片数量NOTE: 有一个博客写这个是 上下文交换的次数 ， 和sched 文件中的 nr_switches 数量相同， 不能确定是否正确。

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_Epoll/" title="Epoll vs select vs poll vs io_uring">Epoll vs select vs poll vs io_uring</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.731Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">select方式：使用fd_set结构体告诉内核同时监控那些文件句柄，使用逐个排查方式去检查是否有文件句柄就绪或者超时。该方式有以下缺点：文件句柄数量是有上限的，逐个检查吞吐量低，每次调用都要重复初始化fd_set。
poll方式：该方式主要解决了select方式的2个缺点，文件句柄上限问题(链表方式存储)以及重复初始化问题(不同字段标注关注事件和发生事件)，但是逐个去检查文件句柄是否就绪的问题仍然没有解决。
epoll方式：该方式可以说是C10K问题的killer，他不去轮询监听所有文件句柄是否已经就绪。epoll只对发生变化的文件句柄感兴趣。其工作机制是，使用”事件”的就绪通知方式，通过epoll_ctl注册文件描述符fd，一旦该fd就绪，内核就会采用类似callback的回调机制来激活该fd, epoll_wait便可以收到通知, 并通知应用程序。而且epoll使用一个文件描述符管理多个描述符,将用户进程的文件描述符的事件存放到内核的一个事件表中, 这样数据只需要从内核缓存空间拷贝一次到用户进程地址空间。而且epoll是通过内核与用户空间共享内存方式来实现事件就绪消息传递的，其效率非常高。但是epoll是依赖系统的(Linux)。
异步I&#x2F;O以及Windows，该方式在windows上支持很好，这里就不具体介绍啦。
io_uring 其实是内核5.10之后引进的一种方式，目前还没有应用是使用这个模式的， 但是这个方式大大的减少了应用程序的系统调用次数。 性能有增长。现在有了， 比如 amazon的一些agent类别的软件。 
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_KernelParam_NOHZ/" title="关于内核Config中的参数 CONFIG_NO_HZ">关于内核Config中的参数 CONFIG_NO_HZ</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.734Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">关于Tick， Tickless的研究。

https://www.kernel.org/doc/html/latest/timers/no_hz.html

这几个参数的最终意义都和 Jitter 相关， 设置的参数含义是 ： CPU时钟中断的周期， 如果是 100HZ ， 那么1s的时间内CPU会中断100次。目前最新的内核支持 NOHZ ， 也就是在没有任务的时候处于节能的考虑不进行中断。当有需要运行的业务时还是会正常的触发CPU中断。NOHZ主要的功能时省电， 调整这个参数的意义就是让CPU处在合理的中断次数。过多的中断会导致相关的任务被打断。

git:&#x2F;&#x2F;git.kernel.org&#x2F;pub&#x2F;scm&#x2F;linux&#x2F;kernel&#x2F;git&#x2F;frederic&#x2F;dynticks-testing.git

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_Nginx%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%8E%8B%E6%B5%8B/" title="Nginx Performance Test">Nginx Performance Test</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.957Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">写在最前面， 这个问题还在研究中， 我目前还没有合适的模型用来研究这个问题， Pending….
Nginx 性能测试与压力计算使用al2023  + 默认的yum仓库软件版本， 具体信息记录如下： 
按照下面的配置， 在Nginx上面发布一个目录， 其中放了一个Fedora镜像， 大小大约2.3GB， 固定Nginx处理请求的大小， 控制大部分的因素来尝试获取精确的结果。

OS Version
[root@ip-172-31-53-146 ~]# cat /etc/os-release
NAME="Amazon Linux"
VERSION="2023"
ID="amzn"
ID_LIKE="fedora"
VERSION_ID="2023"
PLATFORM_ID="platform:al2023"
PRETTY_NAME="Amazon Linux 2023"
ANSI_COLOR="0;33"
CPE_NAME="cpe:2.3:o:amazon:amazon_linux:2023"
HOME_URL="https://aws.amazon.com/linux/"
BUG_REPORT_URL="https://github.com/amazonlinux/amazon-linux-2023"
SUPPORT_END="2028-03-01"

在一个机器上面部署MySQL 然后初始化配置一个密码和用户。
[root@ip-172-31-53-200 ~]# mysql --version
mysql  Ver 8.0.32 for Linux on x86_64 (MySQL Community Server - GPL)

直接使用bitnami的wordpress， 仅部署wordpress在另一个机器上面，然后配置连接到上面的那台数据库。 

额外配置一下Docker-compose 的CPU affinity， 绑到cpu0上面， 让这个Container只能用cpu0, 模拟只有一个核心， 后面应该还会给他放开。
   ---
   version: '2'
   services:
     wordpress:
       cpuset: "0"
       network_mode: host

5. 尝试给点压力， 做个基准测试。

### 测试1 单线程计算QPS

> 公式：  1000ms/RT = QPS 
>
> 这个是单线程的QPS 与 RT 的关系， 试图走一把流程验证这个。 

运行命令 wrk 配置 1connection 1thread 进行测试， 测试时间5分钟。

获取 RT，HTTP请求从发出到响应的时间 ： 

![2023-04-27_10-38.png](https://s2.loli.net/2023/04/27/pf2Z5erTzcayPFJ.png)

基于当前获取到的RT就可以计算出单线程的QPS ： 

```bash
RT：45ms # 这个时候获取的RT是客户端从发出数据包 到 收到完整的请求的页面返回的时间， 基于上面的抓包结果计算。
1000ms/45ms = 22 QPS

实际命令返回的结果： 
~ ❯❯❯ wrk -t1 -c1 -d5m --latency http://nginx.liarlee.site:8080
Running 5m test @ http://nginx.liarlee.site:8080
  1 threads and 1 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    45.00ms    3.60ms 169.02ms   88.75%
    Req/Sec    22.24      4.27    30.00     76.72%
  Latency Distribution
     50%   44.28ms
     75%   46.23ms
     90%   48.51ms
     99%   54.40ms
  6668 requests in 5.00m, 337.69MB read
Requests/sec:     22.23 # 确实是 22左右， 基本上和计算得出的结果一致。
Transfer/sec:      1.13MB

测试2 计算服务当前的最佳线程数量默认的情况下， 当一个进程处理的时候， 线程接收到请求， OnCPU处理这个请求， 发出数据库查询，切换到Sleep状态，数据库处理完成，线程回到CPU继续运算，发送结果给客户端，这样完成一个Request。
计算服务的最佳线程数量其实是 需要多少个线程占用CPU，最终可以填满 CPU 1s 的时间， 让CPU尽可能都用来处理业务请求。
所以最大的线程数量是变化的， 与CPU time 相关 或者说 与RT相关， 对于一些特定的请求，控制了大部分变量的场景下， 可以计算一个最佳的线程数量。 计算公式： 

最佳线程数： CPU TIme + Wait Time &#x2F; CPU Time &#x3D; 2 + 42 &#x2F; 2 &#x3D; 22  

打开文件数限制可能的并发数量测试 conntrack 的问题， 发现连接数不太高， 试图通过wrk 提高并发的连接数量。使用命令： 
wrk -t2 -c30000 -d30s http://reg.liarlee.site:80
如果我的理解没有问题， 那么我应该可以创建30000个连接 在 客户端 和 服务端 之间。但是结果： 
# 客户端尝试建立 8000c
ec2-user@arch ~> wrk -t1 -c8000 -d30s http://reg.liarlee.site:80
Running 30s test @ http://reg.liarlee.site:80
  1 threads and 8000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency    49.94ms   42.36ms 428.68ms   92.65%
    Req/Sec    21.15k     8.19k   41.30k    66.08%
  608596 requests in 30.04s, 218.23MB read
  Socket errors: connect 6980, read 0, write 0, timeout 0
Requests/sec:  20260.60
Transfer/sec:      7.27MB

# 服务端看到的是： 
[root@reg tools]# ss -s
Total: 282
TCP:   1103 (estab 28, closed 1063, orphaned 0, timewait 2)

Transport Total     IP        IPv6
RAW	  0         0         0
UDP	  8         4         4
TCP	  40        36        4
INET	  48        40        8
FRAG	  0         0         0
这和预期的差距挺大的， 1103 让我非常容易的想到了 1024 的文件描述符限制。 于是 
ec2-user@arch ~> ulimit -n
1024
客户端  wrk 的文件描述符可用调大到 100000 。
# 客户端
ec2-user@arch ~ [127]> ulimit -n
100000
ec2-user@arch ~> wrk -t2 -c30000 -d30s http://reg.liarlee.site:80
Running 30s test @ http://reg.liarlee.site:80
  2 threads and 30000 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   515.29ms  411.20ms   2.00s    78.35%
    Req/Sec     1.76k     2.70k   11.96k    85.71%
  39021 requests in 30.28s, 13.99MB read
  Socket errors: connect 0, read 298852, write 16, timeout 3115
Requests/sec:   1288.65
Transfer/sec:    473.17KB

# 服务端
[root@reg tools]# ss -s
Total: 282
TCP:   7544 (estab 28, closed 7504, orphaned 1641, timewait 2)

Transport Total     IP        IPv6
RAW	  0         0         0
UDP	  8         4         4
TCP	  40        36        4
INET	  48        40        8
FRAG	  0         0         0
如果从上面的角度来看， 那么客户端的打开文件数是限制， 现在打开文件数调大了， 服务端还是无法建立更多的连接， 这个还得继续看看 ， 我感觉是cpu的问题， 毕竟我访问的页面的是有内容的， 不是一个空请求。
# 尝试看了以下服务端的ss 命令
ec2-user@arch ~> ss -s
Total: 31669
TCP:   31268 (estab 7959, closed 1182, orphaned 1250, timewait 1)

Transport Total     IP        IPv6
RAW	  1         0         1
UDP	  4         2         2
TCP	  30086     30080     6
INET	  30091     30082     9
FRAG	  0         0         0
确实发起了30000 但是 进入estab状态的连接只有 8000 左右。这基本上可以确定是服务端的限制了，具体是什么地方限制了待查。客户端 ： c5 实例类型上面的 wrk服务端 ： t3.micro  - docker compose 部署的 harbor
连接异常导致的orphans一个遗憾的事情， 最近遇到了一个可能是 cilium 的一个问题， 在小规模集群的场景下， Cilium 会错误的处理FIN ， FIN ACK， 导致安全组的 Connection Track 数量被打满。 之前没有注意过这个行为，在Docker的环境中测试下， 尝试使用 iptables 屏蔽容器发送出来的FIN， 记录步骤和命令： 
dnf install -y docker
systemctl enable --now docker
docker pull reg.liarlee.site&#x2F;docker.io&#x2F;library&#x2F;nginx&#x2F;nginx:latest
docker run -dt --rm --name nginx -p 81:80 reg.liarlee.site&#x2F;docker.io&#x2F;library&#x2F;nginx:l ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_OOMKiller/" title="OOM行为">OOM行为</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.737Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">关于OOM行为的思考 以及Kswapd的动作和行为。http://evertrain.blogspot.com/2018/04/oom.html更详细的打分算法见源码  https://github.com/torvalds/linux/blob/master/mm/oom_kill.c
发生之后OOM killer会将kill的信息记录到系统日志&#x2F;var&#x2F;log&#x2F;messages，检索相关信息就能匹配到是否触发。
grep 'Out of memory' /var/log/messages
也可以通过dmesg
dmesg -Tx | egrep -i 'killed process'

查看分数最高的进程#!/bin/bash
for proc in $(find /proc -maxdepth 1 -regex '/proc/[0-9]+'); do
    printf "%2d %5d %s\n" \
        "$(cat $proc/oom_score)" \
        "$(basename $proc)" \
        "$(cat $proc/cmdline | tr '\0' ' ' | head -c 50)"
done 2>/dev/null | sort -nr | head -n 10

保护措施设置OverCommit只有在OverCommit的时候才会触发OOM， 默认是许可一定程度的OverCommit的。 
https://docs.kernel.org/vm/overcommit-accounting.html
vm.overcommit_memory 作用是控制OverCommit是否被许可。
0
    Heuristic overcommit handling. Obvious overcommits of address space are refused. Used for a typical system. It ensures a seriously wild allocation fails while allowing overcommit to reduce swap usage. root is allowed to allocate slightly more memory in this mode. This is the default.
1
    Always overcommit. Appropriate for some scientific applications. Classic example is code using sparse arrays and just relying on the virtual memory consisting almost entirely of zero pages.
2
    Don’t overcommit. The total address space commit for the system is not permitted to exceed swap + a configurable amount (default is 50%) of physical RAM. Depending on the amount you use, in most situations this means a process will not be killed while accessing pages but will receive errors on memory allocation as appropriate.

    Useful for applications that want to guarantee their memory allocations will be available in the future without having to initialize every page.

[root@ip-172-31-9-192 log]# cat /proc/meminfo | grep Comm
CommitLimit:     4794236 kB
Committed_AS:    2344744 kB
---
CommitLimit： 可提交内存的上限， 超过这个上限系统认为目前内存已经是OverCommit。
Committed_AS： 已经提交内存的上限，当前所有进程已经提交的内存使用，这个不是已经分配出去的， 是进程申请的。

设置Killer的行为root@ip-172-31-11-235:/home/ec2-user|⇒  cat /proc/sys/vm/oom_kill_allocating_task
# 值为0：会 kill 掉得分最高的进程
# 值为非0：会kill 掉当前申请内存而触发OOM的进程

设置进程
对于需要保护的进程可以使用OOM_ADJ&#x3D;-17 将这个进程从OOM Killer的列表中移除（已经在内核的2.6之后废弃，处于兼容性保留了这个文件接口
调整OOM_SCORE_ADJ, 范围是 -1000 &lt; oom_score_adj &lt; 1000

直接调整到-1000，会出现在计算分数列表的最后  echo -1000 > /proc/31595/oom_score_adj
  手动触发一次OOM规则， Kill符合要求的进程  echo f > /proc/sysrq-trigger
调整服务的OOM Score对于服务本身的保护方式， 可以采用使用Systemd Unit file里面进行 OOMADJSCORE&#x3D;*** 的方式来指定，例如保护MySQL的进程不会在OOM Killer的列表中。
cat /usr/lib/systemd/system/mariadb.service

    [Service]
    Type=simple
    User=mysql
    Group=mysql

    ExecStartPre=/usr/libexec/mariadb-prepare-db-dir %n
    ExecStart=/usr/bin/mysqld_safe --basedir=/usr
    ExecStartPost=/usr/libexec/mariadb-wait-ready $MAINPID
    # Setting Here. and setting in the /proc/$PID/oom_score_adj.
    OOMScoreAdjust=-1000

sudo systemctl daemon-reload &amp;&amp; sudo systemctl restart mariadb

避免OOM的方式
关闭OverCommit： 关闭OverCommit会导致进程如果无法拿到内存就fast fail ， 不会出现OOMKiller干掉无辜进程的情况。
开启OverCommit， 开启一定程度的Swap： 开启Swap会导致内存在接近99%的时候，会出现系统响应变慢的问题，但是会由于申请的内存没有超过TotalMEM + TotalSWAP，因此不会触发OOM ，但是会导致明显的系统响应问题。 如果超过了 TotalMEM + TotalSWAP 会立刻触发一次OOMkiller结束进程。
开启OverCommit， 调整进程的优先级： 对于特定的进程进行保护。OOM会按照设置的积分计算需要Kill的进程。
开启OverCommit， 调整OOMkiller的行为方式： 计算积分后Kill 或者 直接Kill当前新申请内存的进程。这个方式感觉和默认的关闭OverCommit的行为类型，都是拒绝新的进程以保证旧的进程可以存活。

更可靠的方式Faceboook的oomdhttps://github.com/facebookincubator/oomdoomd使用的是 PSI接口来评估内存的压力，可以通过自定义规则的方式来对来进行内存压力的分析，而不是简单的内存用量。
Fedora的Early OOMhttps://github.com/rfjakob/earlyoomEarlyOOM的作用是提前OOM，这样可以保障用户空间的图形桌面不会到交换空间去， 主要解决的问题是内存压力过大的交换动作会将桌面环境换出导致响应变慢。
输出结果记录Cgroup - Pod
首先是哪个程序超过了cgroup的limit， 触发了cgroup 的oom， 这里是 xray 这个进程自己。
  kern  :warn  : [Thu Aug 17 17:29:27 2023] xray invoked oom-killer: gfp_mask&#x3D;0x100cca(GFP_HIGHUSER_MOVABLE), order&#x3D;0, oom_score_adj&#x3D;-997

这个部分是触发kprint的信息， 解释如下：在哪个CPU（1）上， 运行的进程PID（1586622）， commandline的名称（xray）， 哪个版本的内核， 是否被标记为 taint.
  kern  :warn  : [Thu Aug 17 17:29:27 2023] CPU: 1 PID: 1586622 Comm: xray Kdump: loaded Not tainted 5.14.0-333.el9.x86_64 #1

硬件信息， BIOS信息   
  kern  :warn  : [Thu Aug 17 17:29:27 2023] Hardware name: Red Hat KVM, BIOS 1.15.0-2.module_el8.6.0+2880+7d9e3703 04&#x2F;01&#x2F;2014

打印发生oom时刻的堆栈
  kern  :warn  : [Thu Aug 17 17:29:27 2023] Call Trace:
kern  :warn  : [Thu Aug 17 17:29:27 2023]  &lt;TASK&gt;
kern  :warn  : [Thu Aug 17 17:29:27 2023]  dump_stack_lvl+0x34&#x2F;0x48
kern  :warn  : [Thu Aug 17 17:29:27 2023]  dump_header+0x4a&#x2F;0x201
kern  :warn  : [Thu Aug 17 17:29:27 2023]  oom_kill_process.cold+0xb&#x2F;0x10
kern  :warn  : [Thu Aug 17 17:29:27 2023]  out_of_memory+0xed&#x2F;0x2e0
kern  :warn  : [Thu Aug 17 17:29:27 2023]  mem_cgroup_out_of_memory+0x13a&#x2F;0x150
kern  :warn  : [Thu Aug 17 17:29:27 2023]  try_charge_memcg+0x79d&#x2F;0x860
kern  :warn  : [Thu Aug 17 17:29:27 2023]  ? __mem_cgroup_charge+0x55&#x2F;0x80
kern  :warn  : [Thu Aug 17 17:29:27 2023]  charge_memcg+0x7a&#x2F;0xf0
kern  :warn  : [Thu Aug 17 17:29:27 2023]  __mem_cgroup_ch ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_Perf/" title="Perf 命令的Performance分析">Perf 命令的Performance分析</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.957Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">https://blog.gmem.cc/perf  一个非常详细的博客， 太强啦。
使用perf进行性能的简单输出root@ip-172-31-11-235:~|⇒  perf stat htop -d 1

 Performance counter stats for 'htop -d 1':

        181.764747      task-clock (msec)         #    0.055 CPUs utilized
                52      context-switches          #    0.286 K/sec
                 0      cpu-migrations            #    0.000 K/sec
               320      page-faults               #    0.002 M/sec
   &lt;not supported>      cycles
   &lt;not supported>      instructions
   &lt;not supported>      branches
   &lt;not supported>      branch-misses

       3.283236218 seconds time elapsed



使用perf记录性能指标到文件[root@ip-172-31-41-141 tmp]# perf record -F 99 -a -g -p 44551
[root@ip-172-31-41-141 tmp]# sudo perf record -F 99 -a -g -- sleep 60
Warning:
PID/TID switch overriding SYSTEM
^C[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0.021 MB perf.data (2 samples) ]
root@ip-172-31-11-235:~|⇒  sudo perf script > out.perf

生成火焰图通常的做法是将 out.perf 拷贝到本地机器，在本地生成火焰图：
$ git clone --depth 1 https://github.com/brendangregg/FlameGraph.git
# 折叠调用栈
$ perf script > out.perf
$ FlameGraph/stackcollapse-perf.pl out.perf > out.folded
# 生成火焰图
$ FlameGraph/flamegraph.pl out.folded > out.svg
生成火焰图可以指定参数，–width 可以指定图片宽度，–height 指定每一个调用栈的高度，生成的火焰图，宽度越大就表示CPU耗时越多。FlameGraph&#x2F;flamegraph.pl &lt; out.profile &gt; out.svg
[root@ip-172-31-18-198 timechart]# perf timechart record -g – curl http://localhost:19999[root@ip-172-31-18-198 timechart]# perf timechartWritten 0.0 seconds of trace to output.svg.
制造一个D进程Most proper way is to use freezer cgroup. It puts process to uninterruptible sleep in case of FROZEN cgroup state.
mkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezermount -t cgroup -ofreezer freezer &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezermkdir &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozenecho FROZEN &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;freezer.stateecho pidof you_process &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;tasksecho pgrep cp &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;tasksecho THAWED &gt; &#x2F;sys&#x2F;fs&#x2F;cgroup&#x2F;freezer&#x2F;frozen&#x2F;freezer.state
To put again to interruptible sleep, just change cgroup state to THAWED.
动态追踪
添加一个动态追踪的Tracepoint Eventperf probe --add&#x3D;&quot;probe:io_schedule_timeout&quot;
perf probe --add&#x3D;&quot;probe:io_schedule_timeout%return&quot;
# 使用
perf record -e probe:tcp_sendmsg -a -g -- sleep 5
# 分析
perf report --stdio
移除一个动态追踪的Tracepoint Eventperf probe --del&#x3D;&quot;probe:io_schedule_timeout&quot;
perf probe -d &quot;probe:io_schedule_timeout&quot;
列出所有存在的probe perf probe -l
查看追踪的结果perf script
perf probe -V tcp_sendmsg # 列出可用的变量列表
perf probe --add &#39;tcp_sendmsg size&#39; # 追踪这个变量
# Add a tracepoint for tcp_sendmsg() return, and capture the return value:
perf probe &#39;tcp_sendmsg%return $retval&#39;

关于Off-cpu进程的分析
按步骤生成]$ /usr/share/bcc/tools/offcputime -df -p `pgrep -nx mysqld` 30 > out.stacks
[...copy out.stacks to your local system if desired...]
]$ git clone https://github.com/brendangregg/FlameGraph
]$ cd FlameGraph
]$ ./flamegraph.pl --color=io --title="Off-CPU Time Flame Graph" --countname=us &lt; out.stacks > out.svg
一条命令出图 ]$ grep do_command &lt; out.stacks | .&#x2F;flamegraph.pl --color&#x3D;io --title&#x3D;&quot;Off-CPU Time Flame Graph&quot; --countname&#x3D;us &gt; out.svg

Perf命令的常见参数
内核设置要启用内核动态追踪，需要使用内核编译参数CONFIG_KPROBES&#x3D;y、CONFIG_KPROBE_EVENTS&#x3D;y。要追踪基于帧指针的内核栈，需要内核编译参数CONFIG_FRAME_POINTER&#x3D;y。要启用用户动态追踪，需要使用内核编译参数CONFIG_UPROBES&#x3D;y、CONFIG_UPROBE_EVENTS&#x3D;y
子命令列表

perf支持一系列的子命令：子命令 	说明annotate 	读取perf.data并显示被注解的代码bench 	基准测试的框架config 	在配置文件中读写配置项diff 	读取perf.data并显示剖析差异evlist 	列出perf.data中的事件名称inject 	用于增强事件流的过滤器kmem 	跟踪&#x2F;度量内核内存属性kvm 	跟踪&#x2F;度量KVM客户机系统list 	显示符号化的事件列表lock 	分析锁事件mem 	分析内存访问record 	执行剖析report 	显示剖析结果sched 	分析调度器stat 	获取性能计数top 	显示成本最高的操作并动态刷新trace 	类似于strace的工具probe 	定义新的动态追踪点

perrf record 命令参数 --exclude-perf 	不记录perf自己发起的事件
-p 	收集指定进程的事件，逗号分割的PID列表
-a 	使用Per-CPU模式，如果不指定-C，则相当于全局模式。如果指定-C，则可以选定若干CPU
-g 	记录调用栈
-F 	以指定的频率剖析
-T 	记录样本时间戳
-s 	记录每个线程的事件计数器，配合 perf report -T使用

微基准测试From youtube video： 
root@HaydenArchDesktop /tmp# perf bench sched pipe
# Running &#39;sched/pipe&#39; benchmark:
# Executed 1000000 pipe operations between two processes

     Total time: 2.407 [sec]

       2.407455 usecs/op
         415376 ops/sec


root@HaydenArchDesktop /tmp# taskset -c 0 perf bench sched pipe
# Running &#39;sched/pipe&#39; benchmark:
# Executed 1000000 pipe operations between two processes

     Total time: 2.381 [sec]

       2.381081 usecs/op
         419977 ops/sec
# 这里的时间提升不明显的原因是， 我的Archlinux是ZenKernel， 感觉可能在调度上已经做了不少的事情 ，如果随便启动一个redhat , 这个指标的差距会比较大。

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_Sysctl/" title="Sysctl 云平台参数的收集以及一部分解释">Sysctl 云平台参数的收集以及一部分解释</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.741Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">一些关于sysctl参数设置的收集和解释。
&#x2F;etc&#x2F;sysctl.d&#x2F;00-defaults.confkernel.printk输出内核日志信息的级别。
# 映射到的proc文件系统位置 - /proc/sys/kernel/printk
# Maximize console logging level for kernel printk messages
kernel.printk = 8 4 1 7

# (1) 控制台日志级别：优先级高于该值的消息将被打印至控制台。
# (2) 缺省的消息日志级别：将用该值来打印没有优先级的消息。
# (3) 最低的控制台日志级别：控制台日志级别可能被设置的最小值。
# (4) 缺省的控制台：控制台日志级别的缺省值。
内核中共提供了八种不同的日志级别，在 linux&#x2F;kernel.h 中有相应的宏对应。
#define KERN_EMERG  &quot;&lt;0&gt;&quot;   &#x2F;* systemis unusable *&#x2F;
#define KERN_ALERT  &quot;&lt;1&gt;&quot;   &#x2F;* actionmust be taken immediately *&#x2F;
#define KERN_CRIT    &quot;&lt;2&gt;&quot;   &#x2F;*critical conditions *&#x2F;
#define KERN_ERR     &quot;&lt;3&gt;&quot;   &#x2F;* errorconditions *&#x2F;
#define KERN_WARNING &quot;&lt;4&gt;&quot;   &#x2F;* warning conditions *&#x2F;
#define KERN_NOTICE  &quot;&lt;5&gt;&quot;   &#x2F;* normalbut significant *&#x2F;
#define KERN_INFO    &quot;&lt;6&gt;&quot;   &#x2F;*informational *&#x2F;
#define KERN_DEBUG   &quot;&lt;7&gt;&quot;   &#x2F;*debug-level messages *&#x2F;
kernel.panic设置内核的Panic之后自动重启
# Wait 30 seconds and then reboot
kernel.panic = 30
neigh.default.gc设置arp缓存相关的参数https://zhuanlan.zhihu.com/p/94413312
# Allow neighbor cache entries to expire even when the cache is not full
net.ipv4.neigh.default.gc_thresh1 = 0
net.ipv6.neigh.default.gc_thresh1 = 0

# Avoid neighbor table contention in large subnets
net.ipv4.neigh.default.gc_thresh2 = 15360
net.ipv6.neigh.default.gc_thresh2 = 15360
net.ipv4.neigh.default.gc_thresh3 = 16384
net.ipv6.neigh.default.gc_thresh3 = 16384

# gc_thresh1
存在于ARP高速缓存中的最少层数，如果少于这个数，
垃圾收集器将不会运行。
缺省值是128。
# gc_thresh2
保存在 ARP 高速缓存中的最多的记录软限制。
垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。
缺省值是 512。
# gc_thresh3
保存在 ARP 高速缓存中的最多记录的硬限制，
一旦高速缓存中的数目高于此，
垃圾收集器将马上运行。
缺省值是1024。

&#x2F;etc&#x2F;sysctl.d&#x2F;99-amazon.confsched_autogroup_enabled通过CFS分组提高了桌面环境的性能表现。这个小小的补丁仅为 Linux Kernel 增加了 233 行代码，却将高负荷下桌面响应最大延迟降低到原先的十分之一，平均延迟降低到六十分之一！该补丁的作用是为每个 TTY 动态地创建任务分组。(https://linuxtoy.org/archives/small-patch-but-huge-improvement.html)
# https://cateee.net/lkddb/web-lkddb/SCHED_AUTOGROUP.html
# https://www.postgresql.org/message-id/50E4AAB1.9040902@optionshouse.com
# This setting enables better interactivity for desktop workloads and not
# suitable for many server workloads.
# 启用后，内核会创建任务组来优化桌面程序的调度。它将把占用大量资源的应用程序放在它们自己的任务组，根据PostgreSQL的测试， 关闭这个选项会将数据库的性能提高30%（上面的Link）。 在后台的服务进程中是提高性能的选项。
# 0：禁止
# 1：开启

kernel.sched_autogroup_enabled=0

&#x2F;usr&#x2F;lib&#x2F;sysctl.d&#x2F;00-system.confBridge-nf-call-iptables网桥设备关闭netfilter模块，开关需要按需求来指定。关闭这个模块会在网桥2层可以转发的时候直接转发， 不会走三层进行数据传输，也就是说不会过Iptables。Kubernetes需要开启这个参数的原因是： https://imroc.cc/post/202105/why-enable-bridge-nf-call-iptables/， 修复了Coredns不定期解析失败的问题。
# Disable netfilter on bridges.
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0

&#x2F;usr&#x2F;lib&#x2F;sysctl.d&#x2F;10-default-yama-scope.confYamaYama is a Linux Security Module that collects system-wide DAC security protections that are not handled by the core kernel itself. This is selectable at build-time with CONFIG_SECURITY_YAMA, and can be controlled at run-time through sysctls in &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;yama  
https://www.kernel.org/doc/html/latest/admin-guide/LSM/Yama.html 
[root@ip-172-31-11-235 sysctl.d]$ cat 10-default-yama-scope.conf
# When yama is enabled in the kernel it might be used to filter any user
# space access which requires PTRACE_MODE_ATTACH like ptrace attach, access
# to /proc/PID/&#123;mem,personality,stack,syscall&#125;, and the syscalls
# process_vm_readv and process_vm_writev which are used for interprocess
# services, communication and introspection (like synchronisation, signaling,
# debugging, tracing and profiling) of processes.
#
# Usage of ptrace attach is restricted by normal user permissions. Normal
# unprivileged processes cannot interact through ptrace with processes
# that they cannot send signals to or processes that are running set-uid
# or set-gid.
#
# yama ptrace scope can be used to reduce these permissions even more.
# This should normally not be done because it will break various programs
# relying on the default ptrace security restrictions. But can be used
# if you don't have any other way to separate processes in their own
# domains. A different way to restrict ptrace is to set the selinux
# deny_ptrace boolean. Both mechanisms will break some programs relying
# on the ptrace system call and might force users to elevate their
# priviliges to root to do their work.
#
# For more information see Documentation/security/Yama.txt in the kernel
# sources. Which also describes the defaults when CONFIG_SECURITY_YAMA
# is enabled in a kernel build (currently 1 for ptrace_scope).
#
# This runtime kernel parameter can be set to the following options:
# (Note that setting this to anything except zero will break programs!)
#
# 0 - Default attach security permissions.
# 1 - Restricted attach. Only child processes plus normal permissions ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_btrfs/" title="关于BTRFS的一些测试">关于BTRFS的一些测试</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.743Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">创建BTRFS卷mkfs.btrfs -d single -m raid1 /dev/nvme1n1 /dev/nvme2n1 /dev/nvme3n1 

更改btrfs的元数据冗余  这里最好的办法当然是创建的时候就规划和指定好。    btrfs balance start -dconvert&#x3D;raid1 -mconvert&#x3D;raid1 &#x2F;mnt
创建轻量副本文件cp --reflink source dest 

记录测试结果：

如果是 -d raid0 -m raid1 可以直接将三个EBS IO1 3000IOPS的卷吃满， 直接到9000
如果是 -d raid1 -m raid1 只能达到3000IOPS， 但是容量会有冗余。

Randread[global]
directory=/mnt
ioengine=libaio
direct=1
rw=randread
bs=16M
size=64M
time_based
runtime=20
group_reporting
norandommap
numjobs=1
thread

[job1]
iodepth=2

Result[root@ip-172-31-10-64 fio]# fio ./job1
job1: (g=0): rw=randread, bs=16M-16M/16M-16M/16M-16M, ioengine=libaio, iodepth=2
fio-2.14
Starting 1 thread
job1: Laying out IO file(s) (1 file(s) / 64MB)
Jobs: 1 (f=1): [r(1)] [100.0% done] [256.0MB/0KB/0KB /s] [16/0/0 iops] [eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=26359: Thu Nov 18 07:59:17 2021
  read : io=5232.0MB, bw=266745KB/s, iops=16, runt= 20085msec
    slat (msec): min=1, max=70, avg=21.39, stdev=23.43
    clat (msec): min=2, max=130, avg=101.29, stdev=31.01
     lat (msec): min=8, max=139, avg=122.68, stdev=24.96
    clat percentiles (msec):
     |  1.00th=[    7],  5.00th=[   60], 10.00th=[   63], 20.00th=[   66],
     | 30.00th=[   93], 40.00th=[  110], 50.00th=[  118], 60.00th=[  120],
     | 70.00th=[  122], 80.00th=[  126], 90.00th=[  127], 95.00th=[  128],
     | 99.00th=[  130], 99.50th=[  131], 99.90th=[  131], 99.95th=[  131],
     | 99.99th=[  131]
    lat (msec) : 4=0.31%, 10=3.36%, 20=0.92%, 50=0.31%, 100=28.75%
    lat (msec) : 250=66.36%
  cpu          : usr=0.04%, sys=3.80%, ctx=994, majf=0, minf=8193
  IO depths    : 1=0.3%, 2=99.7%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=327/w=0/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=2

Run status group 0 (all jobs):
   READ: io=5232.0MB, aggrb=266744KB/s, minb=266744KB/s, maxb=266744KB/s, mint=20085msec, maxt=20085msec

Randwrite[global]
directory=/mnt
ioengine=libaio
direct=1
rw=randread
bs=16M
size=64M
time_based
runtime=20
group_reporting
norandommap
numjobs=1
thread

[job1]
iodepth=2
Result[root@ip-172-31-10-64 fio]# fio ./job1
job1: (g=0): rw=randwrite, bs=16M-16M/16M-16M/16M-16M, ioengine=libaio, iodepth=2
fio-2.14
Starting 1 thread
Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/256.0MB/0KB /s] [0/16/0 iops] [eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=26385: Thu Nov 18 08:00:56 2021
  write: io=5248.0MB, bw=267987KB/s, iops=16, runt= 20053msec
    slat (msec): min=1, max=67, avg=12.64, stdev=21.62
    clat (msec): min=12, max=141, avg=109.45, stdev=31.98
     lat (msec): min=18, max=142, avg=122.10, stdev=25.25
    clat percentiles (msec):
     |  1.00th=[   16],  5.00th=[   23], 10.00th=[   66], 20.00th=[   72],
     | 30.00th=[  120], 40.00th=[  124], 50.00th=[  126], 60.00th=[  127],
     | 70.00th=[  128], 80.00th=[  130], 90.00th=[  133], 95.00th=[  135],
     | 99.00th=[  139], 99.50th=[  139], 99.90th=[  141], 99.95th=[  141],
     | 99.99th=[  141]
    lat (msec) : 20=4.57%, 50=0.91%, 100=19.21%, 250=75.30%
  cpu          : usr=1.51%, sys=0.82%, ctx=720, majf=0, minf=1
  IO depths    : 1=0.3%, 2=99.7%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued    : total=r=0/w=328/d=0, short=r=0/w=0/d=0, drop=r=0/w=0/d=0
     latency   : target=0, window=0, percentile=100.00%, depth=2

Run status group 0 (all jobs):
  WRITE: io=5248.0MB, aggrb=267987KB/s, minb=267987KB/s, maxb=267987KB/s, mint=20053msec, maxt=20053msec

Xfs 单独磁盘Randwrite[root@ip-172-31-10-64 fio]# fio ./job1
job1: (g=0): rw=randwrite, bs=16M-16M/16M-16M/16M-16M, ioengine=libaio, iodepth=2
fio-2.14
Starting 1 thread
job1: Laying out IO file(s) (1 file(s) / 64MB)
Jobs: 1 (f=1): [w(1)] [100.0% done] [0KB/128.0MB/0KB /s] [0/8/0 iops] [eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=26541: Thu Nov 18 08:04:01 2021
  write: io=2704.0MB, bw=137688KB/s, iops=8, runt= 20110msec
    slat (msec): min=12, max=130, avg=118.63, stdev=24.61
    clat (msec): min=12, max=130, avg=118.91, stdev=23.73
     lat (msec): min=34, max=255, avg=237.54, stdev=47.53
    clat percentiles (msec):
     |  1.00th=[   14],  5.00th=[   32], 10.00th=[  125], 20.00th=[  125],
     | 30.00th=[  125], 40.00th=[  125], 50.00th=[  125], 60.00th=[  126],
     | 70.00th=[  126], 80.00th=[  126], 90.00th=[  126], 95.00th=[  126],
     | 99.00th=[  129], 99.50th=[  131], 99.90th=[  131], 99.95th=[  131],
     | 99.99th=[  131]
    lat (msec) : 20=1.78%, 50=3.55%, 100=0.59%, 250=94.08%
  cpu          : usr=0.74%, sys=0.49%, ctx=2132, majf=0, minf=1
  IO depths    : 1=0.6%, 2=99.4%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit  ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/19/Linux/Linux_fio_blktrace/" title="Fio 命令使用的说明">Fio 命令使用的说明</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-19T09:45:39.000Z" title="发表于 2022-04-19 17:45:39">2022-04-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.744Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Fio 一些测试和思考， fio ，blktrace等等。
Github地址  https://github.com/axboe/fio  https://tobert.github.io/post/2014-04-17-fio-output-explained.html  http://xiaqunfeng.cc/2017/07/12/fio-test-ceph/  https://fio.readthedocs.io/en/latest/fio_doc.html  http://linuxperf.com/?p=156
命令和说明# fio test file defination
# ini file format 
 ; -- start job file --
[global]
size=1G # 设置测试文件的大小
runtime=30 # 设定运行时间， 如果运行时间之内已经完成了文件大小的写入则会保持文件大小和负载继续写。
bs=4k # 块大小
numjobs=1 # 定义测试的进程数量 默认是1
direct=1 # 是否绕过操作系统的Buffer缓冲区， 1 Enable
ioengine=libaio # io引擎使用libaio模式, 查看可用IO引擎的命令 
group_reporting # 汇总信息显示
thread # 使用单进程多线程的模型， 默认是使用多进程模型。
time_based # 用运行时间为基准， 如果时间没有到达指定的值就继续执行相同的操作， 直接到时间满足要求。

[job1]
rw=write

[job2]
rw=read

[job3]
rw=randwrite

[job4]
rw=randread

[job5]
rw=randrw
; -- end job file --

关于硬盘性能 Iostat首先， 在man iostat的时候已经明确的提示了，iostat 的 svctm （也就是硬盘的servicetime）是一个不准确的值， 会在后续的版本中移除， 因为他所依赖的数据来源（&#x2F;proc&#x2F;diskstats）中，是从Block Level 出发来进行计算的， 所以svctm其实并不是IO控制器所需要的准确时间， 那么出现了两个问题， 我们能观察的数据是那个？ 以及背后的含义是什么?  
调度算法 和 读写请求的合并内核会对IO的请求进行合并， 但是这个合并不是一直存在的， 反映到IOstat的指标是 rrqm&#x2F;s &amp; wrqm&#x2F;s,  这两个表示对于硬盘的读写请求中， 每秒合并的请求数量； 如果你去查看IO的调度算法， none 算法是完全不合并的， 所以这两列一直都是0.  
队列的等待时间iostat中的await一列， 表示请求的等待时间，正常的情况下应该比较低，那么究竟什么情况下才是性能不佳的表现？await 每个I&#x2F;O的平均耗时是用await表示(blktrace command results: Q2C – 整个IO请求所消耗的时间(Q2I + I2D + D2C &#x3D; Q2C)，相当于iostat的await。)，包括了 IO请求在Kernel中的等待时间 + IO请求从内核出发处理完成回到内核的时间，也就是IO time + Service Time。await 一般情况下应该小于10ms ， 如果没有或者比较大的情况下 ， 应该考虑负载的类型， 来衡量硬盘的负载水平。  
Blktrace一个I&#x2F;O请求进入block layer之后，可能会经历下面的过程：
Q Remap: 可能被DM(Device Mapper)或MD(Multiple Device, Software RAID) remap到其它设备
G Split: 可能会因为I/O请求与扇区边界未对齐、或者size太大而被分拆(split)成多个物理I/O
I Merge: 可能会因为与其它I/O请求的物理位置相邻而合并(merge)成一个I/O
D 被IO Scheduler依照调度策略发送给driver
C 被driver提交给硬件，经过HBA、电缆（光纤、网线等）、交换机（SAN或网络）、最后到达存储设备，设备完成IO请求之后再把结果发回。

常见的状态切换 Q–G–I–D–C 
259,2    0        2     0.000001080  7134  Q   W 756856 + 800 [dd]
259,2    0        3     0.000004247  7134  X   W 756856 / 757368 [dd]
259,2    0        4     0.000005806  7134  G   W 756856 + 512 [dd]
259,2    0        5     0.000008792  7134  I   W 756856 + 512 [dd]
259,2    0        6     0.000012844  7134  G   W 757368 + 288 [dd]
259,2    0        7     0.000013202  7134  I   W 757368 + 288 [dd]
259,2    0        8     0.000031708  1830  D   W 756856 + 512 [kworker/0:1H]
259,2    0        9     0.000034250  1830  D   W 757368 + 288 [kworker/0:1H]
259,2    0       10     0.001598439  7135  C   W 756856 + 512 [0]
259,2    0       11     0.001689880  7135  C   W 757368 + 288 [0]
# 主，从设备号 ， 起始Sector 为0 ， 写几个， 时间 ， pid ， 状态 ， R/W ， 未知

保留blktrace的结果为bin文件
blktrace -d /dev/sdb

使用blkparse 分析已经有的记录
# 记录性能数据 到 文件。
root@ip-172-31-11-235:/home/ec2-user|⇒  blktrace -d /dev/nvme0n1p1
^C=== nvme0n1p1 ===
  CPU  0:                    1 events,        1 KiB data
  CPU  1:                    0 events,        0 KiB data
  Total:                     1 events (dropped 0),        1 KiB data
# 每个cpu设备每个设备存储一个单独的文件。
root@ip-172-31-11-235:/home/ec2-user|⇒  ll
total 4.0K
-rw-r--r-- 1 root root 56 Nov  4 17:55 nvme0n1p1.blktrace.0
-rw-r--r-- 1 root root  0 Nov  4 17:55 nvme0n1p1.blktrace.1

root@ip-172-31-11-235:/home/ec2-user|⇒  blkparse -i nvme0n1p1.blktrace.0
# 格式化分析的数据为bin。
root@ip-172-31-11-235:/home/ec2-user|⇒  blkparse -i nvme0n1p1 -d nvme0n1p1.blktrace.bin
# 使用一个简易的图形方式分析结果。
root@ip-172-31-11-235:/home/ec2-user|⇒  btt -i nvme0n1p1.blktrace.bin

历史背景
mdadm 已经不怎么更新和开发了默认推荐使用LVMLVM 和 mdadm 在操作系统都是使用的Raid驱动（内核模块）。
其实也是可以使用btrfs ， 这个测试的结果是 btrfs 的实现和管理成本比 LVM 要少的多。 

相关的问题如果说有一个性能的问题， IOPS达不到指定的数值， 思路？首先查看队列深度是不是足够，看svctm时间长不长，看队列长度 
&#x2F;sys&#x2F;block&#x2F;sda&#x2F;nr_requests 
Iostat命令的理解iostat -xkt 1rrqm&#x2F;s wrqm&#x2F;s  - 读写请求的合并数量r&#x2F;s w&#x2F;s - 读写请求数量avgrq - 队列长度await - 时延util - 时间度量 ， 时间周期之内进行IO操作所占的比例。例如 1 秒的时间之内， 取样的点中有多少是在执行IO操作。
一个例子 ， 如果采样的周期为1s， 那么采样的范围之内 ， 前面的0.5秒有执行IO的操作， 后面的0.5秒没有执行任何的操作， 那么 最后 Util 现在的结果就是50% 。avgrq也是一直平均值， 在采样周期之内如果前后的状况不一致 也会进行平均。
一般情况下这个参数是准确的，但是大部分指标都是取决于监控取样周期的。
操作系统默认输出的块大小是 ： 256 ， 参数可见 ：╰─# cat &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;max_sectors_kb256
max_segments表示设备能够允许的最大段的数目。    – 这应该是一个内存或者buffer的分段指标。 （待定）max_sectors_kb表示设备允许的最大请求大小。      – 可改 。max_hw_sectors_kb表示单个请求所能处理的最大KB（硬约束） – 这个是上一个参数的Limit。  
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2022/04/15/Linux/Linux_Shell-Script-Process-Space/" title="Shell脚本处理目录或者文件名中的空格">Shell脚本处理目录或者文件名中的空格</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-04-15T06:10:54.000Z" title="发表于 2022-04-15 14:10:54">2022-04-15</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.741Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">问题：问题是： 罗列指定的目录下面的文件， 符合要求的文件保留， 未匹配的删除。
解决：简单的Bash脚本， 使用ls拿文件名， 使用For + IF判断即可。但是文件的目录中有的文件名是带有空格的， 而Shell 使用空格做分隔符，因此无法正确的处理完整的文件名。解决方案有特别多， 只记录一个我最后使用的方案：使用IFS变量来定义Shell 的默认分隔符， 将空格替换成\n\b. 
#!/bin/bash
#
SAVEIFS=$IFS
IFS=$(echo -en "\n\b")

for number in &#123;1..10&#125;
do
  mkdir -pv "dir $number"
done

tree

FILE=`ls`

for i in $FILE;
do
  echo "The DirName: $i"
done

IFS=$SAVEIFS


这样就可以拿到名称为dir 1的完整目录名字了，否则会默认吧空格分割的文件名称作为两个目录提取。
⚡ ./delete.sh
.
├── delete.sh
├── dir 1
├── dir 10
├── dir 2
├── dir 3
├── dir 4
├── dir 5
├── dir 6
├── dir 7
├── dir 8
└── dir 9

10 directories, 1 file
The DirName: delete.sh
The DirName: dir 1
The DirName: dir 10
The DirName: dir 2
The DirName: dir 3
The DirName: dir 4
The DirName: dir 5
The DirName: dir 6
The DirName: dir 7
The DirName: dir 8
The DirName: dir 9

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/12/26/Linux/Linux_PerfCommand/" title="Linux中一些常见的性能分析命令">Linux中一些常见的性能分析命令</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-12-25T16:18:36.000Z" title="发表于 2021-12-26 00:18:36">2021-12-26</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.739Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">记录性能分析的思路。
最近的这个半年越来越好奇的事情是， 为什么命令会卡住，为什么命令会执行不下去，为什么命令会等待，等等等等。
那么这些问题， 有的是可以有答案的， 目前也不知道的。 
已经大概掌握的几个不同的方法以及观测的工具， 大概做一个记录。
Strace命令Strace 命令的常见用法strace命令是用来追踪系统调用的，常见的可以追踪的系统调用需要阅读内核部分的代码。 但是常见的系统调用就是集中， read() , write() , ioctl(), futex() , mmap()大部分的时候 我们都是可以观测到卡住的部分的 ， 这种追踪我认为常见的使用场景就是命令卡住了， 或者执行中的程序卡住了。 
命令卡住的分析对于命令卡住的情况， 可以使用类似于如下的命令： 
strace -f -ttt -s 512 echo "123"
这样的话， 在执行的过程中就可以查看相关的内容，比如常见的卡在了系统调用的某个函数上， 这个可以用来定位，命令打开了那些文件，申请的那些内存地址，打开了什么文件，关闭Socket等等等等。目前我的办法的通过对比这个卡住的命令执行到什么函数出现的问题， 对应的在正常的机器上进行对比，就可以猜到大概的问题出现在了哪里。
已经运行中的程序分析strace -f -ttt -s 512 -p 123
执行进程的PID ， 然后strace会attach到进程上， 输出的内容， 也可以查看到当前程序的运行状态。 
总结如上面的两种方式，都可以对运行中明显的问题进行观察， 但是如果没有卡在系统调用的部分， 通过这个命令的观察其实是无法查看的， 因为他记录的是应用程序指令陷入到内核态的部分， 但是常见的应用程序基本上都是用户态的，所以这个部分如果是应用卡在用户态上， 观测的信息就比较有限了。
Perf命令Perf简单的分析perf命令的简单分析， 首先是

perf topperf top 可以用来实时的查看应用程序的相关问题， 收集指标的范围是整个操作系统，所以是比较消耗资源的， 输出的结果也是直接可以查看的， 看完了结果打断即可。 

perf statperf stat 查看相关的统计信息，如下是一个样例，提供了一些静态的指标。 
sudo perf stat
Performance counter stats for 'system wide':
        61,167.25 msec cpu-clock                 #   16.000 CPUs utilized
            4,955      context-switches          #   81.007 /sec
               63      cpu-migrations            #    1.030 /sec
              930      page-faults               #   15.204 /sec
    6,266,524,215      cycles                    #    0.102 GHz                      (83.32%)
      244,046,608      stalled-cycles-frontend   #    3.89% frontend cycles idle     (83.34%)
       66,809,179      stalled-cycles-backend    #    1.07% backend cycles idle      (83.34%)
      818,170,826      instructions              #    0.13  insn per cycle
                                                 #    0.30  stalled cycles per insn  (83.34%)
      155,602,840      branches                  #    2.544 M/sec                    (83.34%)
        1,701,804      branch-misses             #    1.09% of all branches          (83.33%)

      3.823037998 seconds time elapsed
显示的内容是从输入了命令之后的相关信息，主要是一些CPU相关的指标， 比如CPU时钟，上下文交换次数，cpu转移，缺页中断等等等等。

perf recordperf record 我常用的命令是这样的， 他会将记录到的指标输出到当前目录的文件中，然后供report命令来进行分析， 这两个一般来说会合用。 
perf record -a -g -F 1000 -p 123  
perf record -a -g -F 1000 echo 123
perf record -a -g -F 1000 
perf record -a -g -F 1000 -- sleep 60 
三个命令会记录相关的指标到当前目录的perf.data文件中。 大小和采样的频率，时间的数量有关。
perf reportperf report 我比较常用的参数就是 使用 
perf record --stdio
来直接进行查看， 占用时间百分比比较高的函数，前提是 ，这个命令的运行需要有perf.data.

perf schedperf sched 通常是用来查看cpu调度延时的， 这个用的确实不多， 毕竟cpu调度现在基本上都是cfq， 改的人毕竟还是少数， 所以实际的使用比较少。这个指令常用的如下： 
perf sched record 
perf sched latency 
perf sched report
上面的这些都是我比较常用的命令， 临时抓出来看下。


Perf命令输出火焰图perf 命令输出火焰图需要的是Github上面的一个项目， 这个项目的作者也是写性能之巅的作者。
具体的处理流程如下： 
git clone --depth 1 https://github.com/brendangregg/FlameGraph.git
sudo perf script > out.perf
FlameGraph/stackcollapse-perf.pl out.perf > out.folded
FlameGraph/flamegraph.pl out.folded > out.svg
最后输出的out.svg就是结果了，可以通过浏览器来查看。至于查看的方法，其实是看函数所占有的面积， 面积越大说明函数运行的时间越长；那么还有说法是说， 越靠近顶端的应该越尖，如果有顶端比较大的平顶说明可能是有问题的， 这个答案还在求证中。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/10/02/Linux/Linux_Kubernetes-day2/" title="Kubernetes day2">Kubernetes day2</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-10-02T02:49:55.000Z" title="发表于 2021-10-02 10:49:55">2021-10-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.734Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">对于etcd的操作和备份 
Etcd的操作 - EtcdctlEtcd的规划
最好用 固态盘， Pod数量比较多的情况下会非常非常慢， 内存要大.
类似于redis 或者 Zookeeper， KV的存储.
支持watch机制，可以通知给node节点的数据变化.
etcd consul zookeeper 的区别




名称
优点
缺点
接口
一致性算法



zookeeper
1.功能强大，不仅仅只是服务发现2.提供watcher机制能实时获取服务提供者的状态3.dubbo等框架支持
1.没有健康检查2.需在服务中集成sdk，复杂度高3.不支持多数据中心
sdk
Paxos


consul
1.简单易用，不需要集成sdk2.自带健康检查3.支持多数据中心4.提供web管理界面
1.不能实时获取服务信息的变化通知
http&#x2F;dns
Raft


etcd
1.简单易用，不需要集成sdk2.可配置性强
1.没有健康检查2.需配合第三方工具一起完成服务发现3.不支持多数据中心
http
Raft









Etcdctl 的命令
查看etcd的成员清单
]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --write-out=table --endpoints="192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379" member list

查看etcd的节点状态
]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints="192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379" endpoint status -w table

查看etcd存储的数据 
]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints="192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379" get /registry/ --prefix --keys-only | head
查看etcd中的pod信息


]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints="192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379" get /registry/ --prefix --keys-only | grep pod

其他的操作


get &#x2F; put &#x2F; del 等基础操作

Watch机制watch机制是通过不断的查看数据，发生变化就主动的通知客户端，v3支持watch固定的key,也可以watch一个范围的数据。
# watch 一个pod的信息， 然后手动delete这个pod ， 查看etcd 的watch行为和输出的结果。
]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints="192.168.31.21:2379,192.168.31.22:2379,192.168.31.23:2379" watch /registry/pods/monitoring/node-exporter-889hf

数据备份恢复和WAL日志WAL： watch ahead log - 预写日志， 可以通过预写日志来进行数据库的恢复。WAL记录了整个数据变化的过程，在操作写入数据之前先进行wal日志的写入。
etcd v2 的时候直接复制和备份目录，备份文件的方案etcd v3 的备份和恢复， 使用快照的方式。
备份使用的命令和恢复的命令不太一样， etcdctl  vs   etcdutl 
可以写脚本来进行数据进行备份和恢复。
]$ etcdctl snapshot save
]$ etcdctl snapshot restore
]$ etcdctl snapshot status

]$ etcdctl --cacert=/etc/kubernetes/ssl/ca.pem --cert=/etc/kubernetes/ssl/kubernetes.pem --key=/etc/kubernetes/ssl/kubernetes-key.pem --endpoints="192.168.31.21:2379" snapshot save snap-20211002.db

# 测试尝试恢复到临时的目录，测试用。 目录地址用的是tmp下面的。
]$ etcdutl snapshot restore ./snap-20211002.db --data-dir /tmp/etcd-restore
2021-10-02T12:01:13+08:00	info	snapshot/v3_snapshot.go:251	restoring snapshot	&#123;"path": "./snap-20211002.db", "wal-dir": "/tmp/etcd-restore/member/wal", "data-dir": "/tmp/etcd-restore", "snap-dir": "/tmp/etcd-restore/member/snap", "stack": "go.etcd.io/etcd/etcdutl/v3/snapshot.(*v3Manager).Restore\n\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/snapshot/v3_snapshot.go:257\ngo.etcd.io/etcd/etcdutl/v3/etcdutl.SnapshotRestoreCommandFunc\n\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/etcdutl/snapshot_command.go:147\ngo.etcd.io/etcd/etcdutl/v3/etcdutl.snapshotRestoreCommandFunc\n\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/etcdutl/snapshot_command.go:117\ngithub.com/spf13/cobra.(*Command).execute\n\t/home/remote/sbatsche/.gvm/pkgsets/go1.16.3/global/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:856\ngithub.com/spf13/cobra.(*Command).ExecuteC\n\t/home/remote/sbatsche/.gvm/pkgsets/go1.16.3/global/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:960\ngithub.com/spf13/cobra.(*Command).Execute\n\t/home/remote/sbatsche/.gvm/pkgsets/go1.16.3/global/pkg/mod/github.com/spf13/cobra@v1.1.3/command.go:897\nmain.Start\n\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/ctl.go:50\nmain.main\n\t/tmp/etcd-release-3.5.0/etcd/release/etcd/etcdutl/main.go:23\nruntime.main\n\t/home/remote/sbatsche/.gvm/gos/go1.16.3/src/runtime/proc.go:225"&#125;
2021-10-02T12:01:13+08:00	info	membership/store.go:119	Trimming membership information from the backend...
2021-10-02T12:01:13+08:00	info	membership/cluster.go:393	added member	&#123;"cluster-id": "cdf818194e3a8c32", "local-member-id": "0", "added-peer-id": "8e9e05c52164694d", "added-peer-peer-urls": ["http://localhost:2380"]&#125;
2021-10-02T12:01:13+08:00	info	snapshot/v3_snapshot.go:272	restored snapshot	&#123;"path": "./snap-20211002.db", "wal-dir": "/tmp/etcd-restore/member/wal", "data-dir": "/tmp/etcd-restore", "snap-dir": "/tmp/etcd-restore/member/snap"&#125;
]$ ls /tmp/etcd-restore/
member

数据恢复的流程：

创建新的etcd集群
停止kubernetes以及其他的依赖etcd 的服务。 
停止空白的新的集群
使用备份的文件进行集群的恢复
使用在集群的每个节点恢复相同的备份文件
每个节点启动etcd的集群并且进行验证
启动Kubernetes的相关集群和组件。
查看恢复的结果，验证各个组件的相关服务是否已经正常恢复。

Etcd节点的维护
etcdctl add-etcd  
etcdctl del-etcd

资源清单以及API相关的外部服务接口
Container Runtime Interface - CRI


runc
RKT


Container Storage Interface - CSI
Container Network Interface - CNI

Node的相关操作
cordon
uncordon
drain
taint

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/09/27/Linux/Linux_HarborInstallation/" title="Harbor Http 安装部署">Harbor Http 安装部署</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-27T03:21:47.000Z" title="发表于 2021-09-27 11:21:47">2021-09-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.732Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content"> Harbor的部署记录。
Harbor Info
Harbor 项目地址

Harbor HTTP部署因为是临时使用， 所以直接给了HTTP的权限， 为的是不走公网部署 CEPH Cluster， CEPH在BootStrap之后会默认去公网的镜像仓库尝试Pull镜像并且校验镜像和服务，所以给一个私有的仓库， 直接去找私有仓库就免了公网访问卡集群的正常启动的步骤。
下载解压~]$ wget https://github.com/goharbor/harbor/releases/download/v2.3.2/harbor-offline-installer-v2.3.2.tgz
~]$ mv harbor-offline-installer-v2.3.2.tgz /opt 
~]$ tar zxvf /opt/harbor-offline-installer-v2.3.2.tgz

复制修改配置文件~]$ cp /opt/harbor/harbor.yml.tmpl /opt/harbor/harbor.yml

配置默认的存储位置# 注释掉https的部分，如果需要https的话签发一个证书写路径在配置文件中
# 修改默认的存储位置
data_volume: /opt/harbor/image_store
指定Harbor对外提供服务的域名# 修改Harbor的域名或者主机名(需要对应的解析)，也可以直接使用IP地址
# The IP address or hostname to access admin UI and registry service.
# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.
hostname: harbor.local
设置harbor Admin的密码可以登录Dashboard 或者 Pull镜像
# Remember Change the admin password from UI after launching Harbor.
harbor_admin_password: Harbor12345

创建harbor存储镜像的目录# 创建Harbor的存储目录， 可以远程指定到Cephfs上面
~]$ mkdir /opt/harbor/image_store

配置Docker-ce 清华的镜像源这个配置是给Centos &#x2F; RHEL来使用的，来自清华的Repo Help
# 添加repo文件，和修改配置到Tsinghua repo
~]$ wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/centos/docker-ce.repo
~]$ sudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo

# 安装Docker-ce
~]$ sudo yum install docker-ce

# CENTOS8 STREAM 特殊的配置，需要卸载 podman 和 Buildah
~]$ dnf install -y docker-ce --allowerasing

# 开机启动
~]$ sudo systemctl restart docker &amp;&amp; sudo systemctl enable docker

# 安装Docker-compose ，因为CentOS8 默认是没有Docker-compose的 ， 按照官网的流程走就可以。
~]$ sudo curl -L "https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)" -o /usr/local/bin/docker-compose
~]$ sudo chmod +x /usr/local/bin/docker-compose
~]$ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose
安装harbor~]$ cd /opt/harbor
/opt/harbor]$ ./install.sh

启动Harbor直接使用Docker-compose启动即可，如果需要的话可以在配置文件中指定镜像扫描器，来进行镜像的漏洞扫描。
# 进入Harbor运行所在的目录
~]$ cd /opt/harbor	

# 使用Docker-compose的启动命令，适用于服务停止 或者 Docker 重启的时候， 容器没有正常运行。
~]$ docker-compose up -d -f /opt/harbor/docker-compose.yaml

# 查看容器的启动状态
/opt/harbor]$ docker-compose ps
/opt/harbor]$ watch -n 1 docker-compose ps

# 通过Docker-compose停止harbor
/opt/harbor]$ docker-compose down

验证
打开浏览器，访问harbor的地址 默认的端口80 
在所有docker节点上配置不安全的私有仓库，docker login [HARBORIP:PORT]
在所有podman节点上配置不安全的私有仓库，podman login [HARBORIP:PORT]
提示Login Successed， 登录成功，可以正常pull镜像了

Cephadm Bootstrap编辑cephadm文件，修改如下的镜像名称，和仓库的前缀这里其实还是需要再测试的， 按照这个脚本的逻辑， 应该会把所有的镜像都从指定的仓库Pull下来，但是我执行的时候只有ceph&#x2F;ceph:v16一个镜像下来了， 感觉还是有点儿问题的。
DEFAULT_IMAGE = 'harbor.local/ceph/ceph:v16'
DEFAULT_IMAGE_IS_MASTER = False
DEFAULT_IMAGE_RELEASE = 'pacific'
DEFAULT_PROMETHEUS_IMAGE = 'harbor.local/ceph/prometheus:v2.18.1'
DEFAULT_NODE_EXPORTER_IMAGE = 'harbor.local/ceph/node-exporter:v0.18.1'
DEFAULT_ALERT_MANAGER_IMAGE = 'harbor.local/ceph/alertmanager:v0.20.0'
DEFAULT_GRAFANA_IMAGE = 'harbor.local/ceph/ceph-grafana:6.7.4'
DEFAULT_HAPROXY_IMAGE = 'harbor.local/ceph/haproxy:2.3'
DEFAULT_KEEPALIVED_IMAGE = 'harbor.local/ceph/keepalived'
DEFAULT_REGISTRY = 'harbor.local'   # normalize unqualified digests to this

Cephadm 使用私有仓库bootstrap~]$ cephadm bootstrap --mon-ip 192.168.1.211 --allow-overwrite \
  --registry-url harbor.local \
  --registry-username admin \
  --registry-password Harbor12345

查看并且清除ceph-bootstrap的历史记录~]$ ls /etc/systemd/system/ceph*
~]$ ls /usr/lib/systemd/system/ceph*

~]$ rm -rf /etc/systemd/system/ceph*
~]$ rm -rf /usr/lib/systemd/system/ceph*
~]$ docker stop `docker ps -a -q`  

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/09/11/Linux/Linux_Kubernetes-day1/" title="Kubernetes day1">Kubernetes day1</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-11T02:41:37.000Z" title="发表于 2021-09-11 10:41:37">2021-09-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.734Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">云原生的定义
云原生一些概念十二因素应用
基准代码： 一份基准代码，多次部署（用同一个代码库进行版本控制）
依赖： 显式的声明和隔离相互之间的依赖
配置： 在环境中存储配置(配置中心，携程的apollo)
后端服务： 后端服务作为一个附加的资源，数据库，中间件等等
构建，发布，运行： 对程序执行构建或者打包，严格分离构建和运行,打镜像编译等等，构建和运行严格的分离
进程： 使用一个或者多个无状态进程运行应用
端口绑定： 通过端口绑定提供服务
并发： 通过进程模型进行扩展
易处理： 快速的启动，优雅的终止，最大程度的保持健壮性。
开发环境与线上环境等价： 尽可能保持Dev，Prelive，Live环境的一致性
日志： 将所有运行中的进程和后台服务的输出流按照的时间顺序统一收集存储和展示(ELK,Fluend,Logstash,Filebeat等等)
管理进程： 一次性管理类型的进程(Onetime Job)应该和正常的常驻进程使用同样的运行环境

Master - APIserverAPIserver提供了Kubernetes各类资源对象的CRUD以及Watch等等的HTTP REST接口，对象包括Pods，Services, ReplicationsControllers等等, 为RESTful操作提供服务，并且为集群状态提供前端，所有的组件都通过前端进行交互。

特点： RESTful风格

APIserver的请求最后会同步到Etcd。
请求APIserver在权限的鉴定完成之后， 可以查看大部分的信息。
端口默认是 6443 可通过启动参数（–secure-port）来进行调整
绑定的IP地址可以通过 –bind-address 在启动的时候指定
端口用于接受客户端，dashboard的外部HTTPBase的请求
Token 以及证书的HTTPbase的校验
基于指定策略的授权


功能： 

身份认证
验证权限
验证指令
执行操作
返回结果



Master - Kube-scheduler负责将Pod指定到节点上。

取出Pod需要分配的信息， 先排除不可调度的Node，在可用的Node列表之中选择一个合适的Node，将信息写入etcd。等待Node上面的kubelet进行生成Pod。
Node的Kubelet通过APIserver监听到Pod的相关信息，然后获取Pod的清单，下载镜像，启动Pod， Kubelet每秒Watch-APIserver的信息。
三个默认策略： 
LeastRequestedPriority - CPU+MEM直接评分，选择资源目前较低
CalculateNodeLanelPriority - 先匹配标签，在进行评分
BalancedResourceAllocation - 优先分配各项资源的使用率最均衡的节点


队列： PodQueue ， NodeList

Master - Kuber-Controller-Manager提供不同的控制器，例如： 集群内的Node, Pod ReplicaCounts, EndPoint, Namespace, ServiceAccount, ResourceQuota等等资源的控制内容，如果发现异常的时候提供自动化的修复流程。确保所有的服务是在预期的状态下运行。

5s检查一次Node的状态
包括多种不同的控制器
检查所有的控制器 和 Node是否符合预期
如果Node 不可达之后 40s会将节点标记为无法访问
标记为无法访问5min之后， 将删除这个节点，同时在其他的节点重建需要的Pod。


Pod的高可用机制：    1.  NodeMonitorPeriod: 监视周期    2.  NodeMonitorGracePerios: 节点监视观察期    3.  PodEvictionTimeout: 超时驱逐的区间
Kube-ProxyKubernetes网络代理，反映了Node上面的Kubernetes中的服务对象的变化，通过管理IPVS或者IPTABLES的规则来进行网络层的实现。

可以通过配置文件来指定IPVS的调度算法， 一般默认是RR, ipvsadm -ln
直接指定kubelet的配置文件选项ipvs:
  scheduler: sh

Kubelet
汇报节点的状态
监听API server 上面的pod信息变化，并调用Docker 和 Containerd 创建容器
准备Pod所需要的数据卷
返回Pod的运行状态
在Node节点上进行容器的健康检查

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/09/04/Linux/Linux_Docker-again/" title="Docker基础知识二周目">Docker基础知识二周目</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-04T15:43:42.000Z" title="发表于 2021-09-04 23:43:42">2021-09-04</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.728Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Docker/">Docker</a></span></div><div class="content">Docker Container 基础 。 again ….. 
Docker使用的六个名称空间
MNT Namespace - 提供文件的挂载和文件系统的隔离
IPC Namespace - 提供进程间通信
UTS Namespace - 提供主机名隔离
PID Namespace - 提供进程以及进程号隔离
Net Namespace - 提供网络隔离
User Namespace - 提供用户以及用户id的隔离

容器的进程ID关系Host -------------------------------- | container ------------------------|
systemd -- containerd -- container-shim -- | nginx-master -- nginx-worker
               `      -- container-shim -- | nginx-master -- nginx-worker
               `      -- container-shim -- | mysql-master -- mysql-worker
           dockerd -- docker-proxy -- | docker-proxy
               `   -- docker-proxy -- | docker-proxy
               `   -- docker-proxy -- | docker-proxy

containerd负责控制容器的进程，主机，用户，挂载的隔离。dockerd 启动的docker-proxy用来管理iptables进行网络的隔离。docker.service 里面指定了调用Containerd的socket生成容器的过程钟，API调用过程使用的是grpc。无论dockerd还是containerd最终使用的都是runc（container runtime） ，一个基于Go语言的容器创建工具， 根据OCI标准来创建容器。
docker-daemon 限制可以限制生成日志的大小

限制日志的大小使用的是 docker的daemon config， 针对的日志的范围，是docker Container 里面的标准输入输出的日志， 这部分日志的存储是在机器的硬盘上， 位置是（&#x2F;opt&#x2F;docker-storage&#x2F;containers ， 也就是&#x2F;var&#x2F;lib&#x2F;docker&#x2F;containers&#x2F;[CONTAINER_ID]&#x2F;*.json），

 cpu 大约可以分配 1:4  ， mem不建议超过物理内存的分配
压测镜像： lorel&#x2F;docker-stress-ng 
Docker的网络结构使用的是宿主机的docker0桥，同时在Host上面生成虚拟网卡veth多个接口 ， 在容器中生成eth0的虚拟网卡对， 虚拟机中的两个容器之间使用的是物理地址进行寻址访问，所以这个部分可以通过arp命令来进行验证。
Docker容器的资源限制 - Cgroup可以限制 CPU，MEM，DISKIO，NETIO, PRI，PAUSE&#x2F;RESUME。 对于Kubernetes可以对集群的Namespace来进行限制资源的规划。
查看内核可以支持的cgroup Flag. 
┌─[hayden@HaydenArchDesktop] - [~] - [Sun Sep 05, 00:14]
└─[$] &lt;> zcat /proc/config.gz | grep CGROUP
CONFIG_CGROUPS=y
CONFIG_BLK_CGROUP=y
CONFIG_CGROUP_WRITEBACK=y 
CONFIG_CGROUP_SCHED=y
CONFIG_CGROUP_PIDS=y
CONFIG_CGROUP_RDMA=y
CONFIG_CGROUP_FREEZER=y
CONFIG_CGROUP_HUGETLB=y
CONFIG_CGROUP_DEVICE=y
CONFIG_CGROUP_CPUACCT=y
CONFIG_CGROUP_PERF=y
CONFIG_CGROUP_BPF=y
CONFIG_CGROUP_MISC=y
# CONFIG_CGROUP_DEBUG is not set
CONFIG_SOCK_CGROUP_DATA=y
CONFIG_BLK_CGROUP_RWSTAT=y
CONFIG_BLK_CGROUP_IOLATENCY=y
CONFIG_BLK_CGROUP_IOCOST=y
# CONFIG_BFQ_CGROUP_DEBUG is not set
CONFIG_NETFILTER_XT_MATCH_CGROUP=m
CONFIG_NET_CLS_CGROUP=m
CONFIG_CGROUP_NET_PRIO=y
CONFIG_CGROUP_NET_CLASSID=y

┌─[hayden@HaydenArchDesktop] - [~] - [Sun Sep 05, 00:14]
└─[$] &lt;> zcat /proc/config.gz | grep MEM | grep CG
CONFIG_MEMCG=y
CONFIG_MEMCG_SWAP=y
CONFIG_MEMCG_KMEM=y
CPUKubernetes 限制是1&#x2F;1000Core ，Docker是1&#x2F;10Core, 只需要通过 –cpus 选项指定容器可以使用的 CPU 个数就可以了，并且还可以指定如 1.5 之类的小数。
CPU OverCommit 1:8
MEMMEM的限制单位是M； 同时可以限制Swap的使用，但是限制Swap的使用需要内核的支持，大部分时候还是会关闭掉SWAP， 虽然避免了OOM，但是会影响服务的质量。
MEM 不建议OverCommit

一般情况下 ： 分配0.5&#x2F;1个CPU，mem 2G&#x2F;4G； 根据业务的不同， 一般会是高可用的部署，两个一起对外提供服务；所以会启动两个容器，不需要提供太高的硬件，符合要求即可。

关于OOM的问题和可以调整的参数(&#x2F;proc&#x2F;[PID]&#x2F;)：

oom_adj 取值范围 -17 to +15 ， 为了兼容旧程序保留的方式
oom_score 一般是自动计算出来的结果，综合计算的结果, 参考 ： CPU时间，存活时间，oom_adj计算之后的结果。
oom_score_adj OOM分数的偏移量，-1000 to +1000, 可以设置-1000表示永远不会被Kill

DISKIODisk OverCommit : 1:1.2
PAUSE\RESUME</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/09/01/Linux/Linux_Ceph04/" title="Ceph Cluster 04 - CRUSH算法">Ceph Cluster 04 - CRUSH算法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-01T14:29:26.000Z" title="发表于 2021-09-01 22:29:26">2021-09-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.725Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">ceph笔记04
修改CRUSH算法的分配方式基础概念：五种运行图： MON服务维护
Monitor Map - 监控的运行图 MON的状态
OSD Map - OSD的状态， 每隔六秒钟汇报一次状态
PG Map - PG的运行图， PG的状态和映射关系
CRUSH Map - 一致性hash算法，和数据块和osd的分配关系， 动态更新，当客户端请求一个文件的时候，会通过CRUSH算法会根据osd的map创建PG组合的来对文件的存储进行负载和分配。假如有20个osd，创建32重组合分配对应的pg和osd的对应关系，选主从，选分布方式和节点的同步关系，叫做CRUSH Map。
MDS Map - Metadata Map ，元数据和数据文件的映射关系。

5种算法对节点的选择方式
Uniform
List 
Tree
Straw - 早期的版本，分布不是特别的均衡，抽签算法
Straw2 - 目前已经发展中的版本， 抽签算法 （Default）

对PG的动态调整默认情况下是动态调整的，但是可以手动调整为给予权重，设置PG分配的倾向，例如1T权重是1，等等等等
查看状态以及调整方式ceph osd df需要关注的值有两个：

weight - 根据磁盘的空间进行的调整，默认自动计算， 可调。
reweight - 磁盘的所有权重相加之后， 单个osd所占用的比例，由于默认的分配是相对概率的平衡，所以分配可能还是会有一些不均衡，通过这个可以进行再次的平衡。

调整的效果就是希望立刻重新平衡PG的数量(需要注意在业务负载低的时候执行) 数据均衡分配。迫使算法动态的更新PG的位置。

调整WEIGHT的值ceph osd crush reweight osd.10 {WEIGHT} # 调整weight值， 越大权重越高，分配的PG数量越大。

调整REWEIGHTceph osd crush reweight {OSD_ID} {REWEIGHT} # 范围是 0 - 1， 也是越大权重越高。


对运行图进行操作
创建一个保存运行图的目录mkdir &#x2F;cephmap&#x2F;ceph -pceph osd getcrushmap -o &#x2F;cephmap&#x2F;ceph&#x2F;crushmap  # 同时可以做备份使用，将运行图导出到文件中，当需要的时候可以通过这个文件还原。
转换map的二进制格式为文本，Ubuntu 需要安装 crushbase 包。crushtool -d &#x2F;cephmap&#x2F;ceph &gt; &#x2F;cephmap&#x2F;ceph&#x2F;crushmap.txt
编辑这个文件主要编辑type部分， 按照不同的调度需求，按照不同的osd，不同主机调度，不同的机架，不同的机柜，不同的PDU，不同的房间，不同的数据中心， 不同的区域城市， 顶层。下面是主机的配置部分。每个主机的算法，osd的分配情况，权重。Rules 副本池的规则里面定义了step take default  基于default配置端里面的规则来进行osd的分配。step chooseleaf firstn 0(按照顺序选择，先选到的就是) type host（按照什么类别进行选择高可用的类型）max_size 副本池配置文件之中可以定义副本数
转换成二进制文件crushtool -c &#x2F;cephmap&#x2F;ceph&#x2F;crushmap.txt -o &#x2F;cephmap&#x2F;ceph&#x2F;crushmap_new 
导入查看是否生效ceph osd setcrushmap -i &#x2F;cephmap&#x2F;ceph&#x2F;crushmap_new
查看是否生效ceph osd crush rule dump
创建一个存储池ceph osd pool create magedu-ssdpool 32 32 magedu_ssd_ruleceph pg ls-by-pool  magedu-ssdpool | awk ‘{print $1,$2,$15}’

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/08/26/Linux/Linux_RangerUsage/" title="Linux_Ranger_Usage">Linux_Ranger_Usage</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-25T17:49:46.000Z" title="发表于 2021-08-26 01:49:46">2021-08-26</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.740Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">ranger 备忘
g/ Go root 
gh Go home
gg Go to top 
G  Go bottom



# 文件操作 复制、剪切、删除、粘贴 （针对当前文件或则选择的内容）
yy 复制
dd 剪切
pp 粘贴
F5 复制
F5 剪贴
F8 删除
Delete 删除

# 书签
mX 把当前目录做一个书签 (X 表示任何字符)
&#39;X 跳到X书签代表的目录

# 标签 不同标签可以复制、粘贴、移动
gn 新建一个标签
Alt+N 跳转到N号标签 (代表一个数字)
gt,gT 跳转到前个标签，后个标签

# 排序 对文件和目录列表进行排序，以便查看。
ot 根据后缀名进行排序 (Type)
oa 根据访问时间进行排序 (Access Time 访问文件自身数据的时间)
oc 根据改变时间进行排序 (Change Time 文件的权限组别和文件自身数据被修改的时间)
om 根据修改进行排序 (Modify time 文件自身内容被修改的时间)
ob 根据文件名进行排序 (basename)
on 这个好像和basename差不多(natural)
os 根据文件大小进行排序(Size)

# 重命名 修改文件名有两种模式：当前文件和批量改名
cw 新文件名 -- 修改当前文件名
A -- 在当前文件名后追加文字
I -- 在当前文件名前追加文字
:bulkrename --针对mark过的文件批量改名

# 执行shell命令
! -- 进入命令模式执行shell命令
s -- 同上
# -- 同！，但结果输出到一个pager。相当于 cmd | less
@ -- 同！，但会把选择的文件作为参数放在最后。
S -- 进入一个新的shell。exit后回到当前的ranger

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/08/24/Linux/Linux_Ceph03/" title="Ceph Cluster 03 - CephFS">Ceph Cluster 03 - CephFS</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-24T14:29:26.000Z" title="发表于 2021-08-24 22:29:26">2021-08-24</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.725Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">ceph笔记03
Cephfs的使用Cephfs的使用条件
当我们需要多个服务来挂载和实时的同步的时候， 使用到CEPHFS，可以实现文件系统的共享。内核里面现在这个时间已经内置cephfs的挂载模块， 可以直接挂载不需要安装。
cephfs运行需要MDS服务，用来存储缓存的文件信息。总体需要创建两个存储池，单独创建一个存储MDS信息的存储池， 同时需要创建一个数据池来提供存储空间。
启用mds的服务 ceph orch mds 2
创建ceph的存储池ceph mds stat
# 创建一个cephfs的metadata池
ceph osd pool create metadata 32 32 
# 创建一个cephfs的data池
ceph osd pool create cephfsdata 64 64 
# 创建ceph的状态
ceph osd pool ls 
ceph -s 
# 创建cephfs的文件系统
ceph fs new defaultfs metadata cephfsdata

# 新的版本里面已经不需要手动创建mds和两个对应的存储池了， 只是需要一条命令就可以自动创建。
[ceph: root@ceph01 /]# ceph fs volume create test
[ceph: root@ceph01 /]# ceph fs volume ls
[ceph: root@ceph01 /]# ceph mds stat
# 需要提前获取挂载的Token： 
[root@HaydenArchDesktop ceph]# sudo scp root@ceph01:/etc/ceph/ceph.client.admin.keyring /etc/ceph/
[root@HaydenArchDesktop ceph]# mount -t ceph :/ /mnt -o name=admin
[root@HaydenArchDesktop mnt]# mount | grep ceph
192.168.31.11:6789,192.168.31.12:6789,192.168.31.13:6789:/ on /mnt type ceph (rw,relatime,name=admin,secret=&lt;hidden>,acl)

#
# journalctl -f > /mnt/journal.log   
# 查看cephfs的写入文件的动作过程。 
# 同时使用 tail -f 在另一个窗口中查看文件的内容。 测试写入和查看内容的差距。 


用户权限MDS可以启动多个实例，多个实例会自动来负载不同资源的文件元数据的缓存。客户端通过MON节点进行授权 和 获取MDS的的位置。MDS现在最新的版本默认已经是一主一备， 自动会生成高可用的模式。（之前的手动部署多个MDS服务器， 然后指定MDS的角色）
创建一个普通用户来进行身份的验证# 创建一个测试的用户
ceph auth add client.testuser mon 'allow r' mds 'allow rw' osd 'allow rwx pool=cephfsdata'
# 测试所建立的用户的权限，获取认证的keyring
ceph auth get client.testuser
# 导出用户的Keyring， 用来做集群的校验
ceph auth get client.testuser -o ceph.client.testuser.keyring
# 导出用户的key ， Kubernetes的挂载会使用到
ceph auth print-key client.testuser > testuser.key
# 验证用户是否可以获取到集群的状态
ceph --user testuser -s 

# 将挂载点添加到fstab自动开机挂载
MON01:6789,MON02:6789,.........:/ /mnt ceph defaults,name=testuser,secretfile=/etc/ceph/testuser.key,_netdev 0 0
Dashboard权限# 启用或者禁用账户
[ceph: root@ceph01 /]# ceph dashboard ac-user-enable admin

# 重置Dashboard用户的密码
# Old Version
[ceph: root@ceph01 /]# ceph dashboard set-login-credentials admin -i /etc/ceph/dashboard_password

# New version command.
[ceph: root@ceph01 /]# ceph dashboard -h | grep ac-user
dashboard ac-user-add-roles &lt;username> [&lt;roles>...]         Add roles to user
dashboard ac-user-create &lt;username> [&lt;rolename>] [&lt;name>]   Create a user. Password read from -i &lt;file>
dashboard ac-user-del-roles &lt;username> [&lt;roles>...]         Delete roles from user
dashboard ac-user-delete [&lt;username>]                       Delete user
dashboard ac-user-disable [&lt;username>]                      Disable a user
dashboard ac-user-enable [&lt;username>]                       Enable a user
dashboard ac-user-set-info &lt;username> &lt;name> [&lt;email>]      Set user info
dashboard ac-user-set-password &lt;username> [--force-         Set user password from -i &lt;file>
dashboard ac-user-set-password-hash &lt;username>              Set user password bcrypt hash from -i &lt;file>
dashboard ac-user-set-roles &lt;username> [&lt;roles>...]         Set user roles
dashboard ac-user-show [&lt;username>]                         Show user info

MDS 高可用# 提升默认的主节点的数量， 来提高MDS服务的吞吐量
ceph fs set defaultfs max_mds 2 
ceph fs get defaultfs
# 变成两主两备， （设置Rank）
# 参数： mds_standby_replay true 
         mds_standby_for_name: MDS_NAME
         mds_standby_for_rank: 备份指定级别的mds
         mds_standby_for_fscid: 指定文件系统ID，会联合rank配置生效，如果指定了rank就是指定文件系统的rank会进行主备，如果未指定就是指定文件系统的所有Rank。

如果是一对一的高可用 ， 需要对每个mds进行独立的配置。配置样例： # 配置的结果是mds1主， mds2 备； mds3 主， mds4 备.
[mds.ceph-mds1]
mds_standby_replay = true 
mds_standby_for_name = ceph-mds2
mds_standby_for_fscid = defaultfs 

[mds.ceph-mds3]
mds_standby_replay = true
mds_standby_for_name = ceph-mds4

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/08/23/Linux/Linux_Ceph02/" title="Ceph Cluster 02 - OSD/RBD">Ceph Cluster 02 - OSD/RBD</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-23T07:04:12.000Z" title="发表于 2021-08-23 15:04:12">2021-08-23</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.724Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Ceph的使用笔记。
创建存储池
# 创建一个PG为64 ，PGP为64的存储池。
[ceph: root@ceph01 /]# ceph osd pool create test-64 64 64
pool 'test-64' created

# 创建一个自动识别的大小的存储池。
[ceph: root@ceph01 /]# ceph osd pool create test
pool 'test' created

# 查看已经存在的存储池。
[ceph: root@ceph01 /]# ceph osd pool ls
device_health_metrics
test
test-64

# 查看存储池的PG 和 PGP 的信息和关系。
[ceph: root@ceph01 /]# ceph pg ls-by-pool test

# 查看OSD的状态。
[ceph: root@ceph01 /]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME        STATUS  REWEIGHT  PRI-AFF
-1         0.58557  root default
-5         0.14639      host ceph01
 2    hdd  0.04880          osd.2        up   1.00000  1.00000
 7    hdd  0.04880          osd.7        up   1.00000  1.00000
11    hdd  0.04880          osd.11       up   1.00000  1.00000
-7         0.14639      host ceph02
 3    hdd  0.04880          osd.3        up   1.00000  1.00000
 6    hdd  0.04880          osd.6        up   1.00000  1.00000
10    hdd  0.04880          osd.10       up   1.00000  1.00000
-9         0.14639      host ceph03
 1    hdd  0.04880          osd.1        up   1.00000  1.00000
 5    hdd  0.04880          osd.5        up   1.00000  1.00000
 9    hdd  0.04880          osd.9        up   1.00000  1.00000
-3         0.14639      host ceph04
 0    hdd  0.04880          osd.0        up   1.00000  1.00000
 4    hdd  0.04880          osd.4        up   1.00000  1.00000
 8    hdd  0.04880          osd.8        up   1.00000  1.00000

# 测试上传一个文件并且查看文件的状态和信息。
[ceph: root@ceph01 rpm]# pwd
/var/lib/rpm
[ceph: root@ceph01 rpm]# ls -lh ./Packages
-rw-r--r-- 1 root root 20M Jul  8 17:56 ./Packages

[ceph: root@ceph01 rpm]# rados put msg1 ./Packages --pool=test
[ceph: root@ceph01 rpm]# rados put msg1 ./Packages --pool=test-64

[ceph: root@ceph01 rpm]# rados ls --pool=test
msg1

[ceph: root@ceph01 rpm]# ceph osd map test msg1
osdmap e68 pool 'test' (2) object 'msg1' -> pg 2.c833d430 (2.10) -> up ([9,3,0], p9) acting ([9,3,0], p9)

[ceph: root@ceph01 rpm]# ceph osd map test-64 msg1
osdmap e68 pool 'test-64' (3) object 'msg1' -> pg 3.c833d430 (3.30) -> up ([2,0,3], p2) acting ([2,0,3], p2)

# 删除上传的文件
[ceph: root@ceph01 rpm]# rados rm msg1 --pool=test
[ceph: root@ceph01 rpm]# rados rm msg1 --pool=test-64
[ceph: root@ceph01 rpm]# rados ls --pool=test
[ceph: root@ceph01 rpm]# rados ls --pool=test-64

# 删除刚刚添加的存储池。(临时的解决方案是使用如下的命令， 永久生效的配置写入 /etc/ceph/ceph.conf)
[ceph: root@ceph01 ~]# ceph tell mon.* injectargs '--mon-allow-pool-delete=true'
mon.ceph01: &#123;&#125;
mon.ceph01: mon_allow_pool_delete = 'true'
mon.ceph02: &#123;&#125;
mon.ceph02: mon_allow_pool_delete = 'true'
mon.ceph03: &#123;&#125;
mon.ceph03: mon_allow_pool_delete = 'true'

# 执行删除命令就不会报错了。
[ceph: root@ceph01 ~]# ceph osd pool rm test test --yes-i-really-really-mean-it
pool 'test' removed
[ceph: root@ceph01 ~]# ceph osd pool rm test-64 test-64 --yes-i-really-really-mean-it
pool 'test-64' removed

# 创建一个KVM的块设备池。
[ceph: root@ceph01 ~]# ceph osd pool create kvm 256 256
pool 'kvm' created

# 设置Application enable flag
[ceph: root@ceph01 ~]# ceph osd pool application enable kvm rbd
enabled application 'rbd' on pool 'kvm'
# 初始化池。
[ceph: root@ceph01 ~]# rbd pool init -p kvm
# 创建一个可挂载的镜像。
[ceph: root@ceph01 ~]# rbd create disk01 --size 5G --pool kvm
# 查看镜像的相关信息
[ceph: root@ceph01 ~]# rbd ls --pool kvm
disk01
  size 5 GiB in 1280 objects
  order 22 (4 MiB objects)
  snapshot_count: 0
  id: 1703d21a8f2ba
  block_name_prefix: rbd_data.1703d21a8f2ba
  format: 2
  features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
  op_features:
  flags:
  create_timestamp: Mon Aug 23 09:13:58 2021
  access_timestamp: Mon Aug 23 09:13:58 2021
  modify_timestamp: Mon Aug 23 09:13:58 2021

# 安装Ceph的客户端以及rbd命令。
[root@HaydenArchDesktop hayden]# sudo pacman -S ceph
[root@HaydenArchDesktop hayden]# rbd -p kvm map disk01
/dev/rbd0
[root@HaydenArchDesktop hayden]# lsblk
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
rbd0        254:0    0     5G  0 disk

# 格式化文件系统。 
[root@HaydenArchDesktop test]# mkfs.xfs /dev/rbd0
# 挂载。
[root@HaydenArchDesktop test]# mount /dev/rbd0 /opt/test/
# 移除挂载
[root@HaydenArchDesktop test]# umount /opt/test
# 取消这个映射关系。
[root@HaydenArchDesktop test]# rbd unmap /dev/rbd0
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/08/21/Linux/Linux_Ceph01/" title="Ceph Cluster 01 - Installation">Ceph Cluster 01 - Installation</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-08-21T15:09:06.000Z" title="发表于 2021-08-21 23:09:06">2021-08-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.724Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Ceph 的学习笔记和记录。
开始部署信息OS Version: Fedora 34 ServerCEPH Version: v15.2.0 (Octopus) +DOCKER Version: 20.10.8
配置实例的 Hostname 和网络vim /etc/hosts
  192.168.122.121 ceph01 ceph01.liarlee.site
  192.168.122.122 ceph02 ceph02.liarlee.site
  192.168.122.123 ceph03 ceph03.liarlee.site
  192.168.122.124 ceph04 ceph04.liarlee.site

[root@ceph01 ~]$ ssh 192.168.122.121 echo "ceph01.liarlee.site" > /etc/hostname
[root@ceph01 ~]$ ssh 192.168.122.122 echo "ceph02.liarlee.site" > /etc/hostname
[root@ceph01 ~]$ ssh 192.168.122.123 echo "ceph03.liarlee.site" > /etc/hostname
[root@ceph01 ~]$ ssh 192.168.122.124 echo "ceph04.liarlee.site" > /etc/hostname

[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.121/24
[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.122/24
[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.123/24
[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.address 192.168.122.124/24

[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.gateway 192.168.122.1
[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.dns 192.168.122.1
[root@ceph01 ~]$ nmcli conn mod enp1s0 ipv4.method manual
[root@ceph01 ~]$ nmcli conn up enp1s0

[root@ceph01 ~]$ sudo systemctl stop firewalld &amp;&amp; sudo systemctl disable firewalld
[root@ceph01 ~]$ vim /etc/config/selinux # change it to disabled

CONFIG Dnf Repo# config docker
[root@ceph01 ~]$ wget -O /etc/yum.repos.d/docker-ce.repo https://download.docker.com/linux/fedora/docker-ce.repo
[root@ceph01 ~]$ sudo sed -i 's+download.docker.com+mirrors.tuna.tsinghua.edu.cn/docker-ce+' /etc/yum.repos.d/docker-ce.repo
# Add GPG Key.
[root@ceph01 ~]$ sudo rpm --import 'https://mirrors.tuna.tsinghua.edu.cn/ceph/keys/release.asc'
# update cache and install cephadm.
[root@ceph01 ~]$ dnf makecache -y &amp;&amp; dnf install curl wget htop -y
[root@ceph01 ~]$ dnf install cephadm -y

INSTALL Docker[root@ceph01 ~]$ sudo dnf remove docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-latest-logrotate \
                  docker-logrotate \
                  docker-selinux \
                  docker-engine-selinux \
                  docker-engine
[root@ceph01 ~]$ sudo dnf install docker-ce docker-ce-cli containerd.io -y
[root@ceph01 ~]$ sudo systemctl start docker containerd
[root@ceph01 ~]$ sudo systemctl enable docker containerd

# Config proxy for docker daemon.
[root@ceph01 ~]$ mkdir /etc/systemd/system/docker.service.d/
[root@ceph01 docker.service.d]$ cat http-proxy.conf
  [Service]
  Environment="HTTP_PROXY=http://192.168.31.199:7890/"
  Environment="HTTPS_PROXY=http://192.168.31.199:7890/"

INSTALL CEPHCephadm tools were default in Fedora Repo, No need to change the repo to tsinghua or aliyun. Just install. WOW ~ Fedora YYDS.
启动一个ceph集群[root@ceph01 ~]$ cephadm bootstrap --mon-ip 192.168.122.121 --allow-fqdn-hostname
Please notice the output, context include username and password and dashboard address.
Ceph Dashboard is now available at:

       URL: https://localhost.localdomain:8443/
      User: admin
  Password: 20jrekw4ko

Enabling client.admin keyring and conf on hosts with "admin" label
You can access the Ceph CLI with:

  sudo /usr/sbin/cephadm shell --fsid e8997974-029f-11ec-a59a-525400c06f36 -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

使用 Ceph Shell 命令来管理集群节点# temprary use
[root@ceph01 ~]$ cephadm shell -- ceph -s

# start a interactive shell
[root@ceph01 ~]$ cephadm shell

# check the ceph status 
[ceph: root@ceph01 /]$ ceph -s

# list ceph hosts
[ceph: root@ceph01 ceph]$ ceph orch host ls --format yaml

# general a new ssh key for cephadm
[ceph: root@ceph01 ceph]$ cephadm get-pub-key > /etc/ceph/ceph.pub

# copy new key to hosts
[ceph: root@ceph01 ceph]$ ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.122.122
[ceph: root@ceph01 ceph]$ ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.122.123
[ceph: root@ceph01 ceph]$ ssh-copy-id -f -i /etc/ceph/ceph.pub root@192.168.122.124

维护集群ADD Hosts[ceph: root@ceph01 ceph]$ ceph orch host add ceph02.liarlee.site 192.168.122.122
[ceph: root@ceph01 ceph]$ ceph orch host add ceph03.liarlee.site 192.168.122.123
[ceph: root@ceph01 ceph]$ ceph orch host add ceph04.liarlee.site 192.168.122.124

# set mon sub-network 
[ceph: root@ceph01 /]$ ceph config set mon public_network 192.168.122.0/24

ADD Osd# auto-detect available devices (need time to sync the status 1 by 1)
# NOTE: Strangely enough, the command automatically recognizes all devices, including the ZRAM!  QAQ.....
[ceph: root@ceph01 /]$ ceph orch apply osd --all-available-devices

# list devices
[ceph: root@ceph01 /]$ ceph orch device ls
Hostname             Path        Type  Serial  Size   Health   Ident  Fault  Available
ceph01.liarlee.site  /dev/vdb    hdd           21.4G  Unknown  N/A    N/A    Yes
ceph01.liarlee.site  /dev/vdc    hdd           21.4G  Unknown  N/A    N/A    Yes
ceph01.liarlee.site  /dev/vdd    hdd           21.4G  Unk ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2021/07/01/Linux/Linux_Ubuntu-kernel-Complie/" title="Ubuntu 18.04 内核编译初试">Ubuntu 18.04 内核编译初试</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-07-01T04:48:29.000Z" title="发表于 2021-07-01 12:48:29">2021-07-01</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.742Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Ubuntu 18.04 上编译内核，生成deb安装包的过程和遇到的问题。使用工具： Linux-tkg系统版本： Ubuntu18.04内核版本： 5.12-muqss-6ms-skylake
背景说明看到B站大佬的教学视频， 使用Linuxtkg进行内核的编译打包和添加muqssCPU调度器， 想自己尝试一下， 同时熟悉一下Ubuntu常见的工具链，所以就开始做了这个事情。

首先是需要对系统进行初始化， 需要一些底层的工具包。 在Ubuntu上面还是有些问题的， 这个坑自己踩了。
]$ sudo apt install zstd git wget sudo bc rsync kmod cpio libelf-dev build-essential fakeroot libncurse5-dev libssl-dev ccache bison flex qtbase5-dev kernel-package

NOTE: 这里面我遇到少了pkg但是没有明确报错的是：zstd &amp; kernel-package ，这两个如果没有正确的安装报错是比较模糊的， 完全不能定向到问题是缺少了这两个包。 （而官方的手册中也没有写需要安装kernel-package就比较坑爹

之后就是下载Linux-tkg ，建议直接从Github下载， Linux-tkg official address
]$ git clone https://github.com/Frogging-Family/linux-tkg.git

在clone完成之后， 执行下面的.&#x2F;install.sh config, 按照提示回答相关的问题。 

或者可以直接编辑其中的customization.cfg来设置默认的参数。（这个文件中都是有说明的，还算是比较详细， 一般可以直接看懂。 

回答完成问题之后会生成对应的config文件。

执行.&#x2F;install.sh install， 之后会有几个不同的发行版的选择， 只需要选择自己需要的即可， 如果是Deb系的会自动启动编译命令 make -j32 deb-pkg ,自动生成的软件包在.&#x2F;DEBS&#x2F;*.deb.直接安装即可。如果是rpm也会生成RPMS。

如果中途报错 ， 我也不知道具体的处理方式。目前发现的问题如下：

不同版本的ubuntu 表现是不同的， 震惊！ 同样的设置参数 ，同样的内核代码和补丁，在Ubuntu 18.04 和 20.04 ， 20.04 会成功。
不同环境的结果是不同的， 在一台服务器上面 起Docker容器进行初始化之后编译 会成功的。 在笔记本上不一定会成功。 
目前没有总结出任何的相关规律， 太任性了只能说。 
如果是不打包成deb，目前的成功率是100%（包括直接将文件释放到系统中， 和打成rpm）。




现在能想到的差不多这些吧 ， 反正编译出错就是dpkg-package ERROR 2 ， 我现在满脑子都是ERROR 2了， 告辞。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2020/11/16/Database/Databases_Mongo_JavaException-MangoDB-ssl/" title="Java连接数据库报错No subject alternative names present">Java连接数据库报错No subject alternative names present</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-11-16T13:28:41.000Z" title="发表于 2020-11-16 21:28:41">2020-11-16</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.718Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Database/">Database</a></span></div><div class="content">配置监控的时候需要配置和获取MongoDB的信息，使用华为云的MongoDBPaaS服务，如果开启了SSL就无法正常连接。
报错如下： 
Timed out after 30000 ms while waiting to connect. Client view of cluster state is &#123;type=UNKNOWN, servers=[&#123;address=192.168.1.1:8635, type=UNKNOWN, state=CONNECTING, exception=&#123;com.mongodb.MongoSocketWriteException: Exception sending message&#125;, caused by &#123;javax.net.ssl.SSLHandshakeException: java.security.cert.CertificateException: No subject alternative names present&#125;, caused by &#123;java.security.cert.CertificateException: No subject alternative names present&#125;&#125;]

网上搜索到的结果基本上都是从开发的角度解决这个错误， 如果是从运维的方向上 ， 没有什么有效的方法可以解决这个问题么？比如通过一些参数或者设置。
有的 ： 

在连接串里加上sslinvalidhostnameallowed&#x3D;true即可, 设置是直接允许无效的hostname证书， 算是不解决证书问题一个解决方案吧。

所以 最后的连接参数变成： 
mongodb:&#x2F;&#x2F;192.168.1.1:8635&#x2F;test?authSource&#x3D;admin&amp;ssl&#x3D;true&amp;sslinvalidhostnameallowed&#x3D;true

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2020/08/20/Linux/Linux_PerformanceNote/" title="Linux性能调优笔记">Linux性能调优笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-08-20T09:50:39.000Z" title="发表于 2020-08-20 17:50:39">2020-08-20</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.739Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">应该是一个性能调优的书的笔记， 记不清了， 存货了属于是。
Linux的性能调优CPU性能释放Process生命周期
建立一个新的进程, 表示为父进程， 父进程进入Wait状态。
父进程Fork()系统调用出来的一个子进程。
子进程调用exec()对操作进行执行。
子进程执行结束退出exit()。
子进程变为Zombie进程。
等待父进程回收，更新父进程的运行状态。

进程与线程线程是可以在同一个进程下并发执行的执行单位，他们共享相同的地址，数据和运行空间。线程也叫做（LWP） - 轻量的进程。两者的区别在于，进程在同一个CPU上不能并发执行，且两个进程间不是共享资源的方式进行数据处理的。其他的地方， 进程和线程并无太大的区别，Linux的内核将使用一直的Manner对进程和线程进行调度和处理。
There are several thread implementations avaliable in linux operation system.

Linux threads
传统的Linux进程

Native POSIX thread library
内核 2.6 以后由红帽开发的进程模型。

Next Generation POSIX Thread
IBM开发的新的进程模型。
Note：Linux系统中的环境变量 LD_ASSUME_KERNEL 。


进程优先级
优先级(控制调度的先后顺序): 优先级的范围是： 0 - 139。
Nice值(控制运行时间的长短): 每个进程的nice值，19 到 -20 ，nice值越大 分配的时间越长。19最低 ， -20最高。默认的nice值是0。

进程上下文切换每一个进程有独立的数据存储，当多个进程在同一颗CPU切换时，内核会对进程需要的数据空间进行重定向，这个行为叫做上下文切换。Context Swtiching，负责进程的上下文切换以及调度。在多个进程切换的过程中， 每次切换都会触发一次上下文的切换，是导致性能下降的主要原因。
中断CPU的终端控制通常由 硬中断 和 软中断为主， 硬中断通常见于硬件设备，鼠标键盘网卡硬盘设备。软中断常见于TCP&#x2F;IP协议操作，SCSI协议操作。
Note： 中断的信息显示在： /proc/interrepts。
Note：在多个CPU的系统中，可以将中断集中绑定在某一颗物理的CPU上，可以有效的改善系统的性能。
进程的状态
RUNNING（正常的进程）
STOPPED（已经停止的进程）
UNINTERREPTIBLE（Disk I&#x2F;O）
INTERREPTIBLE（Keyboard I&#x2F;O）
ZOMBIE（只能通过结束或重启父进程来回收僵尸进程）

进程的内存空间进程的内存地址空间：(由顶至低顺序为)Text Segment， Data Segment，Heap Segment， Stack Segment

Text Segment ： 存储进程可执行代码的部分，只读模式。
Data Segment： 包括三个部分
Data： 数据片段， 初始数据的存储，例如静态变量。
BSS：填零数据的存储。数据初始化为0。
Heap（堆内存）： malloc()按需分配的动态内存，堆内存向更高的地址发展。


Stack Segment： 栈内存。存储本地变量， 方法参数，方法返回地址。栈内存向更低的地址发展。

NOTE：可以使用命令pmap查看一个用户空间进程的地址分配情况。可以使用ps命令查看总的内存分配情况。
CPU的NUMA Node
一颗物理CPU 8逻辑核，两个NUMA节点。

4CPU为一组， 在同一个NUMA节点中。

HT技术提供了一个物理两个逻辑CPU。

在 1 情况下，一般不触发跨越NUMA节点的负载均衡，除非子节点过载。
在 2 情况下，提供了调度器tick的 时间调度 和 时间片 调度。
在 3 情况下， 提供调度器tick时间的负载均衡调度。




内存性能Linux内存的结构
物理内存物理的内存的分配分别有32位和64位的不同，

32位系统中只能Linux内核可以直接管理的内存空间只有第一个 1 GB（扣除预留的部分只有896M） ， 剩下的空间需要映射到前面的1GB空间中。这种映射会使性能下降，但是对于应用程序来说是透明且无感的。
64位系统中将ZONE-NORMAL的区域扩展到了64GB - 128GB， 就不需要这种映射操作了。

虚拟内存虚拟内存的地址布局

32位架构中，一个进程可以使用的虚拟内存的空间最大只能有4GB。内存被分为了 3GB的用户空间和1GB的内核空间。
64位架构中完全没有这种限制，每个进程都可以使用全部的内存空间。

虚拟内存的管理物理内存通常对于应用或者用户是不可见的，Linux内核会将任何内存自动映射到虚拟内存。
应用不直接使用物理内存而是向内核申请一个确切空间的虚拟内存映射，并在虚拟内存中接收并处理内存的映射关系。
并且，虚拟内存不是必须映射到物理内存，还可以映射到在硬盘子系统中的Swap文件。
应用通常也不直接写入硬盘子系统，而是写入数据到Cache&#x2F;Buffer中。pdflush内核线程在合适的时间负责将Cache和Buffer中的数据刷写到硬盘。
Linux虚拟内存管理器分配全部的未使用虚拟内存作为磁盘的cache，其他的操作系统只使用内存中的一部分。
同样的，交换的空间的管理同样也是如此，事实上交换空间占用程度并不代表系统的瓶颈所在，相反证明了linux系统资源调度上的高效。
页帧的分配 - 内存分页
Page - 页是在物理内存（Page Frame） 或者 虚拟内存中的连续线性地址空间。
Linux通过控制页单元来控制内存的分配，一页的大小是 4Kb。
内核了解那些分页是可用的，以及他们的确切位置。

Buddy SystemLinux使用这种机制来进行空闲页的管理和维护，并对申请进行分配，始终试图保持内存分页的连续性。当内存的分配失败的时候，会内存的页帧回收。
Page Frame Reclaiming当所有的页已经处于不可用的状态（unavalible）,会触发内存的回收机制，将暂时不在使用中的，或者在使用中但是优先级低的内存重新分配，这种机制叫做内存回收。内核线程pswapd和内核函数try_to_free_page()负责执行这个动作。

文件系统Linux可以支持多种多样的文件系统，这得益于内核的VFS技术，VFS是介于用户进程和文件系统之间的抽象层。由于VFS的存在，那么用户进程无需知道文件系统的类型，就可以直接进行文件系统的使用。
用户进程调用文件系统的流程：
User Process –&gt; System Call –&gt; VFS –&gt; Variety of supported file system
日志：
Ext2简单的文件系统，无日志记录
Ext2 文件系统（BlockGroup）结构：

SuperBlock： 存储信息，在每一个BlockGroup的前面都有一个SuperBlock。

BlockGroupDesciptor： 存储BlockGroup的信息。

Data Block Bitmaps： 空闲的数据块管理。

i-node Bitmap： 空闲的Inode管理

i-node Tables： inode的table存储位置。记录了文件的基本信息，例如： uid，gid, atime, ctime, mtime, dtime,指向数据块的位置。

Data blocks: 实际用户数据的存储位置。
文件系统查找文件的过程： 先在&#x2F;下查看inode信息，查看所找文件的信息，按照路径持续查找，直到找到了文件的inode信息，通过inode提供的信息去数据块读取数据。


Ext3ext3文件系统是ext2 的升级版，主要的变更是支持了文件系统的日志，
ext3支持的日志模式有三种：

journal - 全量记录，数据也元数据都通过日志记录。
Ordered - 只有元数据会记录日志。
Writeback - 记录了元数据的操作，无法保证数据的一致性，突然终止会导致旧数据的出现。

Xfsxfs是新一代的日志文件系统，性能和稳定性都不错。其他资料待补全。

IO子系统进程是如何使用IO子系统进行数据交换的？

进程通过write()系统调用，请求一个文件的写入。
内核更新Pagecache映射到文件。
pdflush内核线程处理pagecache到硬盘。
文件系统层将每个Blockbuffer组成一个Bio结构，提交和写入到块设备层。
块设备层得到上层的请求，执行IO电梯算法操作将数据推入写入队列。
存储驱动接管并执行写入。
硬盘硬件设备执行写入到盘片。

关于缓存(cache)?
由于CPU和硬盘的速度差距太大，因此需要缓存来进行临时的数据存储。
速度递减的结构是：
CPU –&gt; cache –&gt; RAM –&gt; Disk
块层 Block Layer关键的数据结构就是bio结构，bio结构是一种接口，存在于 文件系统层 和 块层之间。
bio就是将相邻的block buffer块整合到一起，发送bio到块层。
块大小直接影响服务器性能的设置。如果需要存储的文件尺寸比较大，那么设置大的blocksize性能更好。
IO电梯算法
Anticipatory
CFQ
BFQ
Deadline
MQ-deadline
NOOP
NONE

存储驱动
SCSI
RAID


网络子系统netstat -n | awk &#39;&#x2F;^tcp&#x2F; &#123;++S[$NF]&#125; END &#123;for (a in S) print a, S[a]&#125;&#39;
# 查看所有的连接状态

当一个应用需要发送网络数据流程：

应用打包自己的数据。
应用通过套接字接口写入数据。
socket buffer用于处理需要被发送的数据。缓冲区引用了数据，并贯穿所有的层。
在每个层中，执行相应的操作，例如数据的封包，添加数据的报文首部。
从网卡的物理接口发出。
以太网帧抵达对端的网卡接口。
如果MAC地址匹配对端网卡的地址，数据帧移动到物理网卡的Buffer。
移动数据包到socket buffer中，同时发出一次CPU硬中断。
数据包逐层解包，直到抵达可以解释的应用层。


通过&#x2F;proc&#x2F;sys&#x2F;net目录可以进行网络子系统的调优。
​	&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;rmem_max​	&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;rmem_default​	&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;wmem_max​	&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;wmem_default​	&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_mem​	&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_rmem​	&#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_wmem
Network API(NAPI)NAPI是一种新的网络处理方式，每次对数据包的操作都会触发系统中断，新的方式使用的是poll的方式。
NAPI 是 Linux 上采用的一种提高网络处理效率的技术，它的核心概念就是不采用中断的方式读取数据，而代之以首先采用中断唤醒数据接收的服务程序，然后 POLL 的方法来轮询数据，
https://www.ibm.com/developerworks/cn/lin ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2020/06/27/Linux/Linux_Elasticsearch-02/" title="ElasticSearch 安装记录">ElasticSearch 安装记录</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-06-27T13:05:02.000Z" title="发表于 2020-06-27 21:05:02">2020-06-27</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.731Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/ElasticSearch/">ElasticSearch</a></span></div><div class="content">记录一下自己的集群安装过程和常见的命令。
Elastic Search三节点的安装
节点的名称和相关参数：



Host-name
IP
Cluster Name
Role



elk01
192.168.122.101
liarlee-elk
Elasticsearch


elk02
192.168.122.102
liarlee-elk
Kibana


elk03
192.168.122.103
liarlee-elk
Filebeat



使用清华的repo
 [elasticsearch]
name&#x3D;Elasticsearch repository for 7.x packages
#baseurl&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;packages&#x2F;7.x&#x2F;yum
baseurl&#x3D;https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;elasticstack&#x2F;7.x&#x2F;yum&#x2F;
gpgcheck&#x3D;0
gpgkey&#x3D;https:&#x2F;&#x2F;artifacts.elastic.co&#x2F;GPG-KEY-elasticsearch
enabled&#x3D;1
autorefresh&#x3D;1
type&#x3D;rpm-md

三个服务器都需要安装Elasticsearch并设置开机启动。
 # 安装
yum makecache fast &amp;&amp; yum install -y elasticsearch 
# 编辑配置文件
vim /etc/elasticsearch/elasticsearch.yml
  node.master: true[只有其中的两台是主节点即可，剩下的03可以设置为False]
  node.name: elk01 [节点的名称，可以自定义，但是一个集群的内部节点名称不能相同]
  network.host: 192.168.122.101[每个机器的外部IP]
  http.port: 9200
  node.data: true [三个节点都可以存储数据]
  cluster.name: liarlee-elk [集群的名称三个机器必须一致]
  cluster.initial_master_nodes: ["elk1.hayden.cluster"]
  discovery.zen.ping.unicast.hosts: ["elk01", "elk02", "elk03"]
  discovery.zen.minimum_master_nodes: 1 [最少的master节点需要有一个]
# 复制到02
scp /etc/elasticsearch/elasticsearch.yml root@elk02:/etc/elasticsearch/elasticsearch.yml
# 复制到03
scp /etc/elasticsearch/elasticsearch.yml root@elk03:/etc/elasticsearch/elasticsearch.yml
# 去对应的机器上修改机器名和相关字段，结束， 尝试启动。 

确认三个节点的服务是否正常启动。
 # 使用curl命令访问节点的restfulAPI
curl http://elk01:9200/_cluster/health?v
curl http://elk01:9200/_cluster/health?pretty
curl http://elk02:9200
curl http://elk03:9200

在02上安装kibana，并且开机启动。
 # 使用yum安装
yum makecache fast &amp;&amp; yum install -y kibana 
systemctl enable kibana &amp;&amp; systemctl start kibana

访问Kibana的WebUI，可以正常的使用。
 # 通过API检查
curl http://elk02:5601
# 通过WebUI检查
firefox http://elk02:5601/

配置filebeat 无脑收集&#x2F;var&#x2F;log&#x2F;messages, 并查看上报的状态。
 # 安装filebeat， 开机启动
Download from ELK offical website : filebeat-7.7.1-linux-x86_64.tar.gz
cd /opt/filebeat/filebeat-7.7.1-linux-x86_64
vim ./filebeat.yml
  filebeat.inputs:
  - type: log
    enabled: true
    paths:
          - /var/log/messages
  output.elasticsearch:
  		hosts: ["192.168.122.101:9200"]
  setup.kibana:
  		host: "192.168.122.102:5601"
# 运行
cd /opt/filebeat/filebeat-7.7.1-linux-x86_64
sudo nohup ./filebeat -e -c filebeat.yml >/dev/null 2>&amp;1 &amp;

观察集群的日志的收集状况，我在这个时候已经没有其他的问题了…. Over.


有些问题
我自己的虚拟机我是经常性的暴力关机的，所以遇到了节点的状态同步不正确，这导致我的ES集群启动的时候总是只有一个节点在线 ，其他的节点无法加入， 可以在ES的日志中观察到，节点试图添加，但是无法成功加入， 这个时候只需要删除无法加入节点的Node数据即可。

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2020/05/30/Linux/Linux_Lvm2-member-mounterror/" title="记一次旧LVM硬盘挂载失败">记一次旧LVM硬盘挂载失败</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-05-30T09:54:47.000Z" title="发表于 2020-05-30 17:54:47">2020-05-30</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.735Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">我之前的硬盘上是fedora默认的LVM分区，我换了硬盘之后，弄了一个硬盘盒，把旧的硬盘放进去，连到电脑上试图把旧的数据取出来。发现系统已经正确的识别了PV，VG，LV，但是不能挂载， 提示无法读取硬盘的Superblock 和提示 mount: unknown filesystem type ‘LVM2_member（这个提示说明你是直接挂载的&#x2F;dev&#x2F;sdx，LVM需要你挂载的应该是逻辑卷，不是物理设备）。


问题是这样的HaydenArchLinux$ lvs
  LV   VG                    Attr       LSize    Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  home fedora_localhost-live -wi------- &lt;118.77g
  root fedora_localhost-live -wi-------   70.00g
  swap fedora_localhost-live -wi-------    7.75g
  
---
HaydenArchLinux$ vgs
  VG                    #PV #LV #SN Attr   VSize    VFree
  fedora_localhost-live   1   3   0 wz--n- &lt;196.52g    0

---
HaydenArchLinux$ vgchange -ay /dev/fedora_localhost-live
  device-mapper: create ioctl on fedora_localhost--live-swap LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFIj295MyZpNmiMsazbwuGuLsrDgl7u509 failed: Device or resource busy
  device-mapper: create ioctl on fedora_localhost--live-home LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFqvyrcOAarRr7ol6o2DbieN2mIRCnqi0m failed: Device or resource busy
  device-mapper: create ioctl on fedora_localhost--live-root LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFkTeSg9t8t8dy1jqHhzD6BRcOoJ7dG0H2 failed: Device or resource busy
  0 logical volume(s) in volume group "fedora_localhost-live" now active
  
---
HaydenArchLinux$ lvdisplay
 --- Logical volume ---
  LV Path                /dev/fedora_localhost-live/home
  LV Name                home
  VG Name                fedora_localhost-live
  LV UUID                qvyrcO-AarR-r7ol-6o2D-bieN-2mIR-Cnqi0m
  LV Write Access        read/write
  LV Creation host, time localhost-live, 2020-04-05 03:07:47 +0800
  LV Status              NOT available
  LV Size                &lt;118.77 GiB
  Current LE             30405
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto

可以注意看上面的vgchange的时候显示LVM-SPkNfSXuuIWyxOArrqJAnfQbYfU4tEKFqvyrcOAarRr7ol6o2DbieN2mIRCnqi0m failed: Device or resource busy， lvdisplay显示lv status是NOT avaliable。
去Google了一下，大部分复制粘贴来的答案都是：
两种方法，第一种是直接mount &#x2F;dev&#x2F;fedora_localhost-live&#x2F;home &#x2F;mnt;第二种是格式化硬盘。
我不需要这种粗鲁的处理方式，第一种和第二种其实都是废话。

处理方式是这样的简单说明一下，使用dmsetup命令，dmsetup是一个偏向底层的逻辑卷管理工具，可以对现在已经有的逻辑卷进行更改。我的初步怀疑是Arch自动识别挂载了旧硬盘的逻辑卷但是，使用了错误的参数，导致设备被占用但是我无法再次挂载使用。使用dmsetup remove 参数将系统现在已经识别出来的逻辑卷移除， 之后手动使用vgchange重新读取，问题解决了。
HaydenArchLinux$ dmsetup remove /dev/fedora_localhost-live/home
HaydenArchLinux$ vgchange -ay
  3 logical volume(s) in volume group "fedora_localhost-live" now active
HaydenArchLinux$ lvdisplay
  --- Logical volume ---
  LV Path                /dev/fedora_localhost-live/home
  LV Name                home
  VG Name                fedora_localhost-live
  LV UUID                qvyrcO-AarR-r7ol-6o2D-bieN-2mIR-Cnqi0m
  LV Write Access        read/write
  LV Creation host, time localhost-live, 2020-04-05 03:07:47 +0800
  LV Status              available
  # open                 0
  LV Size                &lt;118.77 GiB
  Current LE             30405
  Segments               1
  Allocation             inherit
  Read ahead sectors     auto
  - currently set to     256
  Block device           254:1
HaydenArchLinux$ mount /dev/fedora_localhost-live/home /mnt

如上才是正确的处理方式。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2020/05/02/Linux/Linux_Hugepage-configuration/" title="KVM虚拟机开启内存大页">KVM虚拟机开启内存大页</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-05-02T07:09:50.000Z" title="发表于 2020-05-02 15:09:50">2020-05-02</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.733Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Huge Pages是从Linux Kernel 2.6后被引入的。目的是使用更大的内存页面（memory page size） 以适应越来越大的系统内存，让操作系统可以支持现代硬件架构的大页面容量功能。透明大页（Transparent Huge Pages）缩写为THP，这个是RHEL 6（其它分支版本SUSE Linux Enterprise Server 11, and Oracle Linux 6 with earlier releases of Oracle Linux Unbreakable Enterprise Kernel 2 (UEK2)）开始引入的一个功能。具体可以参考官方文档。
概念这两者有啥区别呢？

这两者的区别在于大页的分配机制，标准大页管理是预分配的方式，而透明大页管理则是动态分配的方式。

使用大页的目的：

增加内存寻址的命中率，如果使用旧的内存分页方式，操作系统需要管理很多很多的小的内存页面，查找和命中的效率比较低。
想象一下， 你有一本1000页的书，你需要找到其中的第782页的第20行中的一个“我”字，那么计算机会从第一页开始翻动一页一页的看是否符合要求；现在我将书藉的每100页合成1页，那我们只需要顺序查看10次就可以找到这个字符所在的范围了，之后再去查看这个字符所在的具体位置，速度就会比之前一页一页找快得多。


配置启用手动分配的HugePage
设置操作系统可用的最大内存值
vim &#x2F;etc&#x2F;security&#x2F;limits.conf
* soft memlock 8192
* hard memlock 8192

设置sysctl.conf
# vim &#x2F;etc&#x2F;sysctl.conf
vm.nr_hugepages &#x3D; 72708
# sysctl -p

根据我的测试 ，需要重启才会生效，Hugepage部分的内存会一直属于使用中的内存，并且一直被占用。我的配置用了16G内存之中的8G，所以开机之后，我的8G内存是一直使用中的状态， 无论你的应用是否真正的开始使用了这部分Hugepage.



下面举例了如何查看Hugepages的信息：
$ cat &#x2F;proc&#x2F;meminfo | grep Huge
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
FileHugePages:         0 kB
HugePages_Total:    4096
HugePages_Free:     4096
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:         8388608 kB

我的需求试使用Hugepage来分配给KVM中的虚拟机， 加速虚拟机的内存使用效率，所以当然是在虚拟机的配置文件中配置，编辑虚拟机&#x2F;etc&#x2F;libvirt&#x2F;qemu&#x2F;HaydenGentoo.xml, 改成如下的样子：
&lt;memory unit&#x3D;&#39;KiB&#39;&gt;4194304&lt;&#x2F;memory&gt;
&lt;currentMemory unit&#x3D;&#39;KiB&#39;&gt;4194304&lt;&#x2F;currentMemory&gt;
&lt;memoryBacking&gt;
  &lt;hugepages&#x2F;&gt;
&lt;&#x2F;memoryBacking&gt;

之后重启虚拟机即可。可以观察到如下改变说明已经在使用了。
$ cat &#x2F;proc&#x2F;meminfo | grep Huge    
AnonHugePages:         0 kB
ShmemHugePages:        0 kB
FileHugePages:         0 kB
HugePages_Total:    4096
HugePages_Free:     2048
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
Hugetlb:         8388608 kB

在某些场景下完全关闭THP查看当前的状态
[root@localhost ~]# cat /sys/kernel/mm/transparent_hugepage/defrag
[always] madvise never
[root@localhost ~]# cat /sys/kernel/mm/transparent_hugepage/enabled
[always] madvise never

禁用THP
echo never > /sys/kernel/mm/transparent_hugepage/enabled
echo never > /sys/kernel/mm/transparent_hugepage/defrag

完全禁用 THP
grubby --update-kernel=ALL --args="transparent_hugepage=never"

---
sudo systemctl reboot

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2020/04/10/Other/Books_%E6%AD%A3%E8%A7%81-%E4%BD%9B%E9%99%80%E7%9A%84%E8%AF%81%E6%82%9F-%E8%8A%82%E9%80%89-1/" title="正见——佛陀的证悟（一）诸行无常(选摘)">正见——佛陀的证悟（一）诸行无常(选摘)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-04-10T10:59:51.000Z" title="发表于 2020-04-10 18:59:51">2020-04-10</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.959Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Books/">Books</a></span></div><div class="content">这是从《正见-佛陀的证悟》这书中，摘出来的句子，我认为有意义，或者对四法印中的诸行无常有理解的部分， 一直放在QQ空间中， 前两天拿出来看了一下， 觉得还是有不少的体会，所以把它搬过来了， 后续的内容是不是会更新还是要看我懒不懒了。
只是对老与死的厌恶，并不足以让太子离开王宫而踏入未知的世界；悉达多会采取这么激烈的行动，是因为他实在无法合理地解释所有已生和将出生的一切众生命运就是如此而已。如果所有生者都必须衰朽死亡，那么花园中的孔雀、珍宝、华盖、熏香、音乐，放拖鞋的金质拖盘、进口的玻璃水瓶、他与耶输陀罗和罗睺罗的感情、家庭、国家，都变得毫无意义。这一切的目的到底是什么？
完全不凭借任何科学工具，悉达多太子以吉祥草为垫，坐在一棵菩提树下，探索人类的本性。经过了长时间的思维，他终于了悟到一切万有，包括我们的血肉、我们所有的情绪和我们所有的觉受，都是由两个以上的元素组合而成。当两种或多种元素和合在一起，新的现象就会产生；钉子和木头产生了桌子；水和叶子产生了茶；而恐惧、虔诚和救世主，就产生了神。这些最终的产物，并没有独立于其各别元素的存在。相信它真实独立存在，是最大的骗局。而在和合的同时，各个元素也起了变化。只因接触和合，它们的性质也随之改变了。
一切万有，没有一样是以独立、恒常、纯粹的状态存在。你手上的书不是，原子不是，甚至神祇也不是。
因此悉达多发现，无常并不像一般人以为的就是意味死亡，而是意味变化。任何事物和另一个事物之间的位置或关系转变了，即使是非常细微的变动，都要依循无常的法则。
如果没有盲目的期待，就不会有失望。如果能了解一切都是无常，不会攀缘执著；如果不攀缘执著，就不会患得患失，也才能真正完完全全地活着。
悉达多从恒常的幻相中觉醒，因此我们称他为佛陀、觉者。到现在还是没有人可以长生不老，每个人终究会死，而且每天大概有二十五万人死亡。我们亲近的人不是已经死亡就是将会死亡。然而当亲人死去的时候，我们还是会震惊和悲伤；我们还是继续寻找青春之泉，或是长寿的秘方。
悉达多太子不再需要或渴求长生不老药了。由于了悟到一切事物皆是和合而成，解构无止境，而且一切万有的各个成分，没有一项是以独立、恒常与纯粹的状态存在的，他因此获得解脱。一切和合之物(现在我们知道这是指一切事物)与其无常的本质是合而为一、不可分割的。
当悉达多看到一个人走过，即使他很健康，悉达多所看到的是此人的生与灭同时发生。你也许会认为这样的人生观不太有趣，但在生命的旅程中能够同时看到一体的两面，可以是非常奇妙，而且可能会有很大的满足感。
也就表示现在有个“假设者”，悉达多会同意，只要有[假设者]，就会有上帝存在；但如果没有假设者，就不会有上帝存在。如果没有纸，就不会有书。如果没有水，就不会有冰。如果没有开始，就不会有结束。一件事物的存在，亟需依赖其它事物的存在，因此没有什么是真正独立的。尽管我们以为可以控制变化，但事实上大多是不可能的，因为无法察觉的影响因素太多了。也因为这种相互依存性，一切事物不可避免地会从目前或原始状态中解体。每一个变化中都蕴藏着死亡的因素。今日就是昨日之死。
大部分的人都接受一切生者终将死亡。然而我们对“一切”与“死亡”的定义或许不太一样。对悉达多来说，生指的是一切万有，不仅仅是花朵、蘑菇、人类，而是一切生成或和合的事物。而死亡指的是任何的解体或是解构。无常纯粹是一个简单实在的事实。不可能有一天，某个突发的和合事物会突然变得恒常，更难想象我们能证明这样的事。但是在今天，我们不是将佛陀奉为神明，就是想用科技证明自己比佛陀更高明。
佛陀教导我们，至少我们心中要保持着无常的概念，不要故意去隐藏它。我们借着不断地觉察和合的现象，便会了知因缘相依。认识因缘相依，我们就会认识无常。而当我们知道一切事物皆无常，才不会被种种假设、僵化的信条(不论宗教的或世俗的)、价值体系和盲目信仰所奴役。我们的觉察力可以让我们免于受限于个人的、政治的和感情的戏码之中。我们还可以将这种觉察力导向大至想象之极，小至次原子层次。
由于我们对自己的道德原则感到自豪，而且常强加于别人身上，因此道德观还是具有少许价值。然而，在整个人类历史当中，道德的定义也随着时代精神而一直在改变。美国人度量政治正确性或不正确性的仪表起伏不定，令人迷惑。不管如何称呼种族或文化群体，总是有人会被冒犯，游戏规则一直在改变。
当悉达多提到“一切和合的事物”，他所指的不只是像DNA、你的狗、艾菲尔铁塔、卵子和精子等具体可认知的现象而已。心、时间、记忆和上帝，也是和合而成。而每一和合的成分，又依赖更多不同层次的和合而成。    当悉达多教导无常时，他也超越了一般“结束”的想法，像是那种认为死亡只发生一次就完了的概念。死亡从生、从创造的那一刻开始，就没有停过。每一个变化，都是死亡的一种形式，因此每一个生都包含了另一个事物的死亡。
拿煮鸡蛋来做例子。如果没有不断的变化，蛋就煮不熟；煮好蛋的这个结果，需要某些基本的因缘。很显然的，你要有一颗蛋、一锅水，和一些加热的元素。另外有些非必要的因和缘，像是厨房、灯光、计时器，还有一只把蛋放进锅子的手。另外一个重要的条件，就是没有像是电力中断或是山羊跟进来打翻锅子之类的干扰。此外，每一个条件，例如母鸡，都需要有另一套具足的因缘条件。需要有另一只母鸡生下蛋才能孵出它，还要有安全的地方，有食物才能让它成长。鸡的食物也要有适合的地方生长，并且要能让它吃进去才行。我们可以将非必要和必要条件一直分析到小于原子的程度，而在这个分析的过程中，各种形态、形状、功能和标签也会不断地增加。
当无数的因缘和合在一起，而且没有障碍与干扰，结果是必然的。许多人误以为这是注定的或是运气所致。然而，到了一个程度以后，即使我们祈求蛋不要煮熟，它还是会熟的。
就像蛋一样，所有的现象是由无数的成分所组成，因此它们是可变的。这些无数的成分几乎都不是我们所能控制的，所以会让我们的期待落空。这种不可预料性，遍在于所有的物质、感受、想象、传统、爱情、信任、不信任、怀疑论，甚至上师和弟子以及人与神之间的关系。
信仰，怀疑论以及所有和合的现象一样，都是无常的。
有些人到现在还认为马克·查普曼(Mark Chapman)是谋杀约翰·蓝侬(John Lennon)唯一的罪犯。当我们能了解一个病态而饱受折磨的心是如何形成，并且知道它是在什么样的情况下运作，就比较能够理解并宽恕世界上众多的马克”查普曼。当条件成熟，就像蛋煮熟了一样，即使我们祈祷暗杀事件不要发生，它还是避免不了。超过了某个时间点，我们要改变条件的企图和行为就会徒劳无功了。
恐惧和焦虑是人类心智中主要的心理状态。恐惧的背后是对确定性不断的渴求。我们对未知感到恐惧。人心对肯定的渴望，是根植于我们对无常的恐惧。
但我们常常忘记自己的来日一直都是有限的。即使理智上知道有生必有死，一切和合终将分散，我们的情绪状态还是常常会回到相信恒常的模式，完全忘记相互依存性。这种习气会造成各种负面的情况，像是偏执、寂寞、罪恶感等等。我们会觉得被欺骗、被威胁、被虐待、被冷落，仿佛这个世界只对我们不公平。
佛陀不是一个悲观者、也不是末日论者，他是重视实际者，而我们却多是逃避现实者。当他说一切和合皆是无常，他并不认为那是坏消息，而是简单、科学的事实。我们能认清因缘的不稳定，就会了解自己有力量转化障碍，并且完成不可能的任务。生活中的各个层面都是如此。如果你现在没有一台法拉利，你完全有可能创造出因缘而拥有一台。只要世上有法拉利，你就有机会去拥有它。同样的，如果你想活久一点，可以选择不抽烟和多运动。合理的希望是存在的。而绝望，和它的反面—模一样，都是相信恒常的结果。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2020/04/06/Linux/Linux_Cat-and-less/" title="less命令占用内存过高">less命令占用内存过高</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2020-04-06T04:28:03.000Z" title="发表于 2020-04-06 12:28:03">2020-04-06</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.723Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">在华为的月度报告中， 发现了奇怪的问题，有一台机器在每天都会内存使用率飙到100%，偶尔的几次还会出现OOM的记录。在追踪问题的过程中，发现机器上每天的2，4，6，8点会自动运行处理日志的脚本，并统计关键字的数量。
这个脚本的过滤日志思路是， 使用 less 命令打开昨天的所有日志文件，并且读取内容，过滤其中的关键字，命令类似如下：
less yesterday*.log.gz | grep -e "xxxxx" -e "xxxxx" | sort | uniq | wc -l
关于Less, More, Cat, Zcat这个四个命令可以分成两组:

less &#x2F; more 的命令是为了个人类查看文件的内容设计的程序；
cat &#x2F; zcat 是一次性输出内容到屏幕的；

两组命令的唯一区别是是否提供前后反复查看的功能。但是就是这个功能，导致了命令执行的逻辑其实是完全不同的。
less命令的执行方式将查看到的数据放入内存中（USED），所以 如果是一个10G的日志，就会占用10G的内存， 如果不够，系统就会进行交换和将旧数据换出的操作。cat的执行模式是将数据直接输出到&#x2F;dev&#x2F;stdout，同时操作系统会将数据存储到Cache中，不会触发内存的告警， 由于日志中的数据其实只是进行一次过滤， 所以不需要长期的保留， 这也提高了bash处理日志文本的速度。 
补充一下， Vim也是一样的， Vim会尽可能的将文件载入到内存里， 所以日志类的文件是不需要， 也不能使用Vim的.
结论由于两个命令的处理方式不同,因此直接将命令中输出gz日志的命令换成了zcat，即可保证内存的使用不会变多, 这里同时提高的处理文本命令的效率，这个机器专门用来做这一件事情， 所以不需要考虑其他进程会影响日志处理， 造成数据的不一致。所以在脚本中一定一定使用正确的命令和方式处理数据，才能保障性能和效率的兼顾。
最后，优化过的命令为：
zcat yesterday*.log.gz | grep -e "xxxx" -e "xxxx" | sort -u | wc -l
最后的执行时间，从原来的半个小时起步，变成了10分钟即完成。
后续的处理已知我们的问题是不需要系统自动cache住我们过滤的日志数据，我们可以手动释放一下已经无用的cache内容： 
sync &amp;&amp; echo 3 > /proc/sys/vm/drop_caches
后面还添加了并行执行的处理命令， 但是这个并行执行的脚本确实写的非常的垃圾。 另外添加了使用python 自动填写过滤的结果到 Excel 表格。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/11/06/Linux/Linux_k8s-basic-7/" title="Kubernetes集群的学习笔记(7)">Kubernetes集群的学习笔记(7)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-11-06T08:07:53.000Z" title="发表于 2019-11-06 16:07:53">2019-11-06</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.746Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">Kubernetes的Dashboard 和 分级认证权限。
Dashboard的简介Dashboard算是k8s的一个管理的web界面。不做具体的操作，登录的时候使用的是K8S提供的用户名和密码。
Dashboard的部署只需要从github进行apply资源清单即可。
Dashboard的登录使用Token登录
获取系统中默认的admin的token，或者创建一个需要登录和管理的ServiceAccount，然后Binding到Role或者 ClusterRole,进行权限的控制

查看集群中自动创建的dashboard的secret， 系统部署完成之后自动创建了一个可管理全部集群的secret


   ~]$ kubectl get secret -n kube-system | grep dashboard
dashboard-admin-token-g85h7                      kubernetes.io&#x2F;service-account-token   3      109d


在这个secret中有Token相关的信息
~]$ kubectl describe secret dashboard-admin-token-g85h7 -n kube-system

其中的Token字段的内容就是可以用来登录的令牌。复制到dashboard中粘贴即可


使用config文件登录
创建ServiceAccount，Binding到role
获取Secret，然后查看他的token
制作config文件
使用config文件登录

Kubernetes的管理方式
命令
create , run ,expose , delete ,edit ….

命令式配置文件
create -f , delete -f , replace -f 

声明式配置文件
apply -f , patch


一般不混合使用，1.2 使用的是替换，但是3是立刻修改，立刻生效，所以还是比较危险的。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/11/06/Linux/Linux_k8s-basic-6/" title="Kubernetes集群的学习笔记(6)">Kubernetes集群的学习笔记(6)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-11-06T06:45:20.000Z" title="发表于 2019-11-06 14:45:20">2019-11-06</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.746Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">K8S的认证部分， ServiceAccount以及RBAC 。
授权插件
Node
ABAC
RBAC
Webhook

常用的授权插件就是RBAC
基于角色的授权和访问控制基于角色的访问控制，就是将权限授予Role，而不是User。 将权限的控制授予Role， 将User分配到Role。默认的是拒绝全部， 无法也无需定义拒绝权限，定义的Permission是许可访问的权限。

角色，User Accouts OR Service Accounts.
许可， Permission分为两个部分：Operation 以及 Object.

角色以及角色绑定Role – RoleBinding
Role 和 RoleBinding 是建立和控制 NameSpace 级别的权限。
集群角色以及集群角色绑定ClusterRole – ClusterRoleBinding
Cluster 和 ClusterBinding 是建立和控制 Cluster 级别的权限。
NOTE： 特殊的情况，可以对名称空间级别的
特殊的绑定方式ClusterRole – RoleBinding
可以使用 RoleBinding 绑定 ClusterRole， 那么这种情况下，Role只是具有NameSpace的权限。解释一下：雷在同一个Cluster中有多个不同的NameSpace，我需要对每个NameSpace都授权相同的权限，这种场景下:

如果使用RoleBinding 去绑定一个Role，那么每一个NameSpace都需要建立各自的 RoleBinding，并且都要各自绑定到Role。

如果建立一个ClusterRole, 使用RoleBinding绑定到ClusterRole上面，那么我只需要定义一个即可在全部集群范围内生效这个权限。这样的就相当于批量的进行Role的授权。


相关命令命令不做过多的解释，所有的资源可以通过explain获取，主要是记录逻辑和思路。
~]$ kubectl create role testrole --verb&#x3D;get,list,watch --resources&#x3D;pods -o yaml
~]$ kubectl get role
~]$ kubectl describe role pods-reader
~]$ kubectl create rolebinding test-rolebinding --role&#x3D;testrole --user&#x3D;testuser -o yaml
~]$ kubectl explain rolebinding
~]$ kubectl config use-context USERNAME@kubernetes
~]$ kubectl create clusterrole test-clusterrole --verb&#x3D;get,list,watch --resource&#x3D;pods -o yaml 
~]$ kubectl explain clusterrole
~]$ kubectl get clusterrole
~]$ 

其他补充在RBAC中可以存在三类组件：

user
group
service account

创建Pod的过程中可以指定一个值叫做ServiceAccountName，如果授权ServiceAccount高等级的权限，那么，Pod会以这个Account运行，那么Pod中的应用程序也会有ServiceAccount的权限。也就是提高了Pod应用程序的等级，使得Pod可以对K8S的相关资源进行管理和配置。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/10/22/Linux/Linux_ChangeGnometoi3wm/" title="i3wm的简单配置">i3wm的简单配置</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-10-22T07:09:59.000Z" title="发表于 2019-10-22 15:09:59">2019-10-22</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.725Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">将自己的桌面环境迁移到了i3。Gnome好用是好用的，但是体量还是有点儿大了，吃资源有点多。
这个现在已经更新到 DWM上面了， 对于这里面还有其他的工具是一直在用的 ， 比如 picom 等等。 
安装i3-gaps安装的过程比较简单,pacman 就完事儿了。
pacman -Syyu 
pacman -S i3-gaps
  这里有两个可以选择，一个是i3-wm,  还有一个是i3-gaps, 我用了i3gaps， 好看一些。
pacman -S alacritty 
pacman -S polybar
pacman -S compton (最新的软件改名了，叫picom,但是安装的方式不变的。)
pacman -S picom
pacman -S dmenu
pacman -S rofi
pacman -S feh 
pacman -S variety
pacman -S unzip-natspec  
#unzip-natspec貌似是在archlinuxcn里面的，可以处理zip解压的时候乱码的问题，不用考虑手动指定解压的-O了。

配置i3在已经有Gnome环境的条件下，i3 的配置还是比较轻松的。
I3的使用说明i3默认的操作是使用Windows&#x2F;Super&#x2F;Mod这个按键(或者随便怎么叫吧，后面都说Super，和Gnome的口径一致)，简单列举一下频率较高的使用方式

先说重载配置文件，使用 Super + Shift + r 
退出到DM，使用Super + Shift + e， DM就是你的登录界面。
i3提供了10个虚拟桌面，切换虚拟桌面方式是可以通过Super + 1 - 10 快速切换
在默认的Tiling排列模式中切换排列位置Vertical和Horizontal. 通过Super V 或者 Super H
调整容器布局：
使用Stacking – Super + s 所有的标签堆叠显示最上方提供标签进行切换
使用Tabbed – Super + w 标签页方式显示所有窗口
使用Toggle – Super + e 平铺窗口模式，可切换不同的布局方式


打开程序可以通过terminal开启，或者Super + d 使用dmenu开启（可将 dmenu 替换成 rofi）
关闭程序可以通过Super + Shift + q关闭，或者鼠标点击x，不是所有的窗口都有x
在窗口间移动可使用 Super + {jkl;}四个按键；或者使用Super + {up,down,left,right}的arrowkey
可将窗口变更为Floating， 使用Super + Shift + Space
Floating窗口调整位置可长按Super 使用鼠标拖动调整位置
全屏程序使用 Super + f

i3的自定义配置配置文件的位置配置文件的路径： $HOME&#x2F;.config&#x2F;i3&#x2F;config,所有的配置在这个文件下修改
自定义界面的设置默认的i3启动的时候还是挺丑的，我有四个设置：
gaps inner 5  # 设置i3窗口间的空隙大小，单位是像素。
new_window 1pixel # 设置新的窗口的边界宽度，效果是不显示窗口的title。
new_float 1pixel	# 新的浮动窗口的边界宽度，同上。
smart_borders on 	# 在只有一个窗口的情况下自动最大化当前的窗口，不处理窗口的Gap。

默认terminal程序更改设置默认使用 Super + Enter 打开alacritty
bindsym $mod+Return exec --no-startup-id alacritty

alacritty 是一个使用GPU进行渲染的terminal模拟器，它有自己的配置文件，我的配置文件路径在:$HOME&#x2F;.config&#x2F;alacritty&#x2F;alacritty.conf
*NOTE： 关于Alacritty的问题目前已经解决的差不多了。 还有还有一个问题是关于ssh过去之后不能正确的识别终端类型的BUG，解决方案在后面。
默认打开Firefox我的设置是Super + p
bindsym $mod+p exec --no-startup-id firefox

开机启动一些程序我的开机启动了 Compton, Variety, Remmina,Aria2c, Polybar, ibus-daemon，剩下的看自己喜欢什么就安装什么就OK了。
exec_always --no-startup-id compton --config &#x2F;home&#x2F;liarlee&#x2F;.config&#x2F;compton&#x2F;compton.conf -b
exec_always --no-startup-id variety 
exec_always --no-startup-id remmina
exec_always --no-startup-id aria2c --conf-path&#x3D;&#x2F;home&#x2F;liarlee&#x2F;.aria2&#x2F;aria2.conf
exec_always --no-startup-id sh &#x2F;home&#x2F;liarlee&#x2F;.config&#x2F;polybar&#x2F;polybar_startup.sh
exec_always --no-startup-id ibus-daemon -dr

截屏功能快捷键默认的i3不能用PrintScreen我觉得有点儿难受，所以自己添加了快捷键和保存位置
bindsym --release Print exec &quot;scrot -b -m &#x2F;home&#x2F;liarlee&#x2F;Pictures&#x2F;Scort_ScreenShot&#x2F;screenshot.png&quot;


更新：
之前的这个方式可以作为截图的快捷方式，但是当你需要连续的截图的时候就会特别的难受，因为新截图会自动覆盖掉旧的截图。更新一下新的方式，写一个脚本，当我们按下PrintSc的时候触发这个脚本，就可以对文件重命名了。

也许脚本名称可以叫 - screenshot.sh

#!/bin/bash

snapdate=`date "+%Y%m%d_%H%M%S"`
gnome-screenshot -f /home/hayden/screenshot/Screenshot-$snapdate.png


更新i3的配置文件，添加快捷键的管理。

bindsym --release Print exec "/home/liarlee/Scripts/System/screenshot.sh"


这样再截图就会保存到指定的位置，并且用时间命名区分开不同的截图，不会覆盖了。

调节音量快捷键由于已经有了PulseAudio， 所以i3自动增加了笔记本Func-key的调整，但是如果没有功能键的话，还是要按照如下自定义的。我调整音量用的Super + F2&#x2F;F3, 也可以改其他的
# Use pactl to adjust volume in PulseAudio.
bindsym XF86AudioRaiseVolume exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ +10% &amp;&amp; $refresh_i3status
bindsym $mod+F3 exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ +10% &amp;&amp; $refresh_i3status
bindsym $mod+F2 exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ -10% &amp;&amp; $refresh_i3status
bindsym XF86AudioLowerVolume exec --no-startup-id pactl set-sink-volume @DEFAULT_SINK@ -10% &amp;&amp; $refresh_i3status
bindsym XF86AudioMute exec --no-startup-id pactl set-sink-mute @DEFAULT_SINK@ toggle &amp;&amp; $refresh_i3status
bindsym XF86AudioMicMute exec --no-startup-id pactl set-source-mute @DEFAULT_SOURCE@ toggle &amp;&amp; $refresh_i3status

拼音输入法我在Gnome下使用的是Ibus-rime框架的小鹤双拼，Gnome下开箱即用，但是i3需要做一些简单的配置，更改一些环境变量。

在自己的家目录 : touch .xprofile

添加内容如下：
export GTK_IM_MODULE=ibus
export QT_IM_MODULE=ibus
export XMODIFIERS=@im=ibus

使用source读取配置文件中的环境变量，使环境变量生效。尝试输出echo $XMODIFIER查看是否已经生效，输出空值的话ibus还是无法使用。

结合上面的i3配置文件，使用ibus-daemon -dr启动，不要XIM。如果你使用了-x启动，在浮动的输入法工具栏中会显示一个禁止的标志，说明拼音未正常工作。


配置Polybar我基本上使用的是默认的Bar配置文件，还比较方便。

复制一个配置文件的模板到家目录的文件夹下：
cp -p /usr/share/doc/ploybar/config .config/polybar/config

编辑一个启动脚本
touch polybar_startup.sh

写入如下内容
#!/bin/bash
## kill all old process of ploybar
killall -q polybar
while pgrep -u $UID -x polybar > /dev/null; do sleep 1; done

echo "---" | tee -a /tmp/polybar.log
polybar example &amp; /tmp/polybar.log 2>&amp;1 &amp;
echo "Polybar launched"

结合上面的i3配置进行开机自动运行即可。我觉得默认的这个还行，可以日常使用了，不调整了，太费劲了。


目前还未解决的问题无法在terminal中输入中文目前中文的输入法还是有些问题，无法在alacritty中输入中文，可能和ibus的关系比较大，我尝试使用fctix完全没有任何问题，所有的地方都可以输入中文，所以选择什么方式输入中文，各有所好吧。但是，Gnome环境和i3的环境是会冲突的，所以同时安装ibus和fcitx要考虑一下。

Alacritty中无法输入中文的问题已经解决了！

那么解决的方法如下：
​	在&#x2F;etc&#x2F;profile文件中（或者.zshrc中），添加如下的环境变量
export GTK_IM_MODULE=ibus
export XMODIFIERS=@im=ibus
export QT_IM_MODULES=ibus

之后 ，
source /etc/profile
source ~/.zshrc 

重新启动alacritty就可以使用了。
我想root cause ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/10/18/Linux/Linux_k8s-basic-5/" title="Kubernetes集群的学习笔记(5)">Kubernetes集群的学习笔记(5)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-10-18T03:48:48.000Z" title="发表于 2019-10-18 11:48:48">2019-10-18</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.746Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">K8S集群的存储卷笔记。
整体的存储卷调用结构：
在k8s的集群中，Pod声明自己需要存储卷资源，同时创建自己的PVC，PVC绑定到集群中已经注册的PV资源，例如 已经建立的NFS网络空间。PV资源通过存储系统的分配，直接提供给Pod来使用。
PVC属于名称空间级别，PV属于集群资源。
存储卷的类型
EmptyDir：只在Node上存在的存储卷，Pod删除的时候存储卷也会被移除，无法持久存储，叫做EmptyDir，做临时存储和缓存使用，可以使用Node的内存。

HostPath： Docker的存储卷类型，Node节点上的目录。

网络存储： SAN， NAS; 分布式存储(Glusterfs，Cephfs，rbd); 云存储(EBS， Azure Disk，特定的托管在云上的服务)
kubectl explain pod.spec.volumes

 可以查看支持那些存储。


PVC对于用户来说，无法掌握所有的存储系统的知识和技能，因此，创建了PVC的逻辑层，在定义需要使用存储卷的Pod中只需定义需要的空间以及存储的类型，不需要详细的考虑后端存储的信息和配置。
PVC被定义在Pods的配置中，一个 PVC可以被多个Pod同时访问。
PV存储类： Gold Storage Class, Sliver Storage Class, Bronze Stroage Class.三个不同级别的存储类。存储类直接接受PVC的内容并对PVC定义的容量等信息进行分配。
PV与存储服务的提供者直接绑定且需要在存储服务提供方配置完成。PV与PVC具有一一对应的关系。
PersistentVolumegitRepo仓库的使用：
gitRepo存储卷建立在EmptyDir的基础上，在Pod内建立空目录，同步Git的内容到空目录，Pod运行的过程中不会更改存储卷上的内容。也就是说，git同步是pod建立的时候同步的数据，不会对git项目的数据进行更改。Pod更改了存储卷中的数据不会自动推送到Git上（可以使用Sidecar进行推送和配置）。
PV资源的定义---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv001
  labels:
    name: pv001
spec:
  nfs:
    path: &#x2F;data&#x2F;volumes&#x2F;v1
    server: storage1.liarlee.com
  accessModes: [&quot;ReadWriteMany&quot;,&quot;ReadWriteOnce&quot;]
  capacity:
    storage: 20Gi

使用apply命令进行应用即可。
查看所有的PV使用： kubectl get pv ， 其中显示了PV的名称，大小，访问模式，回收策略，状态，建立时间等。
PersistentVolumeClaim使用的过程中，在Pod中定义PVC；PVC和PV是一一对应的，但是PVC可以被多个Pod调用和挂载。一个PV会Binding一个PVC，PVC在Pod中被定义。
PVC以及PV的状态，未绑定的状态叫做Pending，绑定后叫做Bound。
PVC的定义---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: pvc001
  namespace: default
spec:
  accessModes: [&quot;ReadWriteMany&quot;]
  resource:
    request:
      storage: 6Gi
  

ConfigMap资源configmap是明文资源，secret是base64的编码资源，因此安全程度提高了。configmap相当于外挂的配置文件，当Pods启动的时候直接挂载Configmap读取自己需要的配置内容。
配置应用容器化的方式
自定义命令行参数
配置文件直接放入镜像中
通过环境变量进行配置
存储卷
Cloud Native应用通过环境变量直接配置
通过EntryPoint脚本预处理环境变量作为配置问文件中的信息


docker config命令行方式

Configmap的创建configmap为了将配置文件中镜像中解耦，configmap可以直接注入到容器中直接使用，注入的方式可以使用存储卷，或者使用EntryPoint来处理。
通过命令直接传递键值：
kubectl create configmap cm-nginx --from-literal=nginx_server_name=nginx.liarlee.com --from-literal=nginx_server_port=80
kubectl create configmap cm-nginx --from-file=./nginx.conf # 直接传递文件到ConfigMap
kubectl get cm cm-nginx -o yaml # 用YAML的格式查看ConfigMap

通过YAML文件：
---
apiVersion: v1
kind: ConfigMap
data: 
  nginx.conf................................
metadata:
  name: cm-nginx
  namespace: default

创建Pod时ConfigMap使用环境变量方式：
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
  namespace: default
  labels:
    app: pods-nginx
    tier: frontend
  annotations:
    liarlee.com&#x2F;created-by: &quot;cluster admin&quot;
spec:
  containers:
  - name: pod-nginx
    image: nginx:latest
    ports:
    - name: http
      containerPort: 80
    env:
    - name: NGINX_SERVER_PORT
      valueFrom:
      	configMapKeyRef:
      		name: cm-nginx
      		key: nginx_port
    env:
    - name: NGINX_SERVER_NAME
    	valueFrom:
    		configMapKeyRef:
    			name: cm-nginx
    			key: nginx_server_name

创建Pod的时候使用挂载卷的方式：
通过挂载的方式可以通过修改configmap的方式，同步修改容器内部的配置。
---
apiVersion: v1
kind: Pod
metadata:
  name: pod-nginx
  namespace: default
  labels:
    app: pods-nginx
    tier: frontend
  annotations:
    liarlee.com&#x2F;created-by: &quot;cluster admin&quot;
spec:
  containers:
 	- name: pod-nginx
   	  image: nginx:latest
      ports:
      - name: http
      	containerPort: 80
 	  volumeMounts:
 	  - name: nginxconf
 	  	mountPath: &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;
 	  	readOnly: true
 	Volumes:
 	- name: nginxconf
 	  configMap: 
 	  	name: cm-nginx 

创建Secret的类型kubectl create secret –help

docker-registry – 连接到Docker私有仓库的密钥
generic – 通用的服务密钥
tls – 创建证书

StatefulSet的建立和使用在一定程度上实现了有状态的管理，但是依旧需要写成管理脚本，注入到Pod中。
CoreOS – 提供了 Operator相关的功能用来完善有状态的应用的部署。
两类不同的Pod

Cattle

Pet


最早叫做PetSet， 后面改成StatefulSet。
StatefulSet一般用于管理以下特性的组件：

稳定且有唯一的网络标识符；必须保持Pod的名称和地址稳定持久有效；

稳定且持久的存储；

有序平滑(Graceful)的部署及扩展；启动的时候Pod1 - Pod8；

有序平滑(Graceful)的终止和删除；关闭的时候Pod8 - Pod1；

有序的滚动更新；有顺序的进行Pod的更新；


有三个主要的部分；
  1. Headless Service
     - 类似于Redis，提供一个headless的服务，来确保每一个请求直达后端的pod，可保持pod的接入点稳定且不发生变化。
  2. StatefulSet Controller
     - 需要一个控制器来进行Pod的管理和控制，保持Pod的生命周期，即使Pod被终止也需要在启动后保持和之前一样的Pod信息。
  3. Volume Claim Template
     - 由于有状态的服务不能同时使用同一个PV，所有的节点存储的数据各不相同，所以不能提供一个PVC&amp;PV。因此提供了申请PV的模板，每个Pod提供一个独立的存储卷用来做独立存储。

对于StatefulSet来说，确保所有的Pod名称稳定有效不可变动。大多数有状态的副本都会使用持久存储，多个Pod能不能共用同一个存储？ 不能，每个Pod必须使用不同的存储。
 所以，需要定义PV，定义PVC模板，定义Pod，定义Stateful控制器，定义headless服务这几种。
定义StatefulSet的YAML文件---
apiVersion: v1
kind: Service 
metadata:
  name: sts-headless-svc
  labels:
    Service: sts-headless-svc
spec:
  ports:
  - port: 80
    name: sts-headless-svc
  clusterIP: None
  selector: 
    service: sts-headless-svc

---
apiVersion: app&#x2F;v1
kind: StatufulSet
metadata:
  name: sts
spec:
  serviceName: sts
  replicas: 3
  selector:
    matchlabels:
      service: sts-headless-svc
  template:
    metadata:
      labels:
        service: sts-headless-svc
   	spec: 
   		containers: 
   		- name: sts-container
   		  image: ikubernetes&#x2F;SOMEIMAGE&#39;SNAME
   		  ports:
   		   ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/10/14/Linux/Linux_k8s-basic-4/" title="Kubernetes集群的学习笔记(4)">Kubernetes集群的学习笔记(4)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-10-14T06:37:10.000Z" title="发表于 2019-10-14 14:37:10">2019-10-14</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.745Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">K8S Service资源的笔记以及Ingress资源的笔记。
k8s的serviceservice的模型： userspace(kube-proxy), iptables, ipvs 
Service的类型请求发送的过程：

Client –&gt; Node’s IP:Node’s Port –&gt; Cluster’s IP:ServicePort –&gt; PodIP: ContainerPort

Service的类型：

ClusterIP是将提供服务的Pods统一建立一个集群内部的可访问接口，为集群内部的服务提供入口，PodsPort to ClusterIP:Port.

NodePort是将Node的端口映射出去的方式，每个节点各自对外提供一组IP:Port用来对外提供服务 - PodsPort to NodePort.

LoadBalancer使用LBaaS的方式对外提供服务，共有云环境可用，例如阿里云。

Externalname可将外部的服务引入到集群的内部，需要提供的字段是外部网络的真正的DNS服务的CNAME（FQDN）。

HeadlessService是不提供ClusterIP，可将ServiceName直接解析到PodsIP。


上面的每一个类型的服务都是顺序增强的，也就是说，基础的模式是 ClusterIP 。
HTTPS的处理和思路
启动一个Pod对HTTPS进行卸载和LB
如果需要对HTTPS进行配置，Kubernetes本身提供的Service不能提供7层协议的卸载(将HTTPS卸载为HTTP与后端做通信以及数据交换),那么可行的方案是，建立一个新的Pod，例如nginx，由nginx-pod进行HTTPS的代理和卸载操作，但是这样的话就会有如下的流程： 
User Request –&gt; LBaaS(LB) –&gt; Service - NodePort(LB) –&gt; Pod - nginx proxy(LB) –&gt; BackendPods
PROBLEM: 在这种情况下，用户的请求需要进行两次负载的转换才可以到达Pod，开销太大。

将Nginx的Pod与Node的Net名称空间进行共享
使得Nginx的Pod直接工作在Node的网卡级别上。免去了K8S的Service中间的一次转发。为了避免单点的故障，可将DeamonSet将Pod运行在一部分节点上。


Ingress Controller可以用于处理和卸载HTTPS协议的负载均衡器Pod控制器。
K8S四个附件： DNS， DashBoard， Ingress Controller,  heasper。
如上的那种NginxPod，衍生为Ingress Controller， 独立运行的应用程序组成，不属于Controller Manager。通常由4种选择：HAProxy，Nginx，Envoy，Traefik。微服务使用更多的是Envoy。
Ingress资源定义IngressController如何建立一个期望的前端（Nginx URL Rewrite），同时定义了期望的后端(Upstream servers)，更新后端负载Pod的信息。
Ingress类型Ingress资源的定义格式使用NginxServer的方式进行配置IngressPod
apiVersion: Extensions&#x2F;v1beta1
kind: Ingress
metadata:
  name: ingress-nginx
  namespace: default
  annotations: 
    kubernetes.io&#x2F;ingress.class: &quot;nginx&quot;
spec:
  rules:
  - host: nginx.test.local
    http:
    	paths:
    	- path:
    	  backend:
    		serviceName: svc-nginx
    		servicePort: 80 		 	

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/10/10/Linux/Linux_VIM_tricks/" title="Linux及vim的技巧">Linux及vim的技巧</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-10-10T03:48:58.000Z" title="发表于 2019-10-10 11:48:58">2019-10-10</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.742Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Linux的使用技巧已经vim的常用设置及插件。


1. 删除目录下的所有文件夹，但是保留所有文件find .&#x2F; -type d ! -name . | xargs rm -rf
2. 设置vim的个性化设置设置自动显示行号，设置VIM自动将tab转化为4个空格
:set nu
:set tabstop&#x3D;4
:set softtabstop&#x3D;4
:set shiftwidth&#x3D;4
:set expandtab

3. 已经编辑的文件进行tab空格转换：
TAB替换为空格：:set ts=4
:set expandtab
:%retab!
空格替换为TAB：:set ts=4
:set noexpandtab
:%retab!

4. Vim的个性化配置及插件
自动换行
set wrap

输入命令时自动提示补完
set showcmd

显示行号
set nu

显示当前行号及关联前后行号
set relativenumber

语法高亮
syntax on 

光标所在的行高亮
set cursorline

前后滚动时保留最前和最后的5行
set scrolloff&#x3D;5


</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/10/09/Linux/Linux_k8s-namespace-delete-terminating/" title="k8s所有的NS删除的时候都进入Terminating状态">k8s所有的NS删除的时候都进入Terminating状态</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-10-09T03:17:05.000Z" title="发表于 2019-10-09 11:17:05">2019-10-09</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.747Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">集群无法删除Namespace解决方式。
Namespace 无法删除 始终处于Teminating强制删除的方法，临时方案。
将名称空间的配置文件导出。kubectl get namespace testtest -o json &gt; tmp.json

编辑这个临时文件。vim tmp.json

删除spec字段中的值。
&quot;spec&quot; : &#123;
        &quot;finalizers&quot; : [		# delete this line.
            &quot;kubernetes&quot;		# delete this line.
            ]		# delete this line.
        &#125;
``` 			


使用另一个terminal， 运行本地的proxy， 连接到API server。 kubectl proxy --port=8888

通过ApiServer进行删除 curl -k -H &quot;Content-Type: application/json&quot; -X PUT --data-binary @tmp.json http://127.0.0.1:8001/api/v1/namespaces/[NEEDTODELETENS]/finalize;   
 这里面http的调用路径在 ： tmp.json的 api 字段中。

运行结果返回NameSpace的相关信息应该就是删除了。


Namespace删除卡住的原因
Solution From Github: https://github.com/kubernetes/kubernetes/issues/60807

是某些服务的问题导致了无法删除掉相关的NS

kubectl get apiservice | grep False
kubectl api-resources –verbs&#x3D;list –namespaced -o name | xargs -n 1 kubectl get -n [NEEDTODELETENS]
kubectl delete apiservice v1alpha3.kubevirt.io

其实是这个apiservice影响的，他的状态不正常导致的NS删除的时候卡住，删除这个apiservice就可以了。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/30/Linux/Linux_Cobbler-k8s-reposync/" title="Linux_Cobbler搭建本地YUM源同步k8s阿里云">Linux_Cobbler搭建本地YUM源同步k8s阿里云</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-30T03:02:49.000Z" title="发表于 2019-09-30 11:02:49">2019-09-30</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.726Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">昨天晚上尝试使用阿里云的时候出现问题 ，阿里云的k8s源安装的时候报错，无法正常通过yum安装。内网正好放了一台Cobbler，所以直接从Cobbler同步阿里的repo过来放到内网，防止这个事情再次发生。
Cobbler是什么
Cobbler是一个免费开源系统安装部署软件，用于自动化网络安装操作系统。 Cobbler 集成了 DNS, DHCP,[1][2]软件包更新， 带外管理以及配置管理， 方便操作系统安装自动化。Cobbler 可以支持PXE启动, 操作系统重新安装, 以及虚拟化客户机创建，包括Xen, KVM or VMware. Cobbler透过koan程序以支持虚拟化客户机安装。Cobbler 可以支持管理复杂网路环境，如创建在链路聚合以太网的桥接环境。 FROM Wikipedia

Cobbler Repo的建立k8s的源，Cobbler直接建立的同步不可以是因为k8s的目录结构和一般软件源的结构不同。（开始以为阿里云会一直保持带有Pool文件夹的那个结构， 今天早上看到结构已经和普通的yumrepo一样了，记录一下出现这种问题怎么办好了。）其实解决的方案就是手动同步，使用Cobbler进行源的发布。其实也就是httpd发布出去。

建立阿里云的源[root@cobbler &#x2F;]# cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo
[kubernetes]
name&#x3D;Kubernetes
baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;
enabled&#x3D;1
gpgcheck&#x3D;1
repo_gpgcheck&#x3D;1
gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg
EOF
[root@cobbler &#x2F;]# mv kubernetes.repo &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo
[root@cobbler &#x2F;]# yum clean all
[root@cobbler &#x2F;]# yum makecache
[root@cobbler &#x2F;]# yum repolist
# 在repolist中记录repoid
手动同步源[root@cobbler &#x2F;]# reposync -n --repoid&#x3D;kubernetes -p &#x2F;var&#x2F;www&#x2F;cobbler&#x2F;repo_mirror&#x2F; --allow-path-traversal
手动将已经同步好的目录创建为repo[root@cobbler &#x2F;]# createrepo &#x2F;var&#x2F;www&#x2F;cobbler&#x2F;repo_mirror&#x2F;kubernetes&#x2F;
最后编辑一下自己的kubernetes.repo源文件，只指向本地的源就可以了。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/24/Linux/Linux_k8s-basic-3/" title="Kubernetes集群的学习笔记(3)">Kubernetes集群的学习笔记(3)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-24T09:30:50.000Z" title="发表于 2019-09-24 17:30:50">2019-09-24</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.745Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">k8s笔记YAML格式定义资源。


通过YAML定义PodsapiVersionkubectl api-versions查看所有可用的组名apiVersion:[Group&#x2F;Version]
Kind 资源类别Metadata 元数据
name: uniq Key
namespace
labels Key-Value
annotations 
SelfLink: 资源引用的链接API格式：&#x2F;api&#x2F;group&#x2F;verison&#x2F;namespaces&#x2F;namespace&#x2F;type&#x2F;name



Speckubectl explain pods.spec可使用命令查看：定义用户期望的目标状态。
Status自动维护即可 ，不需要更改。
简单的YAML实例apiVersion: v1 
kind: Pod
metadata:
    name: myapp-pod
    labels:
        app: myapp
        version: v1
spec: 
    containers:
    - name: app
      image: nginx
   - name: php-fpm
      image: php-fpm

对Pods进行标签操作
查看标签kubectl get pods –show-labelskubectl get pods -l LABEL_NAME –show-labels

增加标签kubectl label pods pod-demo KEY VALUE

修改标签kubectl label pods pod-demo KEY VALUE –overwirte

指定selector选择标签 - 等值关系， 集合关系的标签选择器。kubectl get pods -l release&#x3D;stablekubectl get pods -l release!&#x3D;stablekubectl get pods -l “release in (v1, v2, v3)”kubectl get pods -l “release notin (v1, v2, v3)”
许多资源支持内嵌字段，matchLabels(直接给定键值) ， matchExporession(基于给出的表达式进行选择)。常用的操作符号，In ; Notin ； Exists；NotExists；


Pod的生命周期
初始化容器init c(初始化主容器的执行环境)，可以有多个，串行执行，直到最后一个init c执行结束。
main c在所有的初始化完成之后开始启动，在容器的运行时间与main c的执行时间基本一致；main c刚刚启动之后，可以手动执行Poststart；结束之前可以进行Prestop；
在Pod运行的过程中，提供Pod的Liveness probe； 提供Pod的Readiness probe。

常见的Pod状态：

Pending： 挂起，调度尚未完成；
Running： 运行状态；
Failed： 失败；
Succeeded： 成功；
Unknown: kubelet失去联络或者无法获取Pod信息时；

创建Pod的阶段：

创建提交请求给API server，目标状态保存到etcd;
API Server 请求 Schduler 进行Pod的调度；
API取得Pod的调度结果后，将信息记录到etcd；
Node节点获取到API server上的Pod状态更新后，开始按照调度的信息进行Pod的建立。

Pod生命周期中的重要行为：

初始化容器： init container ; 
容器探测： liveness ,readiness;

容器的重启策略： restartPolicyAlways, OnFailure, Never; 三种策略中默认的设置时Always.
Pod的终止过程：

发送term信号， 默认等待30s，如果30s还未终止就强制终止。

存活检测可通过三种类型进行探测： ExecAction, TCPSocketAction, HTTPGetAction;
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/24/Linux/Linux_k8s-changelooplvm-directlvm/" title="K8S将loop-lvm改为direct-lvm说明">K8S将loop-lvm改为direct-lvm说明</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-24T03:10:36.000Z" title="发表于 2019-09-24 11:10:36">2019-09-24</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.746Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">对k8s集群进行存储驱动的调整，从loop-lvm 切换到direct-lvm。
k8s的几种不同的存储驱动
AUFS - 这是一个经过时间检验的存储驱动
DeviceMapper - Redhat系默认的驱动，有loop和direct两种不同配置
Btrfs - 我…. 这个文件系统的快照真的是贼好用，但是性能什么的….我倒觉得都一般
ZFS - 还没用过
VFS - 还没用过
Overlay2 - 简单的接触了一下，docker目前推荐的存储驱动

关于存储驱动选择的相关博客及文章
Docker引擎 - 选择存储驱动Docker五种存储驱动原理及应用场景和性能测试对比Docker系统八：Docker的存储驱动 

Loop-lvm这是docker默认安装之后的选择，因为这样可以out-of-box，但是据说稳定性不佳，我没遇到稳定性的问题，但是遇到了IO高导致的整个虚拟机运行缓慢。Loop-LVM其实使用了linux中的使用loop设备我之前安装的一套k8s默认是使用overlay2的存储，可能是内核的版本过低导致无法使用其他的存储驱动，所以我觉得默认使用了loop-lvm。

loop-lvm的工作模式是，默认在&#x2F;var&#x2F;lib&#x2F;docker&#x2F;devicemapper&#x2F;devicemapper&#x2F;目录下生成data&amp;metadata两个稀疏文件（我目前还不知道什么叫做稀疏文件），并将两个文件挂载为loop设备做为块设备来使用。按照这个说法的话确实如果直接对裸设备的想能和稳定性都应该更强。所以下面可以动手啦~

Direct-lvm这里直接放一个官方文档的链接好了。Device-Mapper-driver
切换loop-lvm到direct-lvm想要切换的原因其实是已经安装好的这个k8s的master节点在跑了很久之后，总是被zabbix监控到报警，硬盘负载高；cpu进程数高。cpu的进程数量高可以理解，毕竟监控了如此多的容器。硬盘负载高这个报警在系统中发现是&#x2F;dev&#x2F;loop2这个设备。对应去查找 docker info中的信息，发现这是k8s的存储所使用的。进而搜索到了关于loop-lvm&amp;direct-lvm的相关问题，发现使用loop设备的方式应该实在性能上有影响的，k8s不推荐生产环境使用，所以考虑切换过来，今后毕竟还是要长期使用的。值得注意的是：切换一定会导致之前的容器无法使用。而且目前来看关键的数据是不能恢复的，所以最好是在之前已经做好了计划。
自动托管配置自动托管的配置主要是两部分，首先是建立一个空的LVM，不需要挂载，只要系统识别到即可。之后是更改配置文件及重启docker服务。

在虚拟机中加入一块新的硬盘，fdisk中识别为&#x2F;dev&#x2F;sdb；
关闭docker。systemctl stop docker.
在配置文件中加入如下的配置，注意格式不要错，不要丢下末尾的逗号：&#123;
  &quot;storage-driver&quot;: &quot;devicemapper&quot;,   # 告诉docker应用使用的存储驱动
  &quot;storage-opts&quot;: [
    &quot;dm.directlvm_device&#x3D;&#x2F;dev&#x2F;sdb&quot;,	# 指定使用的块设备，不需要格式化，不需要分区。在这里指定了设备docker会自动完成创建LVM等等操作。
    &quot;dm.thinp_percent&#x3D;95&quot;,	# 指定Thinpool占用的百分比
    &quot;dm.thinp_metapercent&#x3D;1&quot;,	# 指定Thinpool Meta数据使用的百分比
    &quot;dm.thinp_autoextend_threshold&#x3D;80&quot;,	# 指定自动扩容的阈值
    &quot;dm.thinp_autoextend_percent&#x3D;20&quot;,	# 指定自动扩容的比例
    &quot;dm.directlvm_device_force&#x3D;false&quot;	# 是否强制格式化设备，默认是false。如果使用dockerd启动的时候出现了提供需要强行格式化设备的提示，就改为True。
  ]
&#125;
重新启动docker服务。如果正常启动可通过docker info 查看是否已经切换过来。
如果没有能成功启动，尝试重启虚拟机；尝试使用dockerd命令直接启动，根据dockerd的日志信息进行相应的修改。
常见错误有：Sep 23 18:38:03 k8s-master dockerd: time&#x3D;&quot;2019-09-23T18:38:03.931876136+08:00&quot; level&#x3D;warning msg&#x3D;&quot;[graphdriver] WARNING: the devicemapper storage-driver is deprecated, and will be removed in a future release&quot;
# 存储驱动将会在未来的版本被移除的警告。这不会导致docker无法运行。
Sep 23 18:25:52 localhost dockerd: Error starting daemon: error initializing graphdriver: &#x2F;dev&#x2F;sdb is already part of a volume group &quot;docker&quot;: must remove this device from any volume group or provide a different device
# 这个问题说明 docker 认为你的&#x2F;dev&#x2F;sdb上已经被创建了LVM，你需要手动指定，自动托管不会对这个设备进行操作。这会导致docker无法启动。
# 首先你需要把&#x2F;dev&#x2F;sdb这个设备从LVM里面移除，lvdelete,pvdelete,vgdelete, 将设备还原为默认的状态，之后重启docker，将设备的所有操作控制都交给docker来做，就不会有这个错误了。

总结在更换之后目前性能稳定，IO的负载也下来了。总体来看还是不错的。不过随着kubernetes的发展，我觉得这种问题应该会越来越少。还是推荐在安装的时候直接调整，不然数据的随时确实带来了一些麻烦。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/23/Linux/Linux_RabbitMQ-Problems/" title="RabbitMQ_SysV风格管理脚本模板">RabbitMQ_SysV风格管理脚本模板</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-23T07:48:52.000Z" title="发表于 2019-09-23 15:48:52">2019-09-23</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.740Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">需要自己写一个RabbitMQ的SysV脚本，所以找了一个模板，如果需要的时候可以改改就用了。
SysV脚本#!/bin/sh
#
# rabbitmq-server RabbitMQ broker
#
# chkconfig: - 80 05
# description: Enable AMQP service provided by RabbitMQ
#
### BEGIN INIT INFO
# Provides:          rabbitmq-server
# Required-Start:    $remote_fs $network
# Required-Stop:     $remote_fs $network
# Description:       RabbitMQ broker
# Short-Description: Enable AMQP service provided by RabbitMQ broker
### END INIT INFO
# Source function library.
. /etc/init.d/functions
# 一些加注释的位置是需要修改的参数
PATH=/sbin:/usr/sbin:/bin:/usr/bin:/data/erlang/bin  # 更改PATH指向Erlang的路径
NAME=rabbitmq-server # 服务的名称，可以和脚本名称一致
DAEMON=/data/rabbitmq_server-3.6.13/sbin/$&#123;NAME&#125;	# 启动为守护进程的命令所在绝对路径
CONTROL=/data/rabbitmq_server-3.6.13/sbin/rabbitmqctl	# 制定rabbitmqctl程序的所在位置， 绝对路径
DESC=rabbitmq-server	# 目标服务
USER=root	# 运行时的用户， 线上服务都使用了root用户
export HOME=/data/rabbitmq_server-3.6.13/	# 指定RabbitMQ的HOME目录，默认是在安装目录下；也有可能是在运行RabbitMQ用户的家目录下
ROTATE_SUFFIX=
# INIT_LOG_DIR=/usr/local/rabbitmq/var/rabbitmq
INIT_LOG_DIR=/data/rabbitmq_server-3.6.13/var/log/rabbitmq	# 指出log的目录
# PID_FILE=/var/run/rabbitmq/pid	
PDI_FILE=/data/rabbitmq_server-3.6.13/var/lib/rabbitmq/mnesia/rabbit@ean-online-dubbo-zk-rmq-server-209.pid # 指出当前RabbitMQ的PID文件所在目录
START_PROG="daemon"
LOCK_FILE=/var/lock/subsys/$NAME 
test -x $DAEMON || exit 0
test -x $CONTROL || exit 0
RETVAL=0
set -e
[ -f /etc/default/$&#123;NAME&#125; ] &amp;&amp; . /etc/default/$&#123;NAME&#125;
[ -f /etc/sysconfig/$&#123;NAME&#125; ] &amp;&amp; . /etc/sysconfig/$&#123;NAME&#125;
ensure_pid_dir () &#123;
    PID_DIR=`dirname $&#123;PID_FILE&#125;`
    if [ ! -d $&#123;PID_DIR&#125; ] ; then
        mkdir -p $&#123;PID_DIR&#125;
        chown -R $&#123;USER&#125;:$&#123;USER&#125; $&#123;PID_DIR&#125;
        chmod 755 $&#123;PID_DIR&#125;
    fi
&#125;
remove_pid () &#123;
    rm -f $&#123;PID_FILE&#125;
    rmdir `dirname $&#123;PID_FILE&#125;` || :
&#125;
start_rabbitmq () &#123;
    status_rabbitmq quiet
    if [ $RETVAL = 0 ] ; then
        echo RabbitMQ is currently running
    else
        RETVAL=0
        # RABBIT_NOFILES_LIMIT from /etc/sysconfig/rabbitmq-server is not handled
        # automatically
        if [ "$RABBITMQ_NOFILES_LIMIT" ]; then
                ulimit -n $RABBITMQ_NOFILES_LIMIT
        fi
        ensure_pid_dir
        set +e
        RABBITMQ_PID_FILE=$PID_FILE $START_PROG $DAEMON \
            > "$&#123;INIT_LOG_DIR&#125;/startup_log" \
            2> "$&#123;INIT_LOG_DIR&#125;/startup_err" \
            0&lt;&amp;- &amp;
        $CONTROL wait $PID_FILE >/dev/null 2>&amp;1
        RETVAL=$?
        set -e
        case "$RETVAL" in
            0)
                echo SUCCESS
                if [ -n "$LOCK_FILE" ] ; then
                    touch $LOCK_FILE
                fi
                ;;
            *)
                remove_pid
                echo FAILED - check $&#123;INIT_LOG_DIR&#125;/startup_\&#123;log, _err\&#125;
                RETVAL=1
                ;;
        esac
    fi
&#125;
stop_rabbitmq () &#123;
    status_rabbitmq quiet
    if [ $RETVAL = 0 ] ; then
        set +e
        $CONTROL stop $&#123;PID_FILE&#125; > $&#123;INIT_LOG_DIR&#125;/shutdown_log 2> $&#123;INIT_LOG_DIR&#125;/shutdown_err
        RETVAL=$?
        set -e
        if [ $RETVAL = 0 ] ; then
            remove_pid
            if [ -n "$LOCK_FILE" ] ; then
                rm -f $LOCK_FILE
            fi
        else
            echo FAILED - check $&#123;INIT_LOG_DIR&#125;/shutdown_log, _err
        fi
    else
        echo RabbitMQ is not running
        RETVAL=0
    fi
&#125;
status_rabbitmq() &#123;
    set +e
    if [ "$1" != "quiet" ] ; then
        $CONTROL status 2>&amp;1
    else
        $CONTROL status > /dev/null 2>&amp;1
    fi
    if [ $? != 0 ] ; then
        RETVAL=3
    fi
    set -e
&#125;
rotate_logs_rabbitmq() &#123;
    set +e
    $CONTROL rotate_logs $&#123;ROTATE_SUFFIX&#125;
    if [ $? != 0 ] ; then
        RETVAL=1
    fi
    set -e
&#125;
restart_running_rabbitmq () &#123;
    status_rabbitmq quiet
    if [ $RETVAL = 0 ] ; then
        restart_rabbitmq
    else
        echo RabbitMQ is not runnning
        RETVAL=0
    fi
&#125;
restart_rabbitmq() &#123;
    stop_rabbitmq
    start_rabbitmq
&#125;
case "$1" in
    start)
        echo -n "Starting $DESC: "
        start_rabbitmq
        echo "$NAME."
        ;;
    stop)
        echo -n "Stopping $DESC: "
        stop_rabbitmq
        echo "$NAME."
        ;;
    status)
        status_rabbitmq
        ;;
    rotate-logs)
        echo -n "Rotating log files for $DESC: "
        rotate_logs_rabbitmq
        ;;
    force-reload|reload|restart)
        echo -n "Restarting $DESC: "
        restart_rabbitmq
        echo "$NAME."
        ;;
    try-restart)
        echo -n "Restarting $DESC: "
        restart_running_rabbitmq
        echo "$NAME."
        ;;
    *)
        echo "Usage: $0 &#123;start|stop|status|rotate-logs|restart|condrestart|try-restart|reload|force-reload&#125;" >&amp;2
      ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/19/Other/Books_All%20this%20I%20did%20without%20you/" title="All this I did without you">All this I did without you</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-19T12:48:05.000Z" title="发表于 2019-09-19 20:48:05">2019-09-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.958Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Books/">Books</a></span></div><div class="content">English Text for exercise the Reading and listening.


Letter No. 028
July 31st, 1978
Gerald Durrell, a respected conservationist wrote a love letter to his future wife, and then one of his students taking her PhD at Duke University, Lee McGeorge.
~
My darling McGeorge,
You said that things seemed clearer when they were written down. Well, here with is a very boring letter in which I will try and put everything down so that you may read and re-read it in horror at your folly in getting involved with me. Deep breath.
To begin with I love you with a depth and passion that I have felt for no one else in this life and if it astonishes you it astonishes me as well. Not I hasten to say, because you are not worth loving. Far from it. It’s just that, first of all, I swore I would not get involved with another woman. Secondly, I have never had such a feeling before and it is almost frightening. Thirdly, I would never have thought it possible that another human being could occupy my waking (and sleeping) thoughts to the exclusion of almost everything else.
Fourthly, I never thought that — even if one was in love — one could get so completely besotted with another person, so that a minute away from them felt like a thousand years.
Fifthly, I never hoped, aspired, dreamed that one could find everything one wanted in a person. I was not such an idiot as to believe this was possible. Yet in you I have found everything I want: you are beautiful, gay, giving, gentle, idiotically and deliciously feminine, sexy, wonderfully intelligent and wonderfully silly as well. I want nothing else in this life than to be with you, to listen and watch you (your beautiful voice, your beauty), to argue with you, to laugh with you, to show you things and share things with you, to explore your magnificent mind, to explore your magnificent mind, to explore your wonderful body, to help you, protect you , serve you, and bash you on the head when I think you are wrong… not to put too fine a point on it I consider that I am the only man outside mythology to have found the crock of gold at the rainbow’s end.
But — having said all that — let us consider things in detail. Don’t let this become public but… well, I have one or two faults. Minor ones, I hasten to say. For example, I am inclined to be overbearing. I do it for the best possible motives (all tyrants say that) but I do tend (without thinking) to tread people underfoot. You must tell me when I am doing it to you, my sweet, because it can be a very bad thing in a marriage.
Right. Second blemish. This, actually, is not so much a blemish  of character  as a blemish of circumstance. Darling I want you to be you in your own right, and I will do everything I can to help you in this. But you must take into consideration that I am also me in my own right and that I have a headstart on you… what I am trying to say is that you must not feel offended if you are sometimes treated simply as my wife. Always remember that what you lose on the swings, you gain on the roundabouts. But I am an established ‘creature’ in the world, and so — on occasions — you will have to live in my shadow. Nothing gives me less pleasure than this but it is a fact of life to be faced.
Third (and very important and nasty) blemish: jealousy. I don’t think you know what jealousy is (thank God) in the real sense of the word. I know you have felt jealousy over Lincoln’s wife and child but this is what I call normal jealousy, and this — to my regret — is not what I’ve got. What I have got is a black moster that can pervert my good sense, my good humour and any goodness that I have in my make-up. It is really a Jekyll and Hyde situation… my Hyde is stronger than my good sense and defeats me, hard though I try. As I told you, I have always known that this lurks within me, but I couldn’t control it, and my monster slumbered and nothing happened to awake it. Then I met you and I felt my monster stir and become half awake when you told me of Lincoln and others you have known, and with your letter my monster came out of its lair, black, irrational, bigoted, stupid, evil, malevolent. You will never know how terribly corrosive jealousy is; it is a physical pain as though you had swallowed acid or red hot coals. It is the most terrible of feelings. But you can’t help it — at least I can’t, and God knows I’ve tried. I don’t want any ex-boyfriends sitting in church when I marry you. On our wedding day, I want nothing but happiness, for both you and me, and I know I won’t be happy if there is a church full of your ex-conquests. When I marry you I will have no past, only a future: I don’t want to drag my past into our future and I don’t want you to do it , either. Remember I am jealous of you because I love you. You are never jealous of something you don’t care about. OK, enough about jealousy.
Now, let me tell you something… I have seen a thousand sunsets and sunrises, on land where it floods forest and mountains with honey-coloured light, at s ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/19/Linux/Linux_Gnome-Tricks/" title="Gnome快捷键">Gnome快捷键</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-19T08:02:35.000Z" title="发表于 2019-09-19 16:02:35">2019-09-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.732Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Gnome桌面环境的快捷键。
快速启动一个应用
Super 
Hot Corner这两个我都用，HotCorner不是一个按键，而是一个动作，是指屏幕的右上角，鼠标指针用力撞过去，撞开所有的应用窗口。在Gnome的环境中，Super是一个超方便的键，当我需要打开vscode的时候，可以在任意时候通过super+code+Enter直接打开vscode应用。当按下Super的时候会自动触发一个全局的搜索，可以通过Super快速查看自己的需要的文件或者应用，这个功能是我最喜欢Gnome的地方。

执行命令
Alt+F2最常用的就是重启Gnome环境，通过Alt+F2 调出的命令窗口，使用r命令重启Gnome。

应用的切换
Super+TAB使用Super+TAB可以在应用之间快速切换。如果Cover-alt-tab的插件，还有三维动画。
Super+&#96;这个组合键是切换应用内窗口的，我自己用的不多。

快速显示主屏幕应用菜单
Super+a快速显示主屏幕的所有应用程序页。

切换工作区
Super+PageUP or PageDown
Ctrl+Alt+UP or DOWN可以在多个工作区之间快速切换，但是我自己常用的是第二种。键位上舒服一些。

移动窗口到其他工作区
Ctrl+Alt+Shift+UP or Down
Super+Shift+PageUP or PageDown将当前的焦点窗口移动到前后的工作区，并保持当前窗口的焦点不变。这个也是特别好用的快捷键。

呼出通知中心和日历
Super+m这个快捷键最早的Gnome上是通知栏，现在用的比较少了，毕竟Gnome已经不是下方的通知栏了，是上面的日历加上通知中心的方式了。我自己的用的时候大部分是看时间和日历。

截图
PrintScreen截取所有屏幕内容，保存到文件。
Alt+PrintScreen截取当前窗口的内容，保存到文件。
Shift+PrintScreen截取选择的区域，保存到文件。

NOTE： 最方便的就是，使用Ctrl+Shift PrintScreen和Ctrl+PrintScreen 可以直接选取的区域截图，将截图保存在剪贴板，截图完成直接粘贴即可。
最大化最小化和分屏
Super+UP最大化到铺满屏幕
Super+LEFT靠左占据一般屏幕
Super+RIGHT靠右占据一般屏幕
Super+H最小化，隐藏

这些基本上就是我觉得常用而且好用的快捷键啦，但是还是可以自定义的，在设置里面，也可以随便设置，但是那样的话就不是拿来直接可以用的啦。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/19/Linux/Linux_Nginx-if-try-files-Notwork/" title="Nginx配置文件中if判断与try_files">Nginx配置文件中if判断与try_files</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-19T01:52:13.000Z" title="发表于 2019-09-19 09:52:13">2019-09-19</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.736Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Nginx的if判断问题，导致try_files字段未能正常生效。
配置文件server &#123;
  listen 80;
  server_name liarlee.site;

  set $mobile_rewrite do_not_perform;
  if ($http_user_agent ~* &quot;(android|bb\d+|meego).+mobile|avantgo|bada\&#x2F;|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\&#x2F;|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino&quot;) &#123;
    set $mobile_rewrite perform;
  &#125;
  if ($http_user_agent ~* &quot;^(1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\&#x2F;|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\&#x2F;)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\&#x2F;)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\&#x2F;(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\&#x2F;|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\&#x2F;|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\&#x2F;|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo|to(pl|sh)|ts(70|m\-|m3|m5)|tx\-9|up(\.b|g1|si)|utst|v400|v750|veri|vi(rg|te)|vk(40|5[0-3]|\-v)|vm40|voda|vulc|vx(52|53|60|61|70|80|81|83|85|98)|w3c(\-| )|webc|whit|wi(g |nc|nw)|wmlb|wonu|x700|yas\-|your|zeto|zte\-)&quot;) &#123;
    set $mobile_rewrite perform;
  &#125;
  if ($http_cookie ~ &#39;gotopc&#x3D;true&#39;) &#123;
    set $mobile_rewrite do_not_perform;
  &#125;

  location &#x2F; &#123;
    root &#x2F;var&#x2F;html&#x2F;www&#x2F;[PC_WEB_ROOT];
     # 问题出现在了这里。
    if ($mobile_rewrite &#x3D; perform) &#123;
      root &#x2F;var&#x2F;html&#x2F;www&#x2F;[MOBILE_WEB_ROOT];
    &#125;
    index index.html index.htm
    error_page 404 index.html
    # 问题就出现在了这里。
    try_files $uri $uri&#x2F; &#x2F;index.html 404; 
  &#125;
&#125;
表现在访问请求来的同时，判断是不是手机访问，如果是PC使用默认的PC_WEB_ROOT,  如果是手机的话，访问到MOBILE_WEB_ROOT。本身逻辑和使用是没有问题的，但是需要使用try_files字段的时候，会导致PC站点的tryfiles可以正常生效；但是手机不会有tryfile的效果。
可行的方式 - Proxypass看到了一篇文章说到nginx的IF语句，可以正常不出奇怪问题的只有Proxy_pass,Rewrite两个，因为项目无法用rewrite所以选择了Proxypass。配置文件分为两个部分，每个网站放在一个Server下。
PC配置文件server &#123;
  listen 80;
  server_name liarlee.site;

  set $mobile_rewrite do_not_perform;
  if ($http_user_agent ~* &quot;(android|bb\d+|meego).+mobile|avantgo|bada\&#x2F;|blackberry|blazer|compal|elaine|fennec|hiptop|iemobile|ip(hone|od)|iris|kindle|lge |maemo|midp|mmp|mobile.+firefox|netfront|opera m(ob|in)i|palm( os)?|phone|p(ixi|re)\&#x2F;|plucker|pocket|psp|series(4|6)0|symbian|treo|up\.(browser|link)|vodafone|wap|windows ce|xda|xiino&quot;) &#123;
    set $mobile_rewrite perform;
  &#125;
  if ($http_user_agent ~* &quot;^(1207|6310|6590|3gso|4thp|50[1-6]i|770s|802s|a wa|abac|ac(er|oo|s\-)|ai(ko|rn)|al(av|ca|co)|amoi|an(ex|ny|yw)|aptu|ar(ch|go)|as(te|us)|attw|au(di|\-m|r |s )|avan|be(ck|ll|nq)|bi(lb|rd)|bl(ac|az)|br(e|v)w|bumb|bw\-(n|u)|c55\&#x2F;|capi|ccwa|cdm\-|cell|chtm|cldc|cmd\-|co(mp|nd)|craw|da(it|ll|ng)|dbte|dc\-s|devi|dica|dmob|do(c|p)o|ds(12|\-d)|el(49|ai)|em(l2|ul)|er(ic|k0)|esl8|ez([4-7]0|os|wa|ze)|fetc|fly(\-|_)|g1 u|g560|gene|gf\-5|g\-mo|go(\.w|od)|gr(ad|un)|haie|hcit|hd\-(m|p|t)|hei\-|hi(pt|ta)|hp( i|ip)|hs\-c|ht(c(\-| |_|a|g|p|s|t)|tp)|hu(aw|tc)|i\-(20|go|ma)|i230|iac( |\-|\&#x2F;)|ibro|idea|ig01|ikom|im1k|inno|ipaq|iris|ja(t|v)a|jbro|jemu|jigs|kddi|keji|kgt( |\&#x2F;)|klon|kpt |kwc\-|kyo(c|k)|le(no|xi)|lg( g|\&#x2F;(k|l|u)|50|54|\-[a-w])|libw|lynx|m1\-w|m3ga|m50\&#x2F;|ma(te|ui|xo)|mc(01|21|ca)|m\-cr|me(rc|ri)|mi(o8|oa|ts)|mmef|mo(01|02|bi|de|do|t(\-| |o|v)|zz)|mt(50|p1|v )|mwbp|mywa|n10[0-2]|n20[2-3]|n30(0|2)|n50(0|2|5)|n7(0(0|1)|10)|ne((c|m)\-|on|tf|wf|wg|wt)|nok(6|i)|nzph|o2im|op(ti|wv)|oran|owg1|p800|pan(a|d|t)|pdxg|pg(13|\-([1-8]|c))|phil|pire|pl(ay|uc)|pn\-2|po(ck|rt|se)|prox|psio|pt\-g|qa\-a|qc(07|12|21|32|60|\-[2-7]|i\-)|qtek|r380|r600|raks|rim9|ro(ve|zo)|s55\&#x2F;|sa(ge|ma|mm|ms|ny|va)|sc(01|h\-|oo|p\-)|sdk\&#x2F;|se(c(\-|0|1)|47|mc|nd|ri)|sgh\-|shar|sie(\-|m)|sk\-0|sl(45|id)|sm(al|ar|b3|it|t5)|so(ft|ny)|sp(01|h\-|v\-|v )|sy(01|mb)|t2(18|50)|t6(00|10|18)|ta(gt|lk)|tcl\-|tdg\-|tel(i|m)|tim\-|t\-mo| ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/18/Linux/Linux_CentOS7-VM-Template/" title="虚拟机制作模板的步骤及设置">虚拟机制作模板的步骤及设置</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-18T08:41:11.000Z" title="发表于 2019-09-18 16:41:11">2019-09-18</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.723Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/ESXi/">ESXi</a></span></div><div class="content">换了工作之后接管了这边旧的ESXi和上面的虚拟机，之前的模板不是特别的合适，自己开始动手做模板。最终期望的目标是：

修改IP地址
yum install 收工

CentOS虚拟机模板制作流程系统的硬件配置系统的硬件规格

4cpu 
8G-RAM

OS的版本系统的版本

CentOS 7.6 1810 x64

安装VMwareToolsyum install -y epel vim wget curl net-tools open-vm-tools htop iotop iftop tree atop sysstat
安装Zabbix-agentrpm -ivh zabbix-agent-$version.rpm 
关闭SELinuxsed -i 's@SELINUX=enforcing@SELINUX=disabled@g' /etc/selinux/config
更改hostnamehostname有一些有趣的问题，CentOS6.8 中的hostname可以定义在/etc/sysconfig/network文件中，系统启动的时候先读取/etc/sysconfig/network文件中的定义，如果没有的话读取/etc/hosts文件中的定义.和我自己的之前的记忆不一样。我之前一直都是之间编辑/etc/hostname，直接将主机名echo到这个文件中就可以了，不过想来这些区别都不大，先这样吧。
清除硬件及网卡的信息将网卡配置文件中的UUID和 HWADDR直接删除或者是注释掉，IPADDR留空。
/etc/sysconfig/network-scripts/ifcfg-eth0
删除udevrm -rf /etc/udev/rules.d/70-*
```  
NOTE：命令的效果貌似和 sys-unconfig 的效果是一样的。
### 关闭防火墙
```bash
systemctl disable firewalld
iptables -F
iptables -X
iptables -Z
更改Grub的等待时间虚拟机中的模板大部分是不需要等待grub给出的操作选单时间的，等待操作的时间是5秒，我们给出1秒就够了。
vim &#x2F;etc&#x2F;default&#x2F;grub
GRUB_TIMEOUT&#x3D;1
清理工作rm -rf &#x2F;etc&#x2F;ssh&#x2F;*key*.
rm -rf &#x2F;root&#x2F;.ssh&#x2F;
systemctl stop rsyslog
systemctl stop auditd
&#x2F;usr&#x2F;bin&#x2F;yum clean all
logrotate -f &#x2F;etc&#x2F;logrotate.conf 
rm -f &#x2F;var&#x2F;log&#x2F;dmesg.old 
rm -rf &#x2F;var&#x2F;log&#x2F;anaconda*
cat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;audit&#x2F;audit.log 
cat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;wtmp 
cat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;lastlog 
cat &#x2F;dev&#x2F;null &gt; &#x2F;var&#x2F;log&#x2F;grubby 
清除系统的唯一ID&gt; &#x2F;etc&#x2F;machine-id
清除系统命令历史记录unset HISTFILE
history -c &amp;&amp; rm -rf &#x2F;root&#x2F;.bash_histroy
Sys-unconfig关机执行sys-unconfig等待关机，然后转换为模板。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/17/Linux/Linux_Systemctl-ManageService/" title="通过systemd管理软件和服务">通过systemd管理软件和服务</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-17T06:30:30.000Z" title="发表于 2019-09-17 14:30:30">2019-09-17</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.741Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">通过systemctl 来管理系统的服务和软件，但是如果是自己安装的软件就没有办法使用了。其实是可以自己定义systemd的管理脚本的，类似与之前的SysV风格的管理脚本。
Systemd是什么
systemd is a suite of basic building blocks for a Linux system. It provides a system and service manager that runs as PID 1 and starts the rest of the system. systemd provides aggressive parallelization capabilities, uses socket and D-Bus activation for starting services, offers on-demand starting of daemons, keeps track of processes using Linux control groups, maintains mount and automount points, and implements an elaborate transactional dependency-based service control logic. systemd supports SysV and LSB init scripts and works as a replacement for sysvinit. Other parts include a logging daemon, utilities to control basic system configuration like the hostname, date, locale, maintain a list of logged-in users and running containers and virtual machines, system accounts, runtime directories and settings, and daemons to manage simple network configuration, network time synchronization, log forwarding, and name resolution.

管得还是挺多的，主要是启动PID为1的进程并启动其他的程序，并行执行，维护挂载点及自动挂载，服务之间的依赖关系，日志进程，
Systemd的Units文件Systemd默认的文件配置路径有：    - &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;*    - &#x2F;run&#x2F;systemd&#x2F;system&#x2F;*    - &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;*
Systemd Unit File 的模板有这样的几个模块：

[Unit]
[Service]
[Install]

UnitUnit的作用是记录文件的通用信息。

Descripition – 对软件或服务的描述。
Before OR After – 定义启动的顺序，在某些服务 启动之前 OR 启动之后 ，在启动这个服务。其中还定义了服务的依赖关系，先后顺序。常用的值有 network.target, Multi-User.target, network.service 等等。
Requires – 并行启动所指定的其他服务。
RequireOverrideable – 类似与require，但是不同的是手动启动的时候不会报错。
Requisite – 只要启动失败了就直接报错停止，强硬版本的requires。
Wants – 启动依赖单元的常用选项，在启动的同时调起其他的Unit，如果其他单元启动失败了也不会影响当前定义的Unit的启动。
Conflicts – 冲突单元，启动的时候发现了Conflict中定义的其他单元就会尝试终止Unit。

Service
Type – 对服务类型的定义，通常有如下三种：- simple 默认的类型，启动就启动，停止就结束了。- forking 守护进程的类型，把必要的启动之后留下守护进程。- oneshot 一次性的服务，启动后就结束了。
ExecStart – 启动的时候执行的命令， 这条命令就是服务的主体。
ExecStartPre OR ExecStartPost – ExecStart执行前后的动作。
ExecStop – 指定服务结束的动作，如果未指定直接kill。
Restart – 定义了重启的条件和动作，常用的参数有： no, on-sucess, on-failure, on-watchdog, on-abort。
SuccessExitStatus – ExecStart的返回值。 SuccessExitStatus=1 2 8 SIGKILL

Install
WantedBy – 定义启动的情景，几种不同的target: multi-user.target | poweroff.target | rescue.target | graphical.target | reboot.target
Alias – 别名的设置在这里定义。

标准配置文件 - Libvirtd[Unit]
Description&#x3D;Virtualization daemon
Requires&#x3D;virtlogd.socket
Requires&#x3D;virtlockd.socket
Wants&#x3D;systemd-machined.service
Before&#x3D;libvirt-guests.service
After&#x3D;network.target
After&#x3D;dbus.service
After&#x3D;iscsid.service
After&#x3D;apparmor.service
After&#x3D;local-fs.target
After&#x3D;remote-fs.target
After&#x3D;systemd-logind.service
After&#x3D;systemd-machined.service
Documentation&#x3D;man:libvirtd(8)
Documentation&#x3D;https:&#x2F;&#x2F;libvirt.org

[Service]
Type&#x3D;simple
EnvironmentFile&#x3D;-&#x2F;etc&#x2F;conf.d&#x2F;libvirtd
ExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;libvirtd $LIBVIRTD_ARGS
ExecReload&#x3D;&#x2F;bin&#x2F;kill -HUP $MAINPID
KillMode&#x3D;process
Restart&#x3D;on-failure
# At least 1 FD per guest, often 2 (eg qemu monitor + qemu agent).
# eg if we want to support 4096 guests, we&#39;ll typically need 8192 FDs
# If changing this, also consider virtlogd.service &amp; virtlockd.service
# limits which are also related to number of guests
LimitNOFILE&#x3D;8192
# The cgroups pids controller can limit the number of tasks started by
# the daemon, which can limit the number of domains for some hypervisors.
# A conservative default of 8 tasks per guest results in a TasksMax of
# 32k to support 4096 guests.
TasksMax&#x3D;32768

[Install]
WantedBy&#x3D;multi-user.target
Also&#x3D;virtlockd.socket
Also&#x3D;virtlogd.socket</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/09/11/Linux/Linux_ElasticSearch-01/" title="ElasticSearch 安装记录">ElasticSearch 安装记录</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-09-11T09:25:57.000Z" title="发表于 2019-09-11 17:25:57">2019-09-11</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.730Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/ElasticSearch/">ElasticSearch</a></span></div><div class="content">ElasticSearch的安装过程。
准备源码包
需要下载的包有三个：网站地址
ElasticSearch- 分布式、RESTful 风格的搜索和分析。&#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch
Logstash - 采集、转换、充实，然后输出。&#x2F;usr&#x2F;local&#x2F;src&#x2F;logstash
Kibana - 实现数据可视化。在 Elastic Stack 中进行导航。&#x2F;usr&#x2F;local&#x2F;src&#x2F;kibana



安装 ElasticSearch
解压下载的安装包
tar zxvf elasticsearch-7.3.1-linux-x86_64.tar.gz
tar zxvf kibana-7.3.1-linux-x86_64.tar.gz
tar zxvf logstash-7.3.1.tar.gz


修改系统参数
vim &#x2F;etc&#x2F;sysctl.conf
fs.file-max&#x3D;65535
vm.max_map_count&#x3D;262144


sysctl -p : 重新读取配置文件中的参数，更新的条目会显示在命令执行结果中。
vim &#x2F;etc&#x2F;security&#x2F;limits.conf * soft nofile 65536
* hard nofile 131072
* soft noproc 4096
* hard noproc 4096	
保存退出。
建立elk用户 useradd elk -p &quot;YOUR-PASSWD&quot;
将&#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch目录的权限给elk用户。 chown -R elk:elk &#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch&#x2F;
在&#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch&#x2F;elasticsearch&#x2F;config&#x2F;目录下修改配置文件elasticsearch.yml。 cluster.name: CLUSTER_NAME
node.name: HOSTNAME &amp; ROLES
node.master: true
node.data: true
network.host: YOUR_HOSTNAME
discovery.zen.ping.unicast.hosts: [&quot;YOUR_OTHER_NODE_HOSTNAME &quot;]

切换到elk用户，尝试启动elk su elk cd &#x2F;usr&#x2F;local&#x2F;src&#x2F;elasticsearch&#x2F;elasticsearch&#x2F;bin&#x2F; .&#x2F;elasticsearch 如果没有错误就可以使用 -d 选项将服务启动到后台。

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/07/24/Linux/Linux_FileManagement/" title="文件系统及文件管理">文件系统及文件管理</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-07-24T07:07:34.000Z" title="发表于 2019-07-24 15:07:34">2019-07-24</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.731Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Linux文件管理笔记
文件系统默认的规定是遵守FHS规定的。

FHS(Filesystem Hierarchy Standard) defines the directory structure and directory contents in Linux distributions.FHS_Website

标准的根文件系统，应该具有如下的结构，所有的文件目录均在根文件系统下。

&#x2F;bin  – 单用户模式下可运行的二进制命令。所有用户都可以使用。
&#x2F;sbin  – 基本的系统二进制文件。
&#x2F;boot – 一般为BootLoaderFiles，例如内核，ramfs，grub等等。
&#x2F;etc  – 常用的应用程序的全局配置文件。Host-specific system-wide configruation。
&#x2F;etc&#x2F;opt  – &#x2F;opt目录下的程序的配置文件。


&#x2F;usr  – 包含了主要的多用户工具及应用。
&#x2F;usr&#x2F;share – Architecture-independent(shared) data.独立架构的共享数据。
&#x2F;usr&#x2F;sbin – 非基础的系统库文件，例如网络管理的守护进程。
&#x2F;usr&#x2F;src – 放内核源码及头文件的目录。
&#x2F;usr&#x2F;bin – 非基础的命令二进制文件，不需要运行在单用户模式下的命令。
&#x2F;usr&#x2F;local – 特指存放本地的数据，例如源码包，二进制包等等。
&#x2F;usr&#x2F;lib – &#x2F;usr&#x2F;bin &amp; &#x2F;usr&#x2F;sbin下的命令所需要的库文件。


&#x2F;opt – Optional application software packages.
&#x2F;mnt – 文件系统的临时挂载位置。
&#x2F;media – 默认的可移动设备挂载位置。
&#x2F;dev  – 所有的设备，字符设备，块设备。
&#x2F;lib – 为&#x2F;bin &amp; &#x2F;sbin目录下的文件及程序提供的库文件。
&#x2F;lib64 – 可替代格式的库文件，这个目录不是必须存在的，例如64位程序会需要这个目录。
&#x2F;tmp – 临时文件存放目录。定期清除。
&#x2F;sys  – 设备，驱动，一些内核功能的相关信息。
&#x2F;proc – 虚拟文件系统展示进程及内核信息文件。系统启动目录创建，系统关闭目录消失。
&#x2F;var – 变量文件。在系统运行的过程中反复变化的文件。比如日志，邮件，信息输出等。
&#x2F;var&#x2F;log – 系统应用程序产生的日志文件默认存放路径。
&#x2F;var&#x2F;spool&#x2F;mail – 每个用户的邮件。


&#x2F;home – 用户的家目录，saved files, personal settings,etc.

Bash文件色彩显示的定义在： &#x2F;etc&#x2F;DIR_COLORS , 文件中定义了所有文件类型在Bash中的色彩。新技巧： 有一个$OLDPWD，可通过cd - 切换到上一次离开的目录
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/07/23/Linux/Linux_k8s-basic-2/" title="Kubernetes集群的学习笔记(2)">Kubernetes集群的学习笔记(2)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-07-23T13:00:03.000Z" title="发表于 2019-07-23 21:00:03">2019-07-23</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.745Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">Kubernetes的基本使用命令。
对控制命令进行分类整理：
查看k8s整体状态的命令
kubectl describe node node1.docker 查看node的详细信息
kubectl version 查看kube的版本信息，同时显示客户端的版本及服务端的版本
kubectl cluster-info 查看kube的集群信息，master节点的所在地址及kubeDNS的所在地址

手动运行Pods的命令
kubectl run nginx –image&#x3D;nginx –replicas&#x3D;5 启动5个nginx的pods
kubectl run nginx –image&#x3D;nginx –port:80 –replicas&#x3D;5 –dry-run&#x3D;true 启动5个nginx的pod的测试，但不执行改变，并expose端口80

查看Pods的命令
kubectl get pods 列出所有节点正在运行的pod的状态信息
kubectl get deployment 列出所有的deployment控制器的信息，所有的pod属于cni0桥，不属于docker桥

删除单独一个Pods的命令
kubectl delete pods hayden-nginx-6dd3ffd4c5-ww7mk删除一个指定的pod

手动建立Pods并对外发布的命令
kubectl expose deployment hayden-nginx –type&#x3D;[{ClusterIP},NodePort,LoadBalancer,ExternalName]  –name&#x3D;nginx-service –target-port&#x3D;80 –protocol&#x3D;TCP创建一个新的service，将内部的通信以service的形式对外交付

查看svc状态的命令
kubectl get svc查看service的状态

查看属于某个名称空间的命令
kubectl get nodes -n kube-system -o wide查看属于kube-system的节点的信息
kubectl get svc -n kube-system -o wide查看属于kube-system的服务的信息

从不重启Pods的设置
kubectl run t-centos –image&#x3D;centos -it –restart&#x3D;Never –replicas&#x3D;1启动1个普通的容器作为客户端进行服务的访问
kubectl describe svc nginx-service查看指定服务的详细信息
kubectl get nodes –show-labels查看节点的标签

对svc进行编辑和修改
kubectl edit svc nginx-service编辑一个service的属性，命令打开vim的编辑界面进行配置文件的更改
kubectl delete svc nginx-service删除一个service
kubectl get deployment -w长时间监控一个控制器的状态改变

对Pods的升级和回滚
kubectl set image deployment hayden-nginx hayden-nginx&#x3D;nginx:latest对容器指定其他的镜像进行动态的升级及更新
kubectl rollout status deployment hayden-nginx对容器版本的更新进行进度的查看

指定版本的回滚
kubectl rollout undo deployment hayden-nginx对升级的版本进行回滚，可指定回滚的版本，默认是上一个版本

Service发布到NodePort外部访问到service的方法是定义type为NodePort，可在创建service的时候定义类型，或者使用kubectl edit svc nginx-service将type变更为NodePort 。
创建资源的逻辑思路刚从docker的管理思路过来的时候还是有些茫然， 不太知道该怎么用。其实从k8s的管理逻辑上，我们将之前的container 变更为 pod，将pod的管理交给一个deployment，将deployment已经启动的所有容器通过对外提供service的方式expose到外部。例如：我需要一个httpd服务，不需要直接去docker启动了，我直接在k8s上定义一个deployment-httpd，通过k8s的控制器去调度容器。如果需要控制容器对外提供服务，那就直接创建一个service，通过service去直接定义和管理对外的服务即可。
</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/07/21/Linux/Linux_k8s-basic-1/" title="Kubernetes集群的学习笔记(1)">Kubernetes集群的学习笔记(1)</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-07-21T08:49:56.000Z" title="发表于 2019-07-21 16:49:56">2019-07-21</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-22T17:43:21.745Z" title="更新于 2023-12-23 01:43:21">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Kubernetes/">Kubernetes</a></span></div><div class="content">Kubernetes基础知识及笔记。
概念定义容器的出现以及容器编排引擎出现的原因。
容器编排工具容器最早的模型时LXC+Linux Namespace。容器的出现导致了我们需要对容器进行管理，单机的管理不能满足业务的需要，于是快速衍生出了多种不同的容器编排工具。Docker提供的工具：

docker compose
docker swarm 
docker machine

IDC的操作系统：

mesos(资源分配工具), marathron(面向容器编排的框架)

Google的工具：

Kubernetes

当一个产品可以占据35%以上的份额就已属于自然垄断。k8s现在已经处于垄断地位。透过容器所产生的衍生概念有: DevOps, MicroServices, BlockChain.开发模式的开发：瀑布模型，进化到了敏捷开发，精益开发，到现在的DevOps.发布线上的做法：蓝绿部署，灰度部署，金丝雀(canary)DevOps几个简单的名词解释：

CI： 持续集成 - 持续集成通俗一些就是快速提交代码，快速变更需求，快速合并代码。

CD： 持续部署 - 持续部署是指在代码提交变更之后，快速进行部署及测试；传统的代码在提交后需要运维人员手动部署，持续部署其实就是缩短的部署所需要的步骤和周期，尽可能将部署操作交由自动化完成。

CD： 持续交付 - 让软件产品的产出过程在一个短周期内完成，以保证软件可以稳定、持续的保持在随时可以发布的状态。
有时候，持续交付也与持续部署混淆。持续部署意味着所有的变更都会被自动部署到生产环境中。持续交付意味着所有的变更都可以被部署到生产环境中，但是出于业务考虑，可以选择不部署。如果要实施持续部署，必须先实施持续交付。


云原生的概念Native Cloud Application To Wikipedia.

 A native cloud application (NCA) is a type of computer software that natively utilizes services and infrastructure from cloud computing providers.

例如：当容器中的nginx需要变更项目文件的时候，容器的环境已经决定了不太容易对项目的变更,早期的程序设计使用配置文件进行定义。因此，最好的办法是可以通过传递宿主机环境变量的方式对镜像进行设置，当项目有变更的时候直接通过读取环境变量的方式对容器内项目的数据进行配置和更改。基于这种思路设计出来的容器或者软件，叫做云原生应用(Native Cloud Application)。
K8s的前世k8s本来开始是Google内部的Brog系统，Google在Docker被发现了之后，快速的使用Go语言重写了Brog系统，借鉴了Brog的逻辑和设计，所有的代码进行了全面的重构。
Kubernetes项目的Github页面
K8s的特点
自动装箱
自我修复
水平扩展
服务发现与负载均衡
自动发布和回滚
密钥的配置和管理 – 将配置定义在k8s的对象中，通过k8s在容器启动的时候自动传递环境变量到容器中。
存储编排 – 可自动创建存储卷供容器需要的自动管理及使用


批量处理执行

K8s的工作流程及逻辑
Kubernetes组合多台主机的资源，整合成资源池，并统一对外提供计算、存储等能力的集群。k8s的集群是Master\Node模型，具有角色分类，需要部署master节点及node节点。master节点一般有三个，node节点理论上可以无限增加。
k8s的简要工作流程： 收到请求后，Master节点上的Scheduler用来收集所有Node上的可用资源信息，并通过API Server告知资源充足的Node启动相关的容器；Node收到请求会查找需要的镜像，如果本地不存在从Docker Registery上面拉取相应的镜像进行所请求容器的启动。
k8s可将 Master节点 + Node节点 + Registery节点 同时托管在k8s自身的环境中，也就是直接托管在docker虚拟化层上，因此我们可以通过kubeadm等相关的工具将所有的组件使用容器的方式进行管理和调度。

Master上的主要组件及服务
API Server # 负责提供对外的及对内的服务接口

Scheduler # 选出适合部署容器的node节点，两级筛选：选出所有可以使用的node，在所有合规的node上进行最优选择，选择取决于调度算法。

Controller-Manager # 检测容器的状态，如果出现异常或不符合定义的状态，就向API申请重新部署相关容器。


Node上的主要服务及组件
Kube-Proxy
容器引擎，docker
Kubelet
Flannel

逻辑组件：PodNode上可以运行及调度逻辑单元pod，pod是k8s的一个逻辑概念，真实运行起来的还是Container，只是k8s为了方便管理，将一个或多个Container封装了成了逻辑上的Pod，打上了相应的标签，使得可以通过Seletor对不同的Pod进行分类管理和选择。Pod是模拟了传统的虚拟机模式(相同主机上的Pod可以使用宿主机的lo进行网络通信),使得可以构建精细的模型。
一个pod中可以有多个容器，共享底层名称空间net,UTS,IPC,另外三个互相隔离user,mnt,pid. 
Sidecar的概念一般来讲,一个Pod只放入一个容器，如果我们需要放置多个容器，一般为一个主要的一个辅助的，辅助的叫做 sidecar.例如主程序运行在一个容器中，收集日志的程序放在sidecar中。 
Pod的标签选择Pod已经被附加了一些元数据，创建完成的Pod可以通过标签的数值直接识别Pod的类别。k8s使用标签选择器selector来过滤符合条件的资源。所有的对象都可以通过标签选择器进行选择。
Pod的不完全分类
自主式pod # 自行控制自己的生命周期
控制器管理的pod # 通过管理器可管理生命周期的pod

k8s使用的pod的控制器
ReplicationController - 最早提供的组件，支持滚动更新和回滚，最早只有一个。现在已经不再使用。
ReplicaSet - 新的工具替代了原始的控制器，但是不直接使用，直接使用的是Deployment来管理无状态的pod。
StatefulSet - 对有状态的Pod进行管理。
DaemonSet - 控制容器的进程为守护进程的控制器，每个Node上都会运行一个，无法指定副本的数量。一般用于信息和日志的收集。
Job, Cronjob - 作业管理， 周期性作业管理。
HPA - HorizontalPodAutoscaler

服务发现的概念为了能快速管理新启动的pod，在pod和用户之间添加了逻辑的中间层，pod启动后需要在service中注册自己的信息,客户端需要相关的服务的时候从service取得相关的信息，完成服务的管理。service是逻辑组件，通过iptables的DNAT来实现相关的service的功能。
其实服务发现相当于消息中间件，有Pods(生产者)来注册，有用户(消费者)来消费，只是不会被消费掉。
Kube-proxy的作用一旦发现了service的改变，反映到Iptables上，同时向API Server报告。 
Kubernetes的安装部署通过kubeadm工具进行安装及部署，部署的过程其实不复杂，只是镜像的拉取麻烦了一些，记录了一些简单的步骤和问题。安装的时候启动了三个虚拟机，一台虚拟机作为Master节点，其他的两台作为Node节点，所有的机器都需要关闭firewalld，k8s会默认托管iptables的相关规则。安装采用k8s自托管的方式，结构如下，列出了在每个节点上必须运行的服务和容器：  



master
Node1
Node2



Kubelet
Kubelet
Kubelet


Pause
Pause
Pause


Flannel
Flannel
Flannel


Kube-Proxy
Kube-Proxy
Kube-Proxy


API Server




Schedular




Controller-manager




Coredns




etcd




机器的信息如下：  



IP-ADDRESS
HOSTNAME
ROLES



192.168.229.144
master.docker
master


192.168.229.145
node1.docker
node


192.168.229.146
node2.docker
node


安装基础环境这里记录一下所有机器都需要安装的环境及软件。

编辑hostname vim /etc/hostname
关闭firewalld systemctl mask firewalld
关闭selinux sudo sed -i &#39;s/SELINUX=enforcing/SELINUX=disabled/&#39; /etc/selinux/config
关闭Swap分区, 注释掉swap的相关行，或者在安装的时候默认不分区即可。 vim /etc/fstab 
配置docker-ce，kubernetes相关软件源,从阿里云直接配置到本地的机器上。 # kubernetes部分
cat &lt;&lt;EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
EOF

# docker -ce部分
curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun
同步镜像仓库，从docker-ce源安装docker-ce&#x2F;docker-cli&#x2F;containerd；其实可以关闭gpgcheck，并将其他同步失败的源关闭即可，我自己只是为了做学习用途，关闭了fedora的update源，总是同步失败耽误我的时间。 dnf makecache -y &amp;&amp; \
dnf install -y docker-ce docker-cli containerd &amp;&amp; \
systemctl start docker &amp;&amp; \
systemctl enable docker 
从kubernetes源安装kubeadm\kubectl\kubernetes-cni\kubelet四个组件。 dnf install - y kubeadm kubectl kubelet kubernetes-cni &amp;&amp; \
systemctl start kubelet &amp;&amp; \
systemctl enable kubelet
 四个组件的用途分别是：
kubeadm 用来进行所有节点的部署及初始化。
kubectl 集群的cli接口。
kubelet 每个Node节点都会启动kubelet进程，用来处理Master节点下发到本节点的任务，管理Pod和其中的容器。kubelet会在API Server上注册节点信息，定期向Mast ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/07/18/Other/Interview_2019/" title="面试记录">面试记录</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-07-18T05:12:33.000Z" title="发表于 2019-07-18 13:12:33">2019-07-18</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.962Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span></div><div class="content">这个月还是发生了不少事情了，面试，换房子，真快。

面试题
有一个字符串s&#x3D;“kjdfdsfevsdf”,使用python单独输出每个字符并在后面加上”th”.
 s = "asdfaurhgauh"
for i in s:
  print(i+'th')	
数据库问题，增删改查。基本上都没答上来。select语句对数据库内容Where做一个筛选。

提取b.txt中的所有域名，awk我写的grep。
grep -E -o "www.[[:alpha:]]*.com" ./b.txt | sort | uniq -c | sort -nr
awk -F / '&#123;print $3&#125;' b.txt | sort | uniq -c | sort -nr
我写的这个唯一个不好的地方就是不能匹配数字的部分，如果域名有数字就提取不出了。其实中间如果全部用正则也可以，但是正则会特别的长。

nginx的反向代理配置文件是不是能看懂，考了一个upstream模块，考了一个weight的分配，考了一个 proxy_pass模块的调用。

描述Raid0 ， 1 ，5的区别，但是没让说raid10。

描述如果你有多的资源如何搭建高可用和高并发，这种题目其实还挺无聊的，高并发靠负载均衡分流，四层转发七层代理。高可用靠的冗余和故障的快速切换。没什么特别的架构，web服务也就是keepalived + lvs。硬件的话F5直接搞，虽然我没见过，但是也听说过。

容器问了一个私有的register,不记得叫什么名字，但是我知道Docker官方有文档。软件名称是Harbor

k8s没什么可说了，昨天晚上听了一堆的k8s的理论，一个集群简单而要三个网络，要不是设计思路清晰，网络就已经是一锅粥了。软件的架构真的是越来越复杂了，现在的网络里面不是桥桥桥，就是NAT转换。

查看Linux系统CPU，MEM，NET-IO，DISK-IO的命令。top | htop | free -m | iostat -a

ext4如果分区超过2T如何操作。这题估计本来是问Ext3文件系统的，改的。Ext4随便分了已经。

如何理解top命令中的load average的含义 这个问题本来是聊到了服务器性能的观察，我说到了top命令查看系统的当前状况，所以后续有了这个问题，其实在系统中，这三个数值的计算方式是通过CPU进程队列的长度来进行计算的，具体的数值及计算公式网上有很多,等我不记得了我在去找资料吧，这种东西已经是死知识了，到处都是。 我说说我的理解，这三个数值其实是system load average,是系统的负载状态，其实是描述了系统的压力变化趋势，计算的CPU参数主要是Running Process &amp; uninterreptable Process , 其实就是正在运行和不可中断运行的进程，这些是CPU的工作量。还有一个衡量的参数实际上是IO，IO的指标也会体现在三个不同时间点的计算中。 如何查看或者分析是我一直理解的不透彻的点，记录一下。单项指标的观察不足以解决系统的问题，需要搭配其他的工具进行分析才可以。 看三个参数的变化趋势，1，5，15三个时间点：  

如果1分钟高，但是后面两个都低，说明系统当前的状态是压力高的，但是是暂时的。
如果是三个数值都高，可以说明可能是系统的性能不足或者有问题需要解决。
如果1分钟的数值低，但是15分钟的数值高，说明系统的压力会慢慢平稳下来，不会持续太久。


ps命令的使用

ps aux 
To see every process on the system using BSD syntax USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.1  0.0 186736 10160 ?        Ss   09:32   0:03 &#x2F;sbin&#x2F;init
root         2  0.0  0.0      0     0 ?        S    09:32   0:00 [kthreadd]
root         3  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [rcu_gp]
root         4  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [rcu_par_gp]
root         6  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [kworker&#x2F;0:0H-kblockd]
root         8  0.0  0.0      0     0 ?        I&lt;   09:32   0:00 [mm_percpu_wq]
root         9  0.0  0.0      0     0 ?        S    09:32   0:00 [ksoftirqd&#x2F;0]
root        10  0.0  0.0      0     0 ?        I    09:32   0:00 [rcu_preempt]
root        11  0.0  0.0      0     0 ?        I    09:32   0:00 [rcu_sched]
root        12  0.0  0.0      0     0 ?        I    09:32   0:00 [rcu_bh]


ps axjf &#x2F; ps -ejH
To print a process tree  PPID   PID  PGID   SID TTY      TPGID STAT   UID   TIME COMMAND
1   555   555   555 ?           -1 Ssl      0   0:00 &#x2F;usr&#x2F;bin&#x2F;gdm
555   585   555   555 ?           -1 Sl       0   0:00  \_ gdm-session-worker [pam&#x2F;gdm-launch-environment]
585   789   789   789 tty1       789 Ssl+   120   0:00  |   \_ &#x2F;usr&#x2F;lib&#x2F;gdm-x-session gnome-session --autostart &#x2F;usr&#x2F;share&#x2F;gdm&#x2F;g
789   791   789   789 tty1       789 Sl+    120   0:01  |       \_ &#x2F;usr&#x2F;lib&#x2F;Xorg vt1 -displayfd 3 -auth &#x2F;run&#x2F;user&#x2F;120&#x2F;gdm&#x2F;Xautho
791   848   789   789 tty1       789 S+       0   0:00  |       |   \_ xf86-video-intel-backlight-helper intel_backlight
789   939   789   789 tty1       789 Sl+    120   0:00  |       \_ &#x2F;usr&#x2F;lib&#x2F;gnome-session-binary --autostart &#x2F;usr&#x2F;share&#x2F;gdm&#x2F;gree
939   970   789   789 tty1       789 Sl+    120   0:04  |           \_ &#x2F;usr&#x2F;bin&#x2F;gnome-shell
970  1032  1032   789 tty1       789 Sl     120   0:00  |           |   \_ ibus-daemon --xim --panel disable
1032  1035  1032   789 tty1       789 Sl     120   0:00  |           |       \_ &#x2F;usr&#x2F;lib&#x2F;ibus&#x2F;ibus-dconf
1032  1200  1032   789 tty1       789 Sl     120   0:00  |           |       \_ &#x2F;usr&#x2F;lib&#x2F;ibus&#x2F;ibus-engine-simple
939  1086   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-rfkill
939  1088   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-smartcard
939  1089   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-clipboard
939  1090   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-xsettings
939  1091   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-housekeeping
939  1092   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-mouse
939  1093   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-power
939  1094   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-screensaver-proxy
939  1095   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-sound
939  1099   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F;lib&#x2F;gsd-color
939  1101   789   789 tty1       789 Sl+    120   0:00  |           \_ &#x2F;usr&#x2F ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/2019/07/13/Linux/Linux_Nginx-Proxy/" title="Nginx反向代理笔记">Nginx反向代理笔记</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">发表于</span><time class="post-meta-date-created" datetime="2019-07-13T09:50:39.000Z" title="发表于 2019-07-13 17:50:39">2019-07-13</time><span class="article-meta-separator">|</span><i class="fas fa-history"></i><span class="article-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-12-23T08:38:56.956Z" title="更新于 2023-12-23 16:38:56">2023-12-23</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Linux/">Linux</a></span></div><div class="content">Nginx的反向代理笔记。
Nginx程序的主要功能
load configuration
launch workers
non-stop upgrade

可以使用epoll单进程响应多个用户请求，如果是BSD可以使用kevent时间驱动模式响应。磁盘一侧使用的是高级IO中的sendfile机制，AIO异步IO，以及内存映射机制来完成硬盘IO的高级特性。

Nginx官方文档及参数说明在这里： nginx documentation

SNAT &amp;&amp; DNAT主要是工作在三层&#x2F;四层的协议SNAT主要的功能是隐藏客户端，DNAT对服务器接受并转发请求。NAT功能无法触及7层协议的上面三层，所以无权控制上面的数据包内容，所以NAT不能对应用层的内容作出更改及缓存。只能对网络的内容及数据包进行直接的转发以及控制。
正向代理 &amp;&amp; 反向代理正向代理是通过代理服务器对客户端发出的请求进行全部修改及转发；反向代理是通过代理服务器对发送到服务器的请求进行全部修改及转发；由于代理的服务器可以控制判定URL的资源内容，因此可以对站点进行动静分离处理。如果这个功能可以工作在应用层叫做代理，如果工作类似SNAT上的叫做正向代理，工作类似DNAT就叫做反向代理。
Nginx反向代理服务器
面向客户端：该方向支持两种协议： http&#x2F;https
面向服务端：该方向支持HTTP&#x2F;FastCGI&#x2F;memcache协议Nginx需要支持相关的协议需要有相关协议的对应模块原理以及流程：  从远程服务器取得数据进行nginx服务器本地的缓存，然后响应给客户端.可修改或具有修改意义的报文有两种：第一种是发到后面处理服务器的报文；第二种是发给客户端的响应报文。

Nginx的代理模块
ngx_http_proxy_module – Nginx的反向代理模块官方说明：Module ngx_http_proxy_module 添加新的反向代理服务配置文件： vim &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;proxy.conf
 server &#123;
    listen 80；
    server_name [SERVER_FQDN]
    location &#x2F; &#123;
        proxy_pass http:&#x2F;&#x2F;[IP_ADDR:PORT]
        # 字段后加上&#x2F;表示将location后的目录映射为后端服务器的目录；
        # 例如：如果是location &#x2F;test，那么在反向代理不加&#x2F;的时候，访问的是后端真实的&#x2F;test目录；
        # 如果加上&#x2F;，访问到的是后端服务器的根目录；
        # 当使用了正则表达式进行了匹配的时候后面不能添加&#x2F;符号；
    &#125;

    location ~* \.&#123;jpg|jpeg|png&#125;$ &#123;
        proxy_pass http:&#x2F;&#x2F;[IP_ADDR:PORT];
        # 正则表达模式下部分在代理服务器的地址后面加上&#x2F;；
    &#125;
&#125;
proxy_pass [RealServerAddress]；# 设置反向代理服务器的地址； proxy_set_header [field] [value]; # 设定传递到后端服务器的请求报文首部的值； proxy_set_header – Nginx的设定代理头文件参数模块的官方说明：proxy_set_header    
 proxy_set_header X-Real-IP $remote_addr;
在设定字段后，更改后端真实服务器的日志记录的值，即可记录所有的真正的客户端IP；  
proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  

proxy_http_headers_module # 设定发给客户端的响应报文的地址的值； Module ngx_http_headers_module – Nginx的客户端响应报文模块官方说明：  Module ngx_http_headers_module  
 add_header X_Via $server_addr;&#96; # 显示后端服务器的真实地址；

</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Liarlee</div><div class="author-info__description">Archlinux User, Support Engineer</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">135</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">66</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">16</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/Liarlee"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/LiarLee" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">都道无人愁似我，今夜雪，有梅花，似我愁。</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/22/Database/Databases_Redis-note/" title="Redis 笔记">Redis 笔记</a><time datetime="2023-12-22T04:54:23.000Z" title="发表于 2023-12-22 12:54:23">2023-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/22/Linux/Linux_CFS%20Schedler/" title="CFS 调度器资料">CFS 调度器资料</a><time datetime="2023-12-22T04:51:25.000Z" title="发表于 2023-12-22 12:51:25">2023-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/22/Linux/Linux_Iptables-and-conntrack/" title="追踪数据包经过的iptables规则">追踪数据包经过的iptables规则</a><time datetime="2023-12-22T04:49:30.000Z" title="发表于 2023-12-22 12:49:30">2023-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/22/Linux/Linux_iostat/" title="Iostat 参数说明">Iostat 参数说明</a><time datetime="2023-12-22T04:44:27.000Z" title="发表于 2023-12-22 12:44:27">2023-12-22</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2023/12/22/Linux/Linux_vmstat/" title="vmstat 参数说明">vmstat 参数说明</a><time datetime="2023-12-22T04:44:27.000Z" title="发表于 2023-12-22 12:44:27">2023-12-22</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分类</span>
            <a class="card-more-btn" href="/categories/" title="查看更多">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AWS/"><span class="card-category-list-name">AWS</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Books/"><span class="card-category-list-name">Books</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Database/"><span class="card-category-list-name">Database</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Docker/"><span class="card-category-list-name">Docker</span><span class="card-category-list-count">5</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/EKS/"><span class="card-category-list-name">EKS</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/ESXi/"><span class="card-category-list-name">ESXi</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/ElasticSearch/"><span class="card-category-list-name">ElasticSearch</span><span class="card-category-list-count">2</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Healthcare-IT/"><span class="card-category-list-name">Healthcare-IT</span><span class="card-category-list-count">2</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>标签</span></div><div class="card-tag-cloud"><a href="/tags/Linux/" style="font-size: 1.5em; color: #99a9bf">Linux</a> <a href="/tags/EKS/" style="font-size: 1.39em; color: #99a5b5">EKS</a> <a href="/tags/RabbitMQ/" style="font-size: 1.1em; color: #999">RabbitMQ</a> <a href="/tags/Redis/" style="font-size: 1.1em; color: #999">Redis</a> <a href="/tags/MongoDB/" style="font-size: 1.1em; color: #999">MongoDB</a> <a href="/tags/Nvidia/" style="font-size: 1.1em; color: #999">Nvidia</a> <a href="/tags/Vim/" style="font-size: 1.1em; color: #999">Vim</a> <a href="/tags/Grafana/" style="font-size: 1.1em; color: #999">Grafana</a> <a href="/tags/Database/" style="font-size: 1.17em; color: #999ca0">Database</a> <a href="/tags/BPF/" style="font-size: 1.1em; color: #999">BPF</a> <a href="/tags/Git/" style="font-size: 1.1em; color: #999">Git</a> <a href="/tags/NAS/" style="font-size: 1.1em; color: #999">NAS</a> <a href="/tags/FileSystem/" style="font-size: 1.14em; color: #999a9c">FileSystem</a> <a href="/tags/TrueNasScale/" style="font-size: 1.1em; color: #999">TrueNasScale</a> <a href="/tags/RPM/" style="font-size: 1.1em; color: #999">RPM</a> <a href="/tags/Bash/" style="font-size: 1.14em; color: #999a9c">Bash</a> <a href="/tags/Cacti/" style="font-size: 1.1em; color: #999">Cacti</a> <a href="/tags/AWS/" style="font-size: 1.35em; color: #99a3b1">AWS</a> <a href="/tags/Filesystem/" style="font-size: 1.1em; color: #999">Filesystem</a> <a href="/tags/GRUB2/" style="font-size: 1.1em; color: #999">GRUB2</a> <a href="/tags/Application/" style="font-size: 1.43em; color: #99a6b8">Application</a> <a href="/tags/Kubernetes/" style="font-size: 1.46em; color: #99a8bc">Kubernetes</a> <a href="/tags/CPU/" style="font-size: 1.14em; color: #999a9c">CPU</a> <a href="/tags/Kernel/" style="font-size: 1.28em; color: #99a0aa">Kernel</a> <a href="/tags/Python/" style="font-size: 1.21em; color: #999da3">Python</a> <a href="/tags/Ansible/" style="font-size: 1.14em; color: #999a9c">Ansible</a> <a href="/tags/Perf/" style="font-size: 1.21em; color: #999da3">Perf</a> <a href="/tags/Docker/" style="font-size: 1.32em; color: #99a2ae">Docker</a> <a href="/tags/%E8%AF%97/" style="font-size: 1.1em; color: #999">诗</a> <a href="/tags/docker/" style="font-size: 1.1em; color: #999">docker</a> <a href="/tags/Network/" style="font-size: 1.25em; color: #999fa7">Network</a> <a href="/tags/Office/" style="font-size: 1.1em; color: #999">Office</a> <a href="/tags/KVM/" style="font-size: 1.1em; color: #999">KVM</a> <a href="/tags/JAVA/" style="font-size: 1.14em; color: #999a9c">JAVA</a> <a href="/tags/Nginx/" style="font-size: 1.25em; color: #999fa7">Nginx</a> <a href="/tags/RIME/" style="font-size: 1.1em; color: #999">RIME</a> <a href="/tags/Orthanc/" style="font-size: 1.1em; color: #999">Orthanc</a> <a href="/tags/Firefox/" style="font-size: 1.1em; color: #999">Firefox</a> <a href="/tags/LVM/" style="font-size: 1.1em; color: #999">LVM</a> <a href="/tags/Memory/" style="font-size: 1.25em; color: #999fa7">Memory</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>归档</span><a class="card-more-btn" href="/archives/" title="查看更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">十二月 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">十月 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">九月 2023</span><span class="card-archive-list-count">2</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/08/"><span class="card-archive-list-date">八月 2023</span><span class="card-archive-list-count">7</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/07/"><span class="card-archive-list-date">七月 2023</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/06/"><span class="card-archive-list-date">六月 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/05/"><span class="card-archive-list-date">五月 2023</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/04/"><span class="card-archive-list-date">四月 2023</span><span class="card-archive-list-count">2</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>网站资讯</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章数目 :</div><div class="item-count">135</div></div><div class="webinfo-item"><div class="item-name">本站总字数 :</div><div class="item-count">149.2k</div></div><div class="webinfo-item"><div class="item-name">本站访客数 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站总访问量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最后更新时间 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2023-12-23T08:39:56.779Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background: rgb(105,105,105)"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Liarlee</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="255,250,240" opacity="0.7" zIndex="-1" count="99" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div></body></html>